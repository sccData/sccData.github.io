<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>scc技术博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-03T08:56:33.278Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>史崇辰</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据结构与算法-3-王争</title>
    <link href="http://yoursite.com/2018/10/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-3-%E7%8E%8B%E4%BA%89/"/>
    <id>http://yoursite.com/2018/10/03/数据结构与算法-3-王争/</id>
    <published>2018-10-03T08:14:19.000Z</published>
    <updated>2018-10-03T08:56:33.278Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除</p></blockquote><a id="more"></a><h3 id="数组">数组</h3><p>开头先引出一个问题, <strong>为什么数组要从0开始编号, 而不是从1开始吗?</strong></p><h3 id="如何实现随机访问">如何实现随机访问?</h3><p>数组(Array)是一种线性表数据结构. 它用一组连续的内存空间, 来存储一组具有相同类型的数组.</p><p>解释几个关键词:</p><p>第一是线性表(Linear List). 顾名思义, 线性表就是数据排成像一条线一样的结构. 每个线性表上的数据最多只有前和后两个方向. 其实除了数组, 链表、队列、栈等也是线性表结构.</p><p>而与它相对立的概念是非线性表, 比如二叉树、堆、图等. 这是因为, 数据之间不是简单的前后关系.</p><p>第二是连续的内存空间和相同类型的数据. 正是因为这两个限制, 所以其具有&quot;随机访问&quot;. 但是, 这也导致了数组中删除、插入一个数据时, 为了保证连续性, 需要做大量的数据搬移工作.</p><p>这里回答一下数组如何实现随机访问?</p><p>主要是通过这个公式来实现的 <span class="math display">\[a[i]\_address = base\_address + i * data\_type\_size\]</span> 其中, base_address是基地址, data_type_size表示数组中每个元素的大小.</p><p>这里纠正一个错误, 在面试时, 常常会被问到数组和链表的区别, 很多人都回答说:&quot;链表适合插入、删除, 时间复杂度<span class="math inline">\(O(1); 数组适合查找, 查找时间复杂度为\)</span>O(1)$&quot;.</p><p>实际上, 这种表述不准确. 数组适合查找操作, 但是查找的时间复杂度并不为<span class="math inline">\(O(1)\)</span>. 即便是排序好的数组, 用二分查找, 时间复杂度也是<span class="math inline">\(O(\log n)\)</span>. 所以, 正确的表述应该是, 数组支持随机访问, 根据下标随机访问的时间复杂度<span class="math inline">\(O(1)\)</span>.</p><h3 id="低效的插入和删除">低效的&quot;插入&quot;和&quot;删除&quot;</h3><h4 id="插入操作">插入操作</h4><p>假设数组的长度为<span class="math inline">\(n\)</span>, 现在, 如果需要将一个数据插入到数组中的第<span class="math inline">\(k\)</span>个位置. 为了把第<span class="math inline">\(k\)</span>个位置腾出来, 给新来的数据, 我们需要将第<span class="math inline">\(k \sim n\)</span>这部分的元素都顺序地往后挪一位. 那插入操作的时间复杂度是多少?</p><p>如果在数组的末尾插入元素, 就不需要移动数据, 时间复杂度为<span class="math inline">\(O(1)\)</span>. 但是如果在开头插入元素, 那所有数据都要往后移一位, 所以最坏时间复杂度为<span class="math inline">\(O(n)\)</span>. 因为在每个位置插入的概率都是一样的, 所以平均时间复杂度是<span class="math inline">\(O(n)\)</span>.</p><p>如果数组中的数据是有序的, 就必须按照上述方法. 但是, 如果数组中存储的数据并没有任何规律, 数组只是被当作一个存储数据的集合. 在这种情况下, 如果将数据插入到第<span class="math inline">\(k\)</span>个位置, 为了避免大规模的数据搬迁, 有一个简单的办法, 直接将第<span class="math inline">\(k\)</span>位的数据搬移到数组元素的最后, 把新的元素直接放入第<span class="math inline">\(k\)</span>个位置. 利用这种处理技巧, 在特定场景下, 在第<span class="math inline">\(k\)</span>个位置插入一个元素的时间复杂度就会降为<span class="math inline">\(O(1)\)</span>.</p><blockquote><p>+未完待续</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="王争课程笔记" scheme="http://yoursite.com/categories/%E7%8E%8B%E4%BA%89%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构与算法" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石 4.机器学习的可行性</title>
    <link href="http://yoursite.com/2018/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%AF%E8%A1%8C%E6%80%A7/"/>
    <id>http://yoursite.com/2018/10/02/机器学习基石-4-机器学习的可行性/</id>
    <published>2018-10-02T12:30:43.000Z</published>
    <updated>2018-10-03T08:08:47.787Z</updated>
    
    <content type="html"><![CDATA[<h3 id="不可能学习的情况">不可能学习的情况</h3><a id="more"></a><p>首先来看一个例子: 给定下图, 你能否判断最后的图形应该属于哪个类别, 是<span class="math inline">\(-1\)</span>还是<span class="math inline">\(+1\)</span></p><div class="figure"><img src="/2018/10/02/机器学习基石-4-机器学习的可行性/1.png" alt="一个学习谜题"><p class="caption">一个学习谜题</p></div><p>可以说<span class="math inline">\(g({\bf x}) = +1\)</span>, 因为所有已知<span class="math inline">\(y_n = +1\)</span>的图形都是对称的. 也可以说<span class="math inline">\(g({\bf x}) = -1\)</span>, 因为所有已知<span class="math inline">\(y_n = -1\)</span>的图形左上角都是黑的. 甚至可以找出更加复杂的规则, 让<span class="math inline">\(g({\bf x})\)</span>取得<span class="math inline">\(+1\)</span>或<span class="math inline">\(-1\)</span>. 可以说, 这是一道公说公有理, 婆说婆有理的问题, 无论怎么样都可以说得到的答案是错误的. 即, 这个问题不可学习.</p><p>接下来看一个更加数学一些的例子: 假设输入空间<span class="math inline">\(\mathcal{X} = \{0, 1 \}^3\)</span>, <span class="math inline">\(\mathcal{Y} = \{\circ, \times\}\)</span>, 已知的数据集<span class="math inline">\(\mathcal{D}\)</span>如下表所示</p><div class="figure"><img src="/2018/10/02/机器学习基石-4-机器学习的可行性/2.png" alt="2"><p class="caption">2</p></div><p>由于输入情况比较简单, 可以列举所有可能的<span class="math inline">\(h\)</span>, 也就是<span class="math inline">\(\mathcal{H}\)</span>是一个有限集. 我们能否学习到一个<span class="math inline">\(g \in \mathcal{H}\)</span>, 使得<span class="math inline">\(g \approx f\)</span>? 答案是, 我们只能保证<span class="math inline">\(g\)</span>在<span class="math inline">\(\mathcal{D}\)</span>中跟<span class="math inline">\(f\)</span>一样. 但是对于那些在<span class="math inline">\(\mathcal{D}\)</span>外的数据, 类似于上面黑白格的例子, 总可以找到一个<span class="math inline">\(f\)</span>使其与<span class="math inline">\(g\)</span>给出的答案不尽相同(甚至完全不同). 但是, 机器学习的目的就是要看算法在未知数据上的表现情况. 因此可以说, <strong>如果<span class="math inline">\(f\)</span>可以取任何形式</strong>, 那么从<span class="math inline">\(\mathcal{D}\)</span>中学习模型以对<span class="math inline">\(\mathcal{D}\)</span>外的元素做判断, 这件事是注定要失败的.</p><p>这个例子告诉我们, 我们想要在<span class="math inline">\(\mathcal D\)</span>以外的数据中更接近目标函数似乎是做不到的, 只能保证对<span class="math inline">\(\mathcal D\)</span>有很好的分类结果. 机器学习的这种特性被称为没有免费午餐(<span class="math inline">\(No\ Free\ Lunch\)</span>)定理. <span class="math inline">\(NFL\)</span>定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器. 不管采用何种学习算法, 至少存在一个目标函数, 能够使得随机猜想算法是更好的算法. 平常所说的一个学习算法比另一个算法更&quot;优越&quot;, 效果更好, 只是针对特定的问题, 特定的先验条件, 数据的分布, 训练样本的数目, 代价或奖励函数等. 从这个例子来看, <span class="math inline">\(NFL\)</span>说明了无法保证一个机器学习算法在<span class="math inline">\(\mathcal{D}\)</span>以外的数据集上一定能分类或预测正确, 除非加上一些假设条件.</p><h3 id="概率成为救世主">概率成为救世主</h3><p>事已至此, 需要开动脑筋, 想一想是否有工具可以帮助我们从已知推断未知. 这里再看一个例子: 假设有一个很大的罐子, 里面装了不计其数的橙球和绿球, 则是否有可能知道橙球的比例? 由于无法一颗一颗地数, 因此直接求这个问题很难, 但是可以通过随机取样的方法来推断这个比例. 例如假设从里面抓了<span class="math inline">\(10\)</span>颗球出来, 里面有<span class="math inline">\(3\)</span>颗是橙色的, 那么橙球的比例可以估计为<span class="math inline">\(30\)</span>%.</p><p>推而广之, 假设罐子中橙球的比例为<span class="math inline">\(\mu\)</span>, 绿球的比例为<span class="math inline">\(1-\mu\)</span>, 这里<span class="math inline">\(\mu\)</span>是一个未知数. 通过某种随机独立程序取样后观测到橙球的比例为<span class="math inline">\(\nu\)</span>, 绿球的比例为<span class="math inline">\(1-\nu\)</span>, 这里<span class="math inline">\(\nu\)</span>是一个已知数, 那么<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>有没有联系? 这时, 无法<strong>确定的</strong>说<span class="math inline">\(\nu\)</span>和<span class="math inline">\(\mu\)</span>, 因为也许罐子里只有<span class="math inline">\(5\)</span>颗绿球, 却有几百颗橙球, 而这次随机抽样只拿到了这<span class="math inline">\(5\)</span>颗绿球--这种情况是可能发生的. 但是可以退而求其次地说, <span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>在<strong>大部分情况下非常接近</strong>. 准确地说, 两者之间的关系可以用<span class="math inline">\(Hoeffding\)</span>不等式来描述. 即假设采样大小为<span class="math inline">\(N\)</span>, <span class="math inline">\(\epsilon\)</span>为一个比较小的数, 那么有 <span class="math display">\[P[|\nu - \mu| &gt; \epsilon \le 2\exp(-2 \epsilon^2 N)]\]</span> 即&quot;= &quot;这个结论&quot;大概差不多是对的&quot;(<span class="math inline">\(probably\ approximately\ correct\)</span>, <span class="math inline">\(PAC\)</span>)</p><p><span class="math inline">\(Hoeffding\)</span>不等式对任何<span class="math inline">\(N\)</span>和<span class="math inline">\(\epsilon\)</span>都成立, 而且不需要知道真实的<span class="math inline">\(\mu\)</span>值. 抽样样本越多(即<span class="math inline">\(N\)</span>越大), 或者界限越松(即<span class="math inline">\(\epsilon\)</span>越大), 则<span class="math inline">\(\mu \approx \nu\)</span>的概率越大</p><p><span class="math inline">\(Fun\ time\)</span>中, 已知<span class="math inline">\(\mu = 0.4\)</span>, 取样数<span class="math inline">\(N = 10\)</span>, 求取样后<span class="math inline">\(\nu \le 0.1\)</span>的概率上界. 这时可以把<span class="math inline">\(\epsilon\)</span>设为<span class="math inline">\(0.3\)</span>, 代入<span class="math inline">\(Hoeffding\)</span>不等式右侧可知上界为<span class="math inline">\(2 \exp (-2 \times 0.3^2 \times 10) \approx 0.3306\)</span>. 但是如果实际计算可知这个概率应该为<span class="math inline">\(0.6^{10} + 10 \times 0.6^9 \times 0.4 \approx 0.04636\)</span>. 即<span class="math inline">\(Hoeffding\)</span>不等式只能提供一个上界, 并不能对概率做出准确预估</p><h3 id="概率与机器学习的联系">概率与机器学习的联系</h3><p>我们可以把&quot;罐中取球&quot;的问题与机器学习问题建立联系, 即</p><ul><li>未知的橙球比例<span class="math inline">\(\mu\)</span>对应于学习问题中未知的目标函数<span class="math inline">\(f({\bf x})\)</span>, 或者也可以对应于, 对给定新样本(测试数据)<span class="math inline">\(\bf x\)</span>, 假设函数值<span class="math inline">\(h({\bf x})\)</span>是否等于目标函数值<span class="math inline">\(f({\bf x})\)</span></li><li>罐中的所有球对应于学习问题中的数据点<span class="math inline">\({\bf x} \in \mathcal{X}\)</span></li><li>取出橙球对应于<span class="math inline">\(h\)</span>判断错误, 即<span class="math inline">\(h({\bf x}) \not = f({\bf x})\)</span>. 注意, 这里<span class="math inline">\(h\)</span>是给定的, 对下一条也如此</li><li>取出绿球对应于<span class="math inline">\(h\)</span>判断正确, 即<span class="math inline">\(h({\bf x}) = f({\bf x})\)</span></li><li>从罐中做大小为<span class="math inline">\(N\)</span>的取样对应于学习问题中从数据集<span class="math inline">\(\mathcal{D} = \{({\bf x}_n, \underbrace{y_n}_{f({\bf x}_n)}\}\)</span>学习</li></ul><p>因此, 同样的道理, 如果N足够大而且<span class="math inline">\({\bf x}_n\)</span>满足独立同分布假设(<span class="math inline">\(iid\)</span>), 那么根据已知的错分情况<span class="math inline">\([\![h({\bf x}_n) \not = y_n]\!]\)</span>, 就可以推断假设函数<span class="math inline">\(h\)</span>在整个数据集上的错分概率<span class="math inline">\([\![h{\bf x}) \not = f({\bf x})]\!]\)</span>. 如果记<span class="math inline">\(h\)</span>对已知数据的错分情况(样本内错误率)为<span class="math inline">\(E_{\rm in}\)</span>, 全集上的错分情况(样本外错误率)为<span class="math inline">\(E_{out}\)</span>, 就会有 <span class="math display">\[\begin{align*}E_{\rm in}(h) &amp;= \frac{1}{N}\sum_{n=1}^N[\![h({\bf x}_n) \not = y_n]\!] \\E_{\rm out}(h) &amp;= \mathcal{E}_{{\bf x} \sim P}[\![h({\bf x}) \not= f({\bf x})]\!]\end{align*}\]</span> 同理套用<span class="math inline">\(Hoeffding\)</span>不等式, 在<span class="math inline">\(N\)</span>足够大的情况下, <span class="math inline">\(E_{in}(h)\)</span>和<span class="math inline">\(E_{out}(h)\)</span>之间相差超过<span class="math inline">\(\epsilon\)</span>的概率为 <span class="math display">\[P[|E_{\rm in}(h) - E_{\rm out}(h)| &gt; \epsilon] \le 2\exp (-2\epsilon^2 N)\]</span></p><p>即也可以说<span class="math inline">\(h\)</span>的样本内错误率&quot;大概差不多&quot;相当于其样本外错误率, <span class="math inline">\(E_{in}(h) \approx E_{out}(h)\)</span>. 因此, 如果<span class="math inline">\(h\)</span>的样本内错误率很小, 其样本外错误率也不会很大. 如果<span class="math inline">\(E_{in}(h) \approx E_{out}(h)\)</span>, <span class="math inline">\(E_{in}(h)\)</span>很小, 那么就能推断出<span class="math inline">\(E_{out}(h)\)</span>很小, 也就是说在该数据分布<span class="math inline">\(P\)</span>下, <span class="math inline">\(h\)</span>与<span class="math inline">\(f\)</span>非常接近. 而且<span class="math inline">\(\mathcal{A}\)</span>真的把<span class="math inline">\(h\)</span>当做<span class="math inline">\(g\)</span>返回, 那么<span class="math inline">\(g = f\)</span>是<span class="math inline">\(PAC\)</span>的. 但是如果<span class="math inline">\(\mathcal{A}\)</span>被写死了, 无论给定什么数据集都要返回<span class="math inline">\(h\)</span>作为<span class="math inline">\(g\)</span>, 那么<span class="math inline">\(E_{in}(h)\)</span>其实往往并不是最小的, 此时<span class="math inline">\(g \not= f\)</span>反而是<span class="math inline">\(PAC\)</span>的. 这意味着真正的学习不是让<span class="math inline">\(\mathcal{A}\)</span>返回固定的<span class="math inline">\(h\)</span>, 而是要让<span class="math inline">\(\mathcal{A}\)</span>在解空间<span class="math inline">\(\mathcal{H}\)</span>中搜索. 不过这提供了一个<strong>验证</strong>的思路, 即可以选出一些未在训练过程中使用的历史数据送给某个候选模型<span class="math inline">\(h\)</span>, 在这之上测试其表现如何.</p><h3 id="概率与真正学习的联系">概率与真正学习的联系</h3><p>真正学习过程中, 我们不会只有一个<span class="math inline">\(h\)</span>来验证, 而是要在<span class="math inline">\(\mathcal{H} = \{h_1, \ldots, h_M\}\)</span>中做出选择. 假设找到了一个<span class="math inline">\(h_i\)</span>在训练集上表现完全正确, 是否应该把它选择为<span class="math inline">\(g\)</span>?</p><p>先不考虑这个问题. 假设有<span class="math inline">\(150\)</span>个人, 每个人掷同一枚硬币<span class="math inline">\(5\)</span>次, 如果有一个人能连续扔出<span class="math inline">\(5\)</span>个正面, 那么这是否意味着这枚硬币不是均匀的? 答案是不可能. 因为假设硬币均匀, 由<span class="math inline">\(150\)</span>人每人掷<span class="math inline">\(5\)</span>次, 至少有一个人扔出连续<span class="math inline">\(5\)</span>个正面的概率是<span class="math inline">\(1-(\frac{31}{32})^{150} &gt; 99\)</span>. 那么返回刚才的例子, 这意味着<span class="math inline">\(h_i\)</span>即便在训练集上全对, 也不确定它就比其它的<span class="math inline">\(h&#39;\)</span>好(可能大家都是瞎猜). 假设将造成<span class="math inline">\(E_{in}\)</span>和<span class="math inline">\(E_{out}\)</span>相差比较大的取样称为&quot;不好的采样&quot;, 可以看到当存在选择的时候, 选择会恶化不好的采样(即让你选到错误<span class="math inline">\(h\)</span>的概率变得很大).</p><p>同样的道理, 也可以把<span class="math inline">\(E_{in}(h)\)</span>和<span class="math inline">\(E_{out}(h)\)</span>差得特别大的数据称为&quot;不好的数据&quot;. 注意<span class="math inline">\(Hoeffding\)</span>不等式只能保证<span class="math inline">\(h\)</span>遇到&quot;不好的数据&quot;的概率不是很大. 事实上, 每个<span class="math inline">\(h_i\)</span>都会对应的一些数据<span class="math inline">\(\mathcal{D}&#39;\)</span>成为&quot;不好的数据&quot;, 例如下表</p><table style="width:100%;"><colgroup><col width="8%"><col width="15%"><col width="15%"><col width="8%"><col width="20%"><col width="8%"><col width="20%"></colgroup><thead><tr class="header"><th></th><th><span class="math inline">\(\mathcal{D}_1\)</span></th><th><span class="math inline">\(\mathcal{D}_2\)</span></th><th><span class="math inline">\(\ldots\)</span></th><th><span class="math inline">\(\mathcal{D}_{1126}\)</span></th><th><span class="math inline">\(\ldots\)</span></th><th><span class="math inline">\(\mathcal{D}_{5678}\)</span></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(h_1\)</span></td><td>BAD</td><td></td><td></td><td></td><td></td><td>BAD</td></tr><tr class="even"><td><span class="math inline">\(h_2\)</span></td><td></td><td>BAD</td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td><span class="math inline">\(h_3\)</span></td><td>BAD</td><td>BAD</td><td></td><td></td><td></td><td>BAD</td></tr><tr class="even"><td><span class="math inline">\(\ldots\)</span></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td><span class="math inline">\(h_M\)</span></td><td>BAD</td><td></td><td></td><td></td><td></td><td>BAD</td></tr></tbody></table><p>从上表可以看到, 给出的这四条数据里, 只有第1126条是&quot;好的&quot;数据, 其余的都会给不同的<span class="math inline">\(h\)</span>带来麻烦. 接下来的问题是, 如果对某条数据<span class="math inline">\(\mathcal{D}_i\)</span>, 只要存在<span class="math inline">\(h_i \in \mathcal{H}\)</span>使得<span class="math inline">\(\mathcal{D}_i\)</span>是<span class="math inline">\(h_i\)</span>&quot;不好的数据&quot;, 那就称该数据为&quot;不好的数据&quot;, 那么对所有<span class="math inline">\(\mathcal{D}\)</span>, 如果算法<span class="math inline">\(\mathcal{A}\)</span>能够自由选择数据, 则算法遇上&quot;不好的数据&quot;的概率有没有上界</p><p>答案是有的. 假设<span class="math inline">\(\mid \mathcal{H} \mid = M\)</span>, 可以做出如下推到 <span class="math display">\[\begin{align*}&amp;P_{\mathcal{D}}[{\rm BAD\ }\mathcal{D}] \\= &amp;P_{\mathcal{D}}[{\rm BAD\ }\mathcal{D} {\rm\ for\ }h_1 {\rm\ or\ }{\rm BAD\ }\mathcal{D} {\rm\ for\ }h_2 {\rm\ or\ }\ldots {\rm\ or\ }{\rm BAD\ }\mathcal{D} {\rm\ for\ }h_M] \\\le &amp;P_\mathcal{D}[{\rm BAD\ }\mathcal{D} {\rm\ for\ }h_1] + P_\mathcal{D}[{\rm BAD\ }\mathcal{D} {\rm\ for\ }h_2] + \ldots + P_\mathcal{D}[{\rm BAD\ }\mathcal{D} {\rm\ for\ }h_M] \\\le &amp;2\exp(-2\epsilon^2N) + 2\exp(-2\epsilon^2N) + \ldots + 2\exp(-2\epsilon^2N) \\= &amp;2M\exp(-2\epsilon^2N)\end{align*}\]</span></p><p>即在假设集为有限集, 数据量足够的情况下, 每个h都是安全的, 即无论如何选, 其返回的g都会有<span class="math inline">\(E_{in}(g) = E_{out}(g)是是PAC\)</span>的. 所以, 最理性的<span class="math inline">\(\mathcal{A}\)</span>会选择使<span class="math inline">\(E_{in}\)</span>最小的<span class="math inline">\(h\)</span>作为<span class="math inline">\(g\)</span>.</p><p>但是当<span class="math inline">\(M\)</span>无限大时, 如何证明机器学习是可行的?</p><blockquote><p>+参考[http://txshi-mt.com/2017/08/06/NTUML-4-Feasibility-of-Learning/]</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;不可能学习的情况&quot;&gt;不可能学习的情况&lt;/h3&gt;
    
    </summary>
    
      <category term="林轩田课程笔记" scheme="http://yoursite.com/categories/%E6%9E%97%E8%BD%A9%E7%94%B0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>复杂度分析（下）</title>
    <link href="http://yoursite.com/2018/09/29/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-2-%E7%8E%8B%E4%BA%89/"/>
    <id>http://yoursite.com/2018/09/29/数据结构与算法-2-王争/</id>
    <published>2018-09-29T15:22:54.000Z</published>
    <updated>2018-10-01T01:45:10.380Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除</p></blockquote><a id="more"></a><h3 id="最好最坏时间复杂度">最好、最坏时间复杂度</h3><p>先给个例子:</p><pre><code>// n 表示数组 array 的长度int find(int[] array, int n, int x){    int i = 0;    int pos = -1;    for(; i&lt;n; ++i){        if (array[i] == x){            pos = i;        }    }    return pos}</code></pre><p>按照上次的分析方法, 这段代码的复杂度是<span class="math inline">\(O(n)\)</span>, 其中, n代表数组的长度.</p><p>但是在数组中查找一个数据, 不需要从头到尾遍历一遍, 在中间可能就可以提前结束循环. 优化代码.</p><pre><code>// n 表示数组 array 的长度int find(int[] array, int n, int x){    int i = 0;    int pos = -1;    for(; i&lt;n; ++i){        if(array[i] == x){            pos = i;            break;        }    }    return pos;}</code></pre><p>如果数组的第一个元素正好是要查找的变量<span class="math inline">\(x\)</span>, 那时间复杂度是<span class="math inline">\(O(1)\)</span>. 但是如果数组中不存在变量<span class="math inline">\(x\)</span>, 那时间复杂度是<span class="math inline">\(O(n)\)</span>.</p><p>由此引入三个概念: 最好情况时间复杂度、最坏情况时间复杂度和平均情况复杂度.</p><p>最好情况时间复杂度和最坏情况时间复杂度都很好理解, 重点记录一下平均情况时间复杂度.</p><h4 id="平均情况时间复杂度">平均情况时间复杂度</h4><p>查找变量x在数组中的位置, 有n+1种情况: 在数组的<span class="math inline">\(0 \sim n-1\)</span>位置中和不在数组中. 把每种情况下, 需要遍历的元素累加起来, 然后再除以n+1, 就可以得到需要遍历的元素个数的平均值, 即: <span class="math display">\[\frac{1+2+3+\cdots+n-1+n}{n+1} = \frac{n(n+3)}{2(n+1)}\]</span> 将其简化后可得<span class="math inline">\(O(n)\)</span>. 但是, 这个结果是有问题的, 因为每种情况出现的概率不一样.</p><p>要查找的变量<span class="math inline">\(x\)</span>, 要么在数组里, 要么不在数组里. 为了方便理解, 假设在数组中与不在数组中的概率都为<span class="math inline">\(\frac{1}{2}\)</span>. 另外, 要查找的数据出现在<span class="math inline">\(0 \sim n-1\)</span>这<span class="math inline">\(n\)</span>个位置的概率也是一样的, 为<span class="math inline">\(\frac{1}{n}\)</span>. 所以, 根据概率乘法法则, 要查找的数据出现在<span class="math inline">\(0 \sim n-1\)</span>中任意位置的概率就是<span class="math inline">\(\frac{1}{2n}\)</span>.</p><p>计算过程为: <span class="math display">\[1 \times \frac{1}{2n}+2 \times \frac{1}{2n}+3\times\frac{1}{2n}+ \cdots +n \times \frac{1}{2n} + n \times \frac{1}{2} = \frac{3n+1}{4}\]</span> 这个值就是概率论中的<strong>加权平均值</strong>, 也叫作<strong>期望值</strong>, 所以平均时间复杂度的全称应该叫<strong>加权平均时间复杂度</strong>或者<strong>期望时间复杂度</strong>.</p><p>这段代码的加权平均时间复杂度仍然是<span class="math inline">\(O(n)\)</span>.</p><h4 id="均摊时间复杂度">均摊时间复杂度</h4><p>均摊时间复杂度, 听起来跟平均时间复杂度有点儿像. 但是, 其应用场景更加特殊、更加有限.</p><pre><code>// array 表示一个长度为 n 的数组// 代码中的 array.length 就等于 nint[] array = new int[n];int count = 0;void insert(int val){    if(count == array.length){        int sum = 0;        for(int i=0; i&lt;array.length; ++i){            sum = sum + array[i];        }        array[0] = sum;        count = 1;    }    array[count] = val;    ++count;}</code></pre><p>最理想的情况是, 数组中有空闲空间, 我们只需要将数据插入到数组下表为<span class="math inline">\(count\)</span>的位置就可以了, 所以最好情况时间复杂度为<span class="math inline">\(O(1)\)</span>. 最坏的情况下, 数组中没有空闲空间了, 我们需要先做一次数组的遍历求和, 然后再将数据插入, 所以最坏情况时间复杂度为<span class="math inline">\(O(n)\)</span>.</p><p>而平均时间复杂度是<span class="math inline">\(O(1)\)</span>. 我们还是可以通过概率论的方法来分析.</p><p>假设数组的长度是n, 根据数据插入的位置的不同, 我们可以分为n种情况, 每种情况的时间复杂度是O(1). 除此之外, 还有一种&quot;额外&quot;的情况, 就是在数组没有空闲空间时插入一个数据, 这个时候的时间复杂度是O(n). 而且, 这n+1种情况发生的概率一样, 都是. 所以, 根据加权平均的计算方法, 我们求得的平均时间复杂度就是: <span class="math display">\[1 \times \frac{1}{n+1}+1\times\frac{1}{n+1}+\cdots+1\times\frac{1}{n+1}+n\times\frac{1}{n+1} = O(1)\]</span> 其实这个例子不需要引入概率论的知识, 对比一下<span class="math inline">\(insert()\)</span>和<span class="math inline">\(find()\)</span>的例子, 就会发现这两者有很大的差别.</p><p>首先, <span class="math inline">\(find()\)</span>在极端情况下, 复杂度才为<span class="math inline">\(O(1)\)</span>. 但<span class="math inline">\(insert()\)</span>在大部分情况下, 时间复杂度都为<span class="math inline">\(O(1)\)</span>. 只有个别情况下, 复杂度才为<span class="math inline">\(O(n)\)</span>. 这是<span class="math inline">\(insert()\)</span>第一个区别与<span class="math inline">\(find()\)</span>的地方.</p><p>第二个不同点, 对于<span class="math inline">\(insert()\)</span>函数来说, <span class="math inline">\(O(1)\)</span>时间复杂度的插入和<span class="math inline">\(O(n)\)</span>时间复杂度的插入, 出现的频率是非常有规律的, 而且有一定的前后时序关系, 一般都是一个<span class="math inline">\(O(n)\)</span>插入之后, 紧跟着<span class="math inline">\(n-1\)</span>个<span class="math inline">\(O(1)\)</span>的插入操作, 循环往复.</p><p>针对这样一种特殊场景的复杂度分析, 我们并不需要像之前那样, 找出所有的输入情况及相应的发生概率, 然后计算加权平均值. 为此, 引入一种更加简洁的分析方法: <strong>摊还分析法</strong>, 通过摊还分析法得到的时间复杂度我们起了一个名字, 叫<strong>均摊时间复杂度</strong>.</p><p>针对上述例子, 每一次<span class="math inline">\(O(n)\)</span>的插入操作, 都会跟着<span class="math inline">\(n-1\)</span>次<span class="math inline">\(O(1)\)</span>的插入操作, 所以把耗时多的那次操作均摊到接下来的<span class="math inline">\(n-1\)</span>次耗时少的操作上, 均摊下来, 这一组连续的操作的均摊时间复杂度就是<span class="math inline">\(O(1)\)</span>.</p><p>总结其应用场景:</p><p>对一个数据结构进行一组连续操作中, 大部分情况下时间复杂度都很低, 只有个别情况下时间复杂度比较高, 而且这些操作之间存在前后连贯的时序关系, 这个时候, 我们就可以将一组操作放在一块儿分析, 看是否能将较高时间复杂度那次操作的耗时, 平摊到其他那些时间复杂度比较低的操作上. 而且, 能够应用均摊时间复杂度分析的场合, 一般均摊时间复杂度就等于最好情况时间复杂度.</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="王争课程笔记" scheme="http://yoursite.com/categories/%E7%8E%8B%E4%BA%89%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构与算法" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>复杂度分析（上）</title>
    <link href="http://yoursite.com/2018/09/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-%E7%8E%8B%E4%BA%89/"/>
    <id>http://yoursite.com/2018/09/26/数据结构与算法-1-王争/</id>
    <published>2018-09-26T12:06:30.000Z</published>
    <updated>2018-09-29T08:48:59.089Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除</p></blockquote><a id="more"></a><h3 id="为什么需要复杂度分析">为什么需要复杂度分析？</h3><p>很多人对复杂度分析有疑问, 认为直接在机器上跑一遍, 就可以得出时间和空间复杂度. 对于这种说法, 我们认为是正确的, 并且很多书籍将其称为<strong>事后统计</strong>. 但是, 这种方法有很大的局限性.</p><ul><li>测试结果依赖于测试环境</li></ul><p>不同的硬件对测试结果影响较大</p><ul><li>测试结果受数据规模的影响很大</li></ul><p>数据规模的大小和有序度, 对测试结果影响较大</p><p>所以, 我们需要一个不用具体的测试数据来测试, 就可以粗略地估计算法的执行效率的方法.</p><h3 id="大o复杂度表示法">大<span class="math inline">\(O\)</span>复杂度表示法</h3><p>以一段代码为例来估计算法的执行时间</p><pre><code>int cal(int n) {    int sum = 0;    int i = 1;    for(; i &lt;= n; ++i){        sum = sum + i;    }    return sum;}</code></pre><p>由于是粗略估计, 假设每行代码执行的时间都一样, 为<span class="math inline">\(t\)</span>. 第2、3行代码分别需要1个<span class="math inline">\(t\)</span>的执行时间, 第4、5行都运行了<span class="math inline">\(n\)</span>遍, 所以需要<span class="math inline">\(2 n * t\)</span>的执行时间, 所以这段代码总的执行时间就是<span class="math inline">\((2 n + 2) * t\)</span>. 可以看出来, 所有的代码执行时间<span class="math inline">\(T(n)\)</span>与每行代码的执行次数成正比.</p><p>再看一段代码</p><pre><code>int cal(int n) {    int sum = 0;    int i = 1;    int j = 1;    for(; i &lt;= n; ++i){        j = 1;        for(; j &lt;= n; ++j){            sum = sum + i * j;        }    }}</code></pre><p>根据以上思路, 可以得出$T(n) = (2n^2 + 2n + 3) * t $.</p><p><em>从中我们可以总结得到一个非常重要的规律, 所有代码的执行时间<span class="math inline">\(T(n)\)</span>与每行代码的执行次数<span class="math inline">\(n\)</span>成正比</em> <span class="math display">\[T(n) = O(f(n))\]</span> 其中<span class="math inline">\(T(n)\)</span>表示代码执行的时间; n表示数据规模的大小; <span class="math inline">\(f(n)\)</span>表示每行代码执行的次数总和. 公式中的<span class="math inline">\(O\)</span>, 表示代码的执行时间<span class="math inline">\(T(n)\)</span>与<span class="math inline">\(f(n)\)</span>表达式成正比.</p><p>所以<span class="math inline">\(T(n) = O(2n + 2)\)</span>, <span class="math inline">\(T(n) = O(2n^2 + 2n + 3)\)</span>, 这就是大<span class="math inline">\(O\)</span>时间复杂度表示法. 大<span class="math inline">\(O\)</span>时间复杂度实际表示的是代码执行时间随数据规模增长的变化趋势, 所以, 也叫做渐进时间复杂度, 简称时间复杂度.</p><p>当<span class="math inline">\(n\)</span>很大的时候, 我们只需记录一个最大量级就可以了, 例如<span class="math inline">\(T(n) = O(n)\)</span>; <span class="math inline">\(T(n) = O(n^2)\)</span>.</p><h3 id="时间复杂度分析">时间复杂度分析</h3><ul><li><p>只关注循环次数最多的一段代码</p><pre><code>int cal(int n) {    int sum = 0;    int i = 1;    for(; i &lt;= n; ++i){        sum = sum + i;    }    return sum;}</code></pre></li></ul><p>总的时间复杂度为<span class="math inline">\(O(n)\)</span></p><ul><li><p>加法法则: 总复杂度等于量级最大的那段代码的复杂度</p><pre><code>int cal(int n){    int sum_1 = 0;    int p = 1;    for(; p &lt; 100; ++p){        sum_1 = sum_1 + p;    }    int sum_2 = 0;    int q = 1;    for(; q&lt;n; ++q){        sum_2 = sum_2 + q;    }    int sum_3 = 0;    int i = 1;    int j = 1;    for(; i&lt;=n; ++i){        for(; j&lt;=n; ++j){            sum_3 = sum_3 + i * j;        }    }return sum_1 + sum_2 + sum_3;}</code></pre></li></ul><p>总的时间复杂度为<span class="math inline">\(O(n^2)\)</span></p><ul><li><p>乘法法则: 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积</p><pre><code>int cal(int n){    int ret = 0;    int i = 1;    for(; i&lt;n; ++i){        ret = ret + f(i);    }}int f(int n){    int sum = 0;    int i = 1;    for(; i&lt;n; ++i){        sum = sum + i;    }    return sum;}</code></pre></li></ul><p>总的时间复杂度为<span class="math inline">\(O(n^2)\)</span></p><h3 id="几种常见时间复杂度实例分析">几种常见时间复杂度实例分析</h3><p>复杂度量级(按数量级递增)</p><ul><li>常量阶<span class="math inline">\(O(1)\)</span></li><li>对数阶<span class="math inline">\(O(logn)\)</span></li><li>线性阶<span class="math inline">\(O(n)\)</span></li><li>线性对数阶<span class="math inline">\(O(nlogn)\)</span></li><li>平方阶<span class="math inline">\(O(n^2)\)</span>、立方阶<span class="math inline">\(O(n^3) \cdots k\)</span>次方阶<span class="math inline">\(O(n^k)\)</span></li><li>指数阶<span class="math inline">\(O(2^n)\)</span></li><li>阶乘阶<span class="math inline">\(O(n!)\)</span></li></ul><p>将上述时间复杂度错略的分为两类：<strong>多项式量级</strong>和<strong>非多项式量级</strong>. 其中, 非多项式量级只有两个: <span class="math inline">\(O(2^n)\)</span>和<span class="math inline">\(O(n!)\)</span>.</p><p>我们把时间复杂度为非多项式量级的算法问题叫做<strong>NP问题</strong>(Non-Deterministic Polynomial, 非确定多项式).</p><p>当数据规模<span class="math inline">\(n\)</span>越来越大时, 非多项式量级算法的执行时间会急剧增加.</p><p>因此, NP问题不是我们讨论的重点. 接下来, 我们主要来看几种常见的<strong>多项式时间复杂度</strong>.</p><ol style="list-style-type: decimal"><li><span class="math inline">\(O(1)\)</span></li></ol><p><span class="math inline">\(O(1)\)</span>只是常量级时间复杂度的一种表示方法, 并不是指只执行了一行代码.</p><pre><code>int i = 8;int j = 6;int sum = i + j;</code></pre><p>只要代码的执行时间不随<span class="math inline">\(n\)</span>的增长而增长, 这样代码的时间复杂度都记作<span class="math inline">\(O(1)\)</span>. <strong>一般情况下, 只要算法中不存在循环语句、递归语句, 即使有成千上万行代码, 其时间复杂度也是<span class="math inline">\(O(1)\)</span></strong>.</p><ol start="2" style="list-style-type: decimal"><li><p><span class="math inline">\(O(logn)\)</span>、<span class="math inline">\(O(nlogn)\)</span></p><pre><code>i = 1;while(i&lt;=n){    i = i * 2;}</code></pre></li></ol><p>从代码中可以看出, 变量<span class="math inline">\(i\)</span>的值为: <span class="math display">\[   2^0\ \ 2^1\ \ 2^2\  \cdots \ 2^k\  \cdots \ 2^x = n\]</span> 通过求解<span class="math inline">\(2^x = n\)</span>, 就可以知道代码的执行次数. 所以其为<span class="math inline">\(O(\log_2n)\)</span>.</p><p>因为<span class="math inline">\(\log_3n\)</span>就等于<span class="math inline">\(\log_32 * \log_2n\)</span>, 所以<span class="math inline">\(O(\log_3n) = O(C * \log_2n)\)</span>, 其中<span class="math inline">\(C = \log_32\)</span>是一个常量. 因此, 在对数时间复杂度的表示方法里, 忽略对数的&quot;底&quot;, 统一表示为<span class="math inline">\(O(\log n)\)</span>.</p><p>如果一段代码的时间复杂度是<span class="math inline">\(O(\log n)\)</span>, 循环<span class="math inline">\(n\)</span>遍, 时间复杂度就是<span class="math inline">\(O(n\log n)\)</span>.</p><ol start="3" style="list-style-type: decimal"><li><p><span class="math inline">\(O(m+n)\)</span>、<span class="math inline">\(O(m*n)\)</span></p><pre><code>int call(int m, int n){    int sum_1 = 0;    int i = 1;    for(; i&lt;m; ++i){        sum_1 = sum_1 + 1;    }    int sum_2 = 0;    int j = 1;    for(; j&lt;n; ++j){        sum_2 = sum_2 + j;    }    return sum_1 + sum_2;}</code></pre></li></ol><p>从代码中可以看出, <span class="math inline">\(m\)</span>和<span class="math inline">\(n\)</span>是表示两个数据规模, 我们无法评判谁的数量级大, 所以, 时间复杂度就为<span class="math inline">\(O(m+n)\)</span>.</p><p>乘法类似.</p><h3 id="空间复杂度">空间复杂度</h3><p>空间复杂度全程就是<strong>渐进空间复杂度</strong>, <strong>表示算法的存储空间与数据规模之间的增长关系</strong>.</p><pre><code>void print(int n){    int i = 0;    int[] a = new int[n];    for(i; i&lt;n; ++i){        a[i] = i*i;    }    for(i=n-1; i&gt;=0; --i){        print out a[i];    }}</code></pre><p>第<span class="math inline">\(2\)</span>行代码中, 我们申请了一个空间存储变量<span class="math inline">\(i\)</span>, 但是它是常量阶, 跟数据规模<span class="math inline">\(n\)</span>没有关系, 所以忽略. 第<span class="math inline">\(3\)</span>行申请了一个大小为<span class="math inline">\(n\)</span>的<span class="math inline">\(int\)</span>类型数组, 除此之外, 剩下的代码都没有占用更多的空间, 所以整段代码的空间<span class="math inline">\(O(n)\)</span>.</p><p>常见的空间复杂度就是<span class="math inline">\(O(1)\)</span>、<span class="math inline">\(O(n)\)</span>、<span class="math inline">\(O(n^2)\)</span>.</p><h3 id="学习关键">学习关键</h3><p><strong>多练</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="王争课程笔记" scheme="http://yoursite.com/categories/%E7%8E%8B%E4%BA%89%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构与算法" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石 3.机器学习的类型</title>
    <link href="http://yoursite.com/2018/09/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%B1%BB%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/09/25/机器学习基石-3-机器学习的类型/</id>
    <published>2018-09-25T05:42:31.000Z</published>
    <updated>2018-10-03T04:59:39.614Z</updated>
    
    <content type="html"><![CDATA[<h3 id="根据输出空间mathcaly分类">根据输出空间<span class="math inline">\(\mathcal{Y}\)</span>分类</h3><a id="more"></a><h4 id="二分类问题">二分类问题</h4><p>重新回顾一下&quot;是非题&quot;的形式. 为了解决这个问题, 需要我们提供一批训练数据<span class="math inline">\(\mathcal{D}\)</span>, 其中我们需要指出对哪些用户发放信用卡, 哪些不发. 像这样答案只有两种可能性(&quot;要&quot;或&quot;不要&quot;)的问题称为<strong>二元分类问题</strong>, 其输出空间<span class="math inline">\(\mathcal{Y}\)</span>通常用集合<span class="math inline">\(\{-1, +1\}\)</span>表示, 类似于&quot;判断题&quot;. 这种问题类型的例子有很多, 包括</p><ul><li>要不要发信用卡</li><li>电子邮件是不是垃圾邮件</li><li>病人有没有生病</li><li>广告是否会赚钱</li></ul><p>等等。</p><p>二元分类问题是机器学习中最基本也是最核心的问题, 很多理论推导和算法模型设计都是从这一类问题出发.</p><h4 id="多分类问题">多分类问题</h4><p>二元分类问题很容易进行扩展, 即如果答案有多个离散的可能性, 那么问题演变为<strong>多元分类问题</strong>. 假设目标类别有<span class="math inline">\(K\)</span>种, 那么<span class="math inline">\(\mathcal{Y}=\{1, 2, \cdots, K\}\)</span>. 一个典型的例子是对硬币进行分类, 看投入的是<span class="math inline">\(1\)</span>角、<span class="math inline">\(5\)</span>角还是<span class="math inline">\(1\)</span>元. 这种问题类似于“选择题”. 这种问题类型的例子包括</p><ul><li>识别手写数字是<span class="math inline">\(0\)</span>到<span class="math inline">\(9\)</span>这十个数字中的哪一种</li><li>识别图片中的水果是哪一种水果</li><li>邮件的进一步分类, 例如是垃圾邮件、社交网络邮件、重要邮件还是促销活动邮件等等</li></ul><h4 id="回归问题">回归问题</h4><p>如果将医疗领域中的问题对应到上述问题中, 那么这两种问题可以对应如下：</p><ul><li>二元分类问题：给定病人特征, 判断病人是否患病</li><li>多元分类问题：给定病人特征, 判断病人患的是哪种癌症</li></ul><p>但是还有一类问题， 例如判断病人手术后多少天可以出院. 这种问题的输出是整个实数集, 或者实数集中的一个连续区间. 这种问题通常被称为<strong>回归分析</strong>. 此时<span class="math inline">\(\mathcal{Y} \in \mathbb{R}\)</span>或<span class="math inline">\(\mathcal{Y} = [{\rm lower},{\rm upper}] \subset \mathbb{R}\)</span>. 这种问题类型的例子包括</p><ul><li>根据公司的状况, 预测其次日股票价格</li><li>根据大气状况, 预测明日气温</li></ul><p>回归问题是一种历史悠久的统计问题, 也是机器学习领域里非常核心的问题</p><h4 id="结构化分析">结构化分析</h4><p>在自然语言处理(<span class="math inline">\(\rm NLP\)</span>)这个领域里, 有一项任务是对输入句子中的每个词标注其词性(<span class="math inline">\({\rm Part\ of\ Speech}, {\rm POS}\)</span>). 例如输入&quot;<span class="math inline">\(\rm {I\ love\ ML}\)</span>&quot;, 程序应该可以将&quot;I&quot;标记为代词, &quot;<span class="math inline">\(\rm{love}\)</span>&quot;标记为动词, &quot;<span class="math inline">\(\rm{ML}\)</span>&quot;标记为名词. 这种任务可以看作是一种多元分类问题, 但是如果输入是以句子为单位, 由于句子中有结构性, 因此输出也是一个结构. 这样的问题可以看做是一个巨大的多类别分类问题, 各个类别是隐藏的, 看不到, 而且不同类别之间有联系, 使得穷举所有可能性变得不可能. 但是我们知道输出存在一定的结构性, 并希望程序能够正确给出判定. 这种问题称为<strong>结构化分析</strong>, 此时<span class="math inline">\(\mathcal{Y}\)</span>是一种结构. 这种问题类型的例子包括</p><ul><li>给定蛋白质数据, 判断蛋白质的结构</li><li>给定语言文本, 给出语法树</li></ul><h3 id="根据数据标签y_n分类">根据数据标签<span class="math inline">\(y_n\)</span>分类</h3><h4 id="有监督学习">有监督学习</h4><p>考虑第一节中的硬币分类问题. 我们可以将所有硬币的特征收集起来, 设成<span class="math inline">\({\bf x}_n\)</span>, 同时可以将硬币的面额给出, 称为<span class="math inline">\(y_n\)</span>. 这两部分可以一起给到机器学习的算法<span class="math inline">\(\mathcal A\)</span>里, 得到<span class="math inline">\(g\)</span>. 这种每个特征组<span class="math inline">\({\bf x}_n\)</span>都有对应的<span class="math inline">\(y_n\)</span>的学习问题称作<strong>有监督学习</strong>. 这里&quot;监督&quot;的意义在于, 对每个特征都可以给出对应的标签, 是一种&quot;完整&quot;的教学.</p><h3 id="无监督学习">无监督学习</h3><p>如果对所有数据, 都没有标签给出, 机器也可以通过类似&quot;自学&quot;或者&quot;自己研究&quot;的方式将其归类. 这种学习问题称作&quot;无监督学习&quot;. 注意这种自动分类(称为聚类)的方法并不一定能得到正确的类数, 而且如何判读聚类结果的好坏也是一个难题. 这种问题类型的例子包括</p><ul><li>将文章按照主题归类</li><li>将用户聚合称为用户群</li></ul><p>当然, 无监督学习不止聚类这一种方向, 还包括了</p><ul><li>密度估计, 即给定<span class="math inline">\(X = \{ {\bf x}_1, \ldots, {\bf x}_n \}\)</span>, 判断哪里稠密哪里稀疏. 例如给定一些带有地点的交通事故资料, 判断哪里是事故多发区(有点像回归分析)</li><li>奇异点检测, 即给定<span class="math inline">\(X = \{ {\bf x}_1, \ldots, {\bf x}_n \}\)</span>, 判断哪些是异常点. 例如给定网络日志, 判断哪些是爬取日志(有点像极端情况下的二元分类)</li></ul><p>无监督学习的目标比较分散, 难以衡量算法的好坏</p><h3 id="半监督学习">半监督学习</h3><p>介于有监督学习和无监督学习之间, 只有少量数据有标签, 大部分标签都没有, 使用算法判断数据的类别. 这类问题的特点是标记数据很贵(标签获取不容易).</p><h3 id="强化学习">强化学习</h3><p>类似于训练宠物的方法: 当人们训练宠物时, 无法直接告诉它给定讯号<span class="math inline">\({\bf x}_n\)</span>以后期望的<span class="math inline">\(y_n\)</span>, 但是可以在它做了错误的回应以后施加惩罚, 通过这种方式告诉它这种做法是错的(当然也可以在它做了的正确的或者你不排斥的回应以后施加奖励). 即此时系统的输入包括了数据<span class="math inline">\(X\)</span>, 系统的行为<span class="math inline">\(\tilde{y}\)</span>和奖惩函数<span class="math inline">\(\rm goodness\)</span>. 这种问题类型的例子包括</p><ul><li>广告系统: <span class="math inline">\(X\)</span>为用户特征, <span class="math inline">\(\tilde{y}\)</span>为给出的广告, <span class="math inline">\(\rm goodness\)</span>是该广告的收入. 这样系统可以学到对给定用户应该给出什么广告</li><li>德州扑克: <span class="math inline">\(X\)</span>为牌型和底池, <span class="math inline">\(\tilde{y}\)</span>为叫牌策略, <span class="math inline">\(\rm goodness\)</span>是牌局结束后的收益. 这样系统可以学到对给定局面的叫牌策略</li></ul><p>强化学习实际上是从一些隐含的数据中学习, 这些数据通常是顺序的</p><h3 id="根据学习方式f-rightarrow-bf-x_n-y_n">根据学习方式<span class="math inline">\(f \Rightarrow ({\bf x}_n, y_n)\)</span></h3><h4 id="批处理学习">批处理学习</h4><p>读进所有的数据, 训练出来模型<span class="math inline">\(g\)</span>, 使用<span class="math inline">\(g\)</span>来处理未知数据. 批处理学习是最常见的一种学习方法</p><h4 id="在线学习">在线学习</h4><p>每看到一个样本就做出预测, 然后根据正确的标签对模型做出更新, 让模型的效果越来越好. 在线学习是一种循序渐进的学习方式. <span class="math inline">\(PLA\)</span>就很容易被改写为在线学习方法, 因为它看到一个错分样本就会更新权重. 另外, 强化学习通常都是在线学习, 因为每次我们只能得到部分数据.</p><h4 id="主动学习">主动学习</h4><p>批处理学习有点像填鸭式教育, 在线学习有点像老师课堂讲授, 但是这两种方式其实都是机器被动学习. 近几年一种新提出的研究方式类似于让机器来主动提问, 即对输入<span class="math inline">\({\bf x}_n\)</span>来提问对应的<span class="math inline">\(y_n\)</span>. 由于机器提问可以有技巧, 因此我们希望这种学习可以加速学习过程, 同时减少对标签的需求. 当标注样本比较昂贵的情况下, 主动学习比较有用</p><h3 id="根据输入空间mathcalx分类">根据输入空间<span class="math inline">\(\mathcal{X}\)</span>分类</h3><h4 id="具体特征">具体特征</h4><p>这种情况下, 输入<span class="math inline">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span>中的每个分量(称为<strong>特征</strong>)都有具体且复杂的物理意义. 这些特征都包含了人类的智慧, 称为&quot;领域知识&quot;, 是机器学习能处理的最简单的输入</p><h4 id="原始特征">原始特征</h4><p>假设要处理的是手写数字识别问题, 我们可以对输入做一些分析, 提取出具体特征, 包括数字是否对称, 笔画是否有弯折等等. 但是, 也可以直接将原始的每个像素的灰度值组合成一个256维向量(假设图片是<span class="math inline">\(16 \times 16\)</span>的). 这个输入比具体特征要抽象一些, 求解也更困难, 不过这些数据里仍然包含了一些简单的物理意义. 原始特征通常需要机器或者人来转换为具体特征, 这个转换的过程称为特征工程</p><h4 id="抽象特征">抽象特征</h4><p>以之前提到的预测用户对电影评分的比赛为例, 对于这个问题, 输入<span class="math inline">\(\mathcal{X} \subseteq \mathbb{N} \times \mathbb{N}\)</span>实际上是用户编号和电影编号组成的二元组, 而这些编号对机器来讲没有任何物理意义, 因此更加需要特征转换</p><blockquote><p>+参考[http://txshi-mt.com/2017/08/05/NTUML-3-Types-of-Learning/]</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;根据输出空间mathcaly分类&quot;&gt;根据输出空间&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{Y}\)&lt;/span&gt;分类&lt;/h3&gt;
    
    </summary>
    
      <category term="林轩田课程笔记" scheme="http://yoursite.com/categories/%E6%9E%97%E8%BD%A9%E7%94%B0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石 2.学习判断是与非</title>
    <link href="http://yoursite.com/2018/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-2-%E5%AD%A6%E4%B9%A0%E5%88%A4%E6%96%AD%E6%98%AF%E4%B8%8E%E9%9D%9E/"/>
    <id>http://yoursite.com/2018/09/10/机器学习基石-2-学习判断是与非/</id>
    <published>2018-09-10T15:34:08.000Z</published>
    <updated>2018-10-03T04:59:10.654Z</updated>
    
    <content type="html"><![CDATA[<h3 id="感知机假设集合">感知机假设集合</h3><a id="more"></a><p>第一章里讲到机器学习的核心就是, 使用算法<span class="math inline">\(\mathcal{A}\)</span>接受数据<span class="math inline">\(\mathcal{D}\)</span>, 从假设集合(所有可能性)<span class="math inline">\(\mathcal{H}\)</span>中选出一个<span class="math inline">\(g\)</span>, 希望<span class="math inline">\(g \approx f\)</span>. 那么我们现在最关心的就是, <span class="math inline">\(\mathcal{H}\)</span>应该是什么样的.</p><p>以之前提到的银行审核发放信用卡的场景为例, 假设我们把每个使用者定义为向量<span class="math inline">\(\bf x\)</span>, 包含<span class="math inline">\(d\)</span>个维度, 例如<span class="math inline">\(x_1\)</span>代表年龄, <span class="math inline">\(x_2\)</span>代表年薪, 等等. 我们可以将这些维度综合起来给使用者一个整体的分数. 如果这个分数超过了某个标准, 那么就发放信用卡; 否则拒绝发放. 这样, 我们需要给每个<span class="math inline">\(x_i, i \in \{ 1, \ldots, d \}\)</span>来赋一个系数<span class="math inline">\(w_i\)</span>, 如果特征对最后的影响是正面的, 那么就给<span class="math inline">\(w_i\)</span>正值, 否则给负值. 如果我们在规定一个阈值<span class="math inline">\(\rm threshold\)</span>, 那么我们的决策方法就可以写成为, 如果<span class="math inline">\(\sum_{i=1}^d w_ix_i &gt; \rm threshold\)</span>, 就批准信用卡申请, 否则就拒绝.</p><p>我们可以进一步地规定输出空间<span class="math inline">\(\mathcal{Y} \in \{-1, +1\}\)</span>, 其中<span class="math inline">\(y=-1\)</span>时表示拒绝, <span class="math inline">\(y=1\)</span>时表示许可. 这样做的好处是我们可以直接使用<span class="math inline">\(\rm sign\)</span>函数来求出<span class="math inline">\(y\)</span>的值, 具体地说, 假设集合<span class="math inline">\(\mathcal{H}\)</span>中的每个元素<span class="math inline">\(h \in \mathcal{H}\)</span>都有如下形式 <span class="math display">\[h({\bf x}) ={\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold})\]</span> 其中<span class="math inline">\({\rm sign}\)</span>函数的定义为 <span class="math display">\[{\rm sign}(x) = \begin{cases} +1 &amp; {\rm if \ } x&gt;0 \\ -1 &amp; {\rm if \ } x&lt;0 \end{cases}\]</span> 即对用户的所有属性做一个加权打分, 看它是否超过阈值. 如果超过, 则批准; 否则就拒绝(如果正好等于阈值, 这种情况很少发生, 甚至可以随机决定<span class="math inline">\(y\)</span>是<span class="math inline">\(-1\)</span>还是<span class="math inline">\(1\)</span>).</p><p>这里我们说<span class="math inline">\(\mathcal{H}\)</span>是一个集合的原因是, 不同的<span class="math inline">\(\bf w\)</span>和<span class="math inline">\(\rm threshold\)</span>都对应了不同的<span class="math inline">\(h\)</span>, 所有这些可能性对应的所有<span class="math inline">\(h\)</span>构成了最后的假设集合<span class="math inline">\(\mathcal{H}\)</span>. <span class="math inline">\(h\)</span>这样的函数类型称为<strong>感知机(perceptron)</strong>, 其中<span class="math inline">\(\bf w\)</span>称为权重. 进一步地, 假设我们把<span class="math inline">\(-\rm threshold\)</span>看做是<span class="math inline">\((-\rm threshold) \cdot (+1)\)</span>, 然后把<span class="math inline">\(+1\)</span>看作是<span class="math inline">\(x_0\)</span>, 那么前面的公式形式可以进一步的简化, 即 <span class="math display">\[\begin{align*}h({\bf x}) &amp;= {\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold}) \\&amp;= {\rm sign}((\sum_{i=1}^d w_ix_i)+\underbrace{(-{\rm threshold})}_{w_0}\cdot \underbrace{(+1)}_{x_0}) \\&amp;= {\rm sign}(\sum_{i=0}^d w_ix_i) \\&amp;= {\rm sign}({\bf w}^\mathsf{T}{\bf x}) \end{align*}\]</span></p><p>这里<span class="math inline">\(\bf w\)</span>和<span class="math inline">\(\bf x\)</span>都看作是列向量, 即维度为<span class="math inline">\((d+1)1\)</span></p><p>我们可以通过一个图例来加强理解. 假如我们顾客的特征数(也就是前面说的属性维度)为<span class="math inline">\(2\)</span>, 那么我们可以把任意输入<span class="math inline">\(\bf x\)</span>画在一个平面<span class="math inline">\(\mathbb{R}^2\)</span>上(类似的, 如果特征数为<span class="math inline">\(d\)</span>, 那么每个输入<span class="math inline">\(\bf x\)</span>都可以在<span class="math inline">\(\mathbb{R}^d\)</span>空间表示, 只是会对我们的可视化造成困难), 每个输入对应平面上的一个点. 这样, <span class="math inline">\(\mathbb{R}^2\)</span>上的<span class="math inline">\(h\)</span>都有如下形式: <span class="math display">\[h({\bf x}) = \rm sign(w_0+w_1x_1+w_2x_2)\]</span> 可以看出, 每个<span class="math inline">\(h\)</span>其实都对应了<span class="math inline">\(\mathbb{R}^2\)</span>上的一条直线. 感知机规定位于直线某一侧的样本都被判定为正例, 另一侧的样本都被判定为负例. 不同的权重会产生不同的分类方式. 假设我们用蓝色的圈o表示正例, 红色的叉×表示负例, 下图给出了两个不同的感知机</p><div class="figure"><img src="/2018/09/10/机器学习基石-2-学习判断是与非/1.png" alt="1"><p class="caption">1</p></div><div class="figure"><img src="/2018/09/10/机器学习基石-2-学习判断是与非/2.png" alt="2"><p class="caption">2</p></div><p>可以看出来右边的感知机在训练集上效果更好, 因为它对所有例子做出了正确分类. 而左侧的感知机在训练集上表现稍逊(一个正例被误判为负, 两个负例被误判为正)</p><p>由于感知机都对应于一个超平面, 因此它也被称为是<strong>线性分类器</strong>(<span class="math inline">\(\mathbb R^2\)</span>的超平面是一条直线, <span class="math inline">\(\mathbb R^3\)</span>的超平面是一个平面, 以此类推).</p><h3 id="感知机学习算法">感知机学习算法</h3><p>在我们知道了<span class="math inline">\(h \in \mathcal H\)</span>的形态以后, 接下来的问题是设计算法<span class="math inline">\(\mathcal A\)</span>来选出最优的<span class="math inline">\(g\)</span>来逼近理想的<span class="math inline">\(f\)</span>. 尽管我们不知道<span class="math inline">\(f\)</span>具体应该是什么, 但是我们知道数据<span class="math inline">\(\mathcal D\)</span>是由<span class="math inline">\(f\)</span>生成的. 因此我们有理由相信, 好的<span class="math inline">\(g\)</span>满足对所有我们已经收集道的数据, 其输出与<span class="math inline">\(f\)</span>的输出尽可能接近, 即<span class="math inline">\(g({\bf x}_n) = f({\bf x}_n) = y_n\)</span>. 因此, 我们可以先找一个超平面, 至少能够对训练集中的数据正确分类. 然而难度在于, <span class="math inline">\(\mathcal H\)</span>的大小通常都是无限的.</p><p>一种解决方案是, 我们可以先初始化一个超平面<span class="math inline">\(g_0\)</span>(为了简单起见, 将其以其权重<span class="math inline">\({\bf w}_0\)</span>代表, 称为初始权重). 我们允许这个超平面犯错, 但我们要设计算法, 让超平面遇到<span class="math inline">\(\mathcal D\)</span>中的错分样本以后可以修正自己. 通常我们可以将<span class="math inline">\({\bf w}_0\)</span>初始化为零向量<span class="math inline">\(\bf 0\)</span>. 然后, 在每一步<span class="math inline">\(t\)</span>, 找到一个使<span class="math inline">\({\bf w}_t\)</span>错分的样本错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>. 即有 <span class="math display">\[\rm sign({\bf w}^T_t {\bf x}_{n(t)}) \not= y_{n(t)}\]</span> 接下里我们试着修正<span class="math inline">\({\bf w}_t\)</span>. 可以看到错分有两种情况:</p><ul><li><p><span class="math inline">\(y\)</span>本来应该是<span class="math inline">\(+1\)</span>, 但是模型判断出来是负值. 也就是说此时<span class="math inline">\(\bf w\)</span>与<span class="math inline">\(\bf x\)</span>之间的角度太大, 因此需要把<span class="math inline">\(\bf w\)</span>往靠近<span class="math inline">\(\bf x\)</span>的方向旋转使它们的角度变小. 可以通过让<span class="math inline">\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf x}_{n(t)}\)</span>达到这个目的</p></li><li><p><span class="math inline">\(y\)</span>本来应该是<span class="math inline">\(-1\)</span>, 但是模型判断出来是正值. 也就是说此时<span class="math inline">\(\bf w\)</span>与<span class="math inline">\(\bf x\)</span>之间的角度太小, 因此需要把<span class="math inline">\(\bf w\)</span>往远离<span class="math inline">\(\bf x\)</span>的方向旋转使它们的角度变大. 考虑到符号, 其实也可以通过让<span class="math inline">\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf w}_{n(t)}\)</span>达到这个目的</p></li></ul><p>因此, 在<span class="math inline">\(t+1\)</span>时刻, 我们总可以通过下式来修正<span class="math inline">\({\bf w}_t\)</span>, 即 <span class="math display">\[{\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}\]</span></p><div class="figure"><img src="/2018/09/10/机器学习基石-2-学习判断是与非/3.png" alt="3"><p class="caption">3</p></div><div class="figure"><img src="/2018/09/10/机器学习基石-2-学习判断是与非/4.png" alt="4"><p class="caption">4</p></div><p>感知机学习算法(Perceptron Learning Algorithm, PLA)就是重复上面的过程, 直到没有错误发生为止. 算法将最后得到的权重<span class="math inline">\(\bf w\)</span>(记做<span class="math inline">\({\bf w}_{PLA}\)</span>)返回<span class="math inline">\(g\)</span>. 完整写法如下:</p><p>对于<span class="math inline">\(t = 0,1, \ldots\)</span></p><p>1). 找到一个使<span class="math inline">\({\bf w}_t\)</span>错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>. 即有 <span class="math display">\[sign({\bf w}_t^T {\bf x}_{n(t)}) \not = y_{n(t)}\]</span> 2). 以如下方法修正<span class="math inline">\({\bf w}_t\)</span>: <span class="math display">\[{\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}\]</span> 直到便利了所有样本一遍以后都没有找到错误为止.</p><p>由此, 也引出两个问题:</p><ul><li>算法真的会停止吗?</li><li>能否确定算法返回的<span class="math inline">\(g \approx f\)</span>?</li></ul><h3 id="感知机的有效性与确定终止性">感知机的有效性与确定终止性</h3><p>回顾PLA算法的停止条件, 它是在没有找到错误的时候才停止, 这要求我们的数据可以用一条线将正例样本和负例样本分割开来(如果不存在这条线, PLA肯定是不可能停止的). 这种条件叫做<strong>线性可分条件</strong>. 接下来, 我们需要证明: 如果数据集的确是线性可分的, 感知机是否总是能找到一个超平面把数据恰好分开.</p><p>假设数据集<span class="math inline">\(\mathcal D\)</span>线性可分, 我们先证明存在一个超平面<span class="math inline">\({\bf w}_f\)</span>使得任意<span class="math inline">\(i \in \{1, \ldots, n\}\)</span>, <span class="math inline">\(y_i = {\rm sign}({\bf w}_f^T{\bf x}_i)\)</span>. 这意味着对每个<span class="math inline">\({\bf x}_i\)</span>, 它与超平面都有一定距离, 即 <span class="math display">\[\min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n &gt; 0\]</span> 其中<span class="math inline">\({\bf w}_f^T{\bf x}_n\)</span>是点<span class="math inline">\({\bf w}_n\)</span>到<span class="math inline">\({\bf w}_f\)</span>的带符号的距离. 在训练过程中遇到的所有错分点<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>(假设在时刻<span class="math inline">\(t\)</span>遇到), 肯定有 <span class="math display">\[y_{n(t)}{\bf w}_f^\mathsf{T}{\bf x}_{n(t)} \ge \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n &gt; 0\]</span> 我们可以先证明, <span class="math inline">\({\bf w}_t\)</span>被<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>纠正以后更加接近纠正以后更加接近<span class="math inline">\({\bf w}_f\)</span>. 我们可以通过两个向量的內积来判断它们是否接近: 两个向量越接近, 內积越大(可以理解为两个向量<span class="math inline">\(\bf u\)</span>和和<span class="math inline">\(\bf v\)</span>越接近, 其夹角, 那么<span class="math inline">\(\cos \theta\)</span>越大, 所以两者的內积<span class="math inline">\({\bf u} \cdot {\bf v} = |\!|{\bf u}|\!||\!|{\bf v}|\!|\cos \theta\)</span>越大), 则 <span class="math display">\[\begin{align*}{\bf w}_f^\mathsf{T}{\bf w}_{t+1} &amp;= {\bf w}_t^\mathsf{T}({\bf w}_t + y_{n(t)}{\bf x}_{n(t)}) \\&amp;\ge {\bf w}_f^\mathsf{T}{\bf w}_t + \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n \\&amp;&gt; {\bf w}_f^\mathsf{T}{\bf w}_t  + 0 = {\bf w}_f^\mathsf{T}{\bf w}_t\end{align*}\]</span></p><p>但是这里又有一个新的问题, 即內积变大不一定说明两个向量接近, 因为向量长度变大也会导致內积变大. 因此接下来我们要证明, 修正<span class="math inline">\({\bf w}_t\)</span>以后新的权重长度不会发生太大的变化这里要用到一个性质即以后, 新的权重长度不会发生太大的变化. 这里要用到一个性质, 即<span class="math inline">\(PLA\)</span>仅在遇到错误的数据时才更新权重即如果权重仅在遇到错误的数据时才更新权重, 即如果权重<span class="math inline">\({\bf w}_t\)</span>被订正意味着被订正, 意味着<span class="math inline">\(sign({\bf w}_t^T{\bf w}_{n(t)}) \not = y_{n(t)}\)</span>也就是, 也就是<span class="math inline">\(y_{n(t)}{\bf w}_t^T{\bf x}_{n(t)} \le 0\)</span>考虑到. 考虑到<span class="math inline">\(y_{n(t)}\)</span>是标量且取值只可能为是标量, 且取值只可能为<span class="math inline">\(1\)</span>或<span class="math inline">\(-1\)</span>(即<span class="math inline">\(y_{n(t)}^2 = 1\)</span>), <span class="math inline">\({\bf w}_t^T{\bf x}_{n(t)}\)</span>也是标量, 因此 <span class="math display">\[\begin{align*}|\!|{\bf w}_{t+1}|\!|^2 &amp;= |\!|{\bf w}_t + y_{n(t)}{\bf x}_{n(t)}|\!|^2\end{align*}\]</span> 简记<span class="math inline">\(y = y_{n(t)}\)</span>, <span class="math inline">\({\bf x} = {\bf x}_{n(t)}\)</span>, <span class="math inline">\({\bf w} = {\bf w}_t\)</span>, 则 <span class="math display">\[\begin{align*}|\!|{\bf w}_{t+1}|\!|^2 &amp;= ({\bf w}+y{\bf x})^\mathsf{T}({\bf w}+y{\bf x}) \\&amp;= {\bf w}^\mathsf{T}{\bf w} + 2y{\bf w}^\mathsf{T}{\bf x} + {\bf x}^\mathsf{T}{\bf x} \\&amp;\le |\!|{\bf w}|\!|^2 + |\!|{\bf x}|\!|^2 \ \ \ \ \ \ \ \ \ \ \ \ \ (y{\bf w}^\mathsf{T}{\bf x} \le 0) \\&amp;\le |\!|{\bf w}|\!|^2 + \max_n |\!|{\bf x}_n|\!|^2\end{align*}\]</span> 即权重经过修正以后, 其长度最多增加<span class="math inline">\(\max_n |\!|{\bf x}_n|\!|^2\)</span></p><p>经由上面两部分, 假设权重的初始向量为<span class="math inline">\(\bf 0\)</span>, 我们可以求出经过<span class="math inline">\(T\)</span>步更新最后得到的权重<span class="math inline">\({\bf w}_{T}\)</span>与<span class="math inline">\(\bf w_{f}\)</span>之间的夹角余弦值的下界. 为了求这个值, 只需要求两个权重归一化以后內积的下界即可, 即 <span class="math display">\[\inf \left(\frac{ {\bf w}_f^\mathsf{T}} {|\!|{\bf w}_f|\!|} \cdot \frac{{\bf w}_T}{|\!|{\bf w}_T|\!|} \right)\]</span> 先看分子. 由于初始<span class="math inline">\({\bf w}_0 = {\bf 0}\)</span>, 因此由之前第一个证明的中间步骤, 我们可以写出第一次更新、第二次更新......后分子的下界, 即 <span class="math display">\[\begin{align*}{\bf w}_f^\mathsf{T}{\bf w}_1 &amp;\ge {\bf w}_f^\mathsf{T} \cdot {\bf 0} + \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \\{\bf w}_f^\mathsf{T}{\bf w}_2 &amp;\ge {\bf w}_f^\mathsf{T}\cdot{\bf w}_1 + \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \ge {\bf x}_f^\mathsf{T} \cdot {\bf 0} + 2\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \\&amp;\vdots \\{\bf w}_f^\mathsf{T}{\bf w}_T &amp;\ge T \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n\end{align*}\]</span></p><p>类似地, 对分母有 <span class="math display">\[|\!|{\bf w}_T|\!|^2 \le T\max_n|\!|{\bf x}_n|\!|^2\]</span> 因此, <span class="math display">\[\begin{align*}\frac{ {\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f|\!|} \cdot \frac{ {\bf w}_T}{|\!|{\bf w}_T|\!|} &amp;\ge \frac{T\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n}{|\!|{\bf w}_f|\!|\sqrt{T\max_n|\!|{\bf x}_n|\!|^2}} \\&amp;= \sqrt{T}\cdot \frac{\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n}{|\!|{\bf w}_f|\!|\sqrt{\max_n|\!|{\bf x}_n|\!|^2}}\end{align*}\]</span> 按照Fun TIme中的记法, 记<span class="math inline">\(R^2 = \max_n |\!|{\bf x}_n|\!|^2, \rho = \min_n y_n \frac{{\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f |\!|}{\bf x}_n\)</span>, 则 <span class="math display">\[\frac{ {\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f|\!|} \cdot \frac{ {\bf w}_T}{|\!|{\bf w}_T|\!|} \ge \sqrt{T}\cdot\frac{\rho}{R}\]</span> 由向量除以其长度得到的是单位向量, 长度为<span class="math inline">\(1\)</span>, 在这种情况下, 两者內积越大一定意味着两者夹角越小, 距离越近. 但是这里需要注意的是, 两者的距离不会无限接近, 到<span class="math inline">\(\cos \theta = 1\)</span>时会停止, 因为两个单位向量的內积最大值为1, 因此从上面的不等式可推出 <span class="math display">\[\sqrt{T} \cdot \frac{\rho}{R} \le 1 \Rightarrow T \le \frac{R^2}{\rho^2}\]</span> 即算法至多更新<span class="math inline">\(\frac{R^2}{\rho^2}\)</span>步后一定会停止.</p><h3 id="感知机在线性不可分数据上的应用">感知机在线性不可分数据上的应用</h3><p>由上面的证明, 假设数据集是线性可分的, 那么<span class="math inline">\(PLA\)</span>算法最后肯定会停止, 而且(对训练集)给出正确的分类. 该算法非常容易实现, 而且结束很快, 使用于任意<span class="math inline">\(\mathbb{R}^d\)</span>空间. 但是这个算法最大的问题, 它提前假设训练集是训练可分的, 而且我们不知道算法什么时候会终止(因为上面给出的上限中用到了<span class="math inline">\({\bf w}_f\)</span>, 而我们不知道它是多少--甚至不知道是否存在!(在线性不可分的时候该向量不存在))</p><p>那么我们来考虑一个最坏的情况, 即数据若的确是线性不可分的话, 应该如何应对. 由于数据产生的过程中可能会混入噪声, 这使得原本线性可分的数据也可能因为噪声的存在而不可分. 但是, 一般情况下, 噪声应该是一小部分, 即我们可以退而且其次, 不去寻找一个完美的超平面, 而是去寻找一个犯错误最少的超平面, 即 <span class="math display">\[{\bf w}_g \leftarrow \mathop{{\rm arg}\min}_{\bf w} \sum_{n=1}^N [\![ y_n \not = {\rm sign}({\bf w}^\mathsf{T}{\bf x}_n)]\!]\]</span> 然而, 求解这个问题被证明是NP难的, 只能采用近似算法求解. 例如, 我们可以保存一个最好的权重, 该权重到目前为止错分的数量最少. 该算法被称为&quot;口袋法&quot;, 其完整细节如下:</p><p>设定初始权重<span class="math inline">\(\hat{\bf w}\)</span></p><p>对时刻<span class="math inline">\(t=0, 1, \cdots\)</span></p><ol style="list-style-type: decimal"><li><p>随机寻找一个<span class="math inline">\({\bf w}_t\)</span>错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span></p></li><li><p>试图通过如下方法修正<span class="math inline">\({\bf w}_t\)</span> <span class="math display">\[   {\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}   \]</span></p></li><li><p>如果<span class="math inline">\({\bf w}_{t+1}\)</span>犯的错误比<span class="math inline">\(\hat{\bf w}\)</span>少, 那么将<span class="math inline">\(\hat{\bf w}\)</span>替换为<span class="math inline">\({\bf w}_{t+1}\)</span></p></li></ol><p>直到足够多次迭代完成. 我们将<span class="math inline">\(\hat{\bf w}\)</span>(称为<span class="math inline">\({\bf w}_{pocket}\)</span>)返回为<span class="math inline">\(g\)</span></p><p>注意在线性可分集合上也可以使用口袋法, 算法也可以返回一个无训练误差的解. 但是由于每次更新权重以后, 都要在所有数据上使用新旧权重各跑一遍, 来计算错分数量, 因此口袋法的执行时间通常比原始<span class="math inline">\(PLA\)</span>的计算时间长很多.</p><blockquote><p>+参考[http://txshi-mt.com/2017/08/03/NTUML-2-Learning-to-Answer-Yes-No/]</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;感知机假设集合&quot;&gt;感知机假设集合&lt;/h3&gt;
    
    </summary>
    
      <category term="林轩田课程笔记" scheme="http://yoursite.com/categories/%E6%9E%97%E8%BD%A9%E7%94%B0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石 1.学习问题</title>
    <link href="http://yoursite.com/2018/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-1-%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/09/09/机器学习基石-1-学习问题/</id>
    <published>2018-09-09T03:48:35.000Z</published>
    <updated>2018-10-03T04:58:34.394Z</updated>
    
    <content type="html"><![CDATA[<h3 id="机器学习的概念">机器学习的概念</h3><a id="more"></a><p>我们可以从人类的学习思维入手. 人类的学习过程, 是从观察出发, 经过大脑内化以后, 变成有用的技巧. 机器学习, 类似地, 是我们希望让计算机模拟人类的学习过程. 这时, 计算机观察到的东西被称作<strong>数据</strong>, 而思考过程实际上是<strong>计算过程</strong>, 技巧则是<strong>提高某一方面的表现</strong>. 因此,</p><div class="figure"><img src="/2018/09/09/机器学习基石-1-学习问题/1.png" alt="机器学习的过程"><p class="caption">机器学习的过程</p></div><p>为什么需要机器学习?</p><p>给定一张照片, 判断照片里的物体是不是一棵树. 使用传统的方法, 就需要对&quot;什么是树&quot;做出回答, 给出树的定义, 将其实现为程序. 按照规则进行判断, 并将其表述出来是很困难的. 然而, 一个小孩认识树的方法其实是通过观察, 经过经验的积累判断这个是树或者不是, 并不是教条的从长辈那里学习判断规则. 类似地, 我们可以让计算机自己从数据中学习树的判断方法. 因此, <strong>机器学习是构建复杂系统的一种方法</strong></p><p>机器学习的使用场景</p><ul><li><p>当我们不能提前想好各种情况, 手工编码规则时. 例如让机器人在火星上导航, 我们不可能提前想到它在火星上会遇到什么样的情况</p></li><li><p>当我们无法轻易地定义问题的解决方案时. 例如要做语音识别/视觉识别, 我们无法对音频信号做出准确定义</p></li><li><p>当人们需要做出快速决策时. 例如高频交易</p></li><li><p>当要让机器服务于海量用户时. 例如做服务个性化定制</p></li></ul><p>因此, 可以从以下三个关键点进行判断, 看是否适合使用机器学习</p><ol style="list-style-type: decimal"><li>问题是&quot;可以学习的&quot;, 即存在一些潜在的模式, 以至于性能可以被提高</li><li>这些规则难以清晰定义</li><li>手里掌握对应的数据</li></ol><h3 id="机器学习的应用">机器学习的应用</h3><p>机器学习在衣食住行四个方面都得到了广泛地应用</p><ul><li><p>衣: Abu-Mostafa 2012利用销售数据和对用户的调研结果构建推荐系统给用户推荐穿搭</p></li><li><p>食: Sadilek et al. 2013利用机器学习, 根据Twitter数据, 来判断餐厅的好坏</p></li><li><p>住: Tsanas and Xifara 2012利用已有房间的特点和耗能, 预测房屋的能用消耗</p></li></ul><p>此外还有两个领域: 教育和娱乐</p><ul><li>教育: 系统根据学生的答题情况, 有针对地提供题目让学生练习其薄弱的部分, 同时将太难的题推后给出. 即, 给定一名学生的答题历史和一个题目, 预测学生是否能做对这道题( KDDCup 2010 )</li><li>娱乐: 系统根据用户的历史打分, 预测用户对新电影的打分( KDDCup 2011 )</li></ul><h3 id="机器学习的过程">机器学习的过程</h3><h4 id="问题背景">问题背景</h4><p>以银行信用卡发卡这一问题为例. 假设银行收集了一些用户的基本信息, 例如下表</p><table><thead><tr class="header"><th>特征</th><th>值</th></tr></thead><tbody><tr class="odd"><td>年龄</td><td>23</td></tr><tr class="even"><td>性别</td><td>女</td></tr><tr class="odd"><td>所在地居住年数</td><td>1</td></tr><tr class="even"><td>工龄</td><td>0.5</td></tr><tr class="odd"><td>负债额</td><td>200,000</td></tr></tbody></table><p>银行要解决的问题是, 对于这样的客户, 是否应该给她发放信用卡</p><h4 id="问题形式化描述">问题形式化描述</h4><p>为了更加形式化地描述这个问题, 需要先定义一些符号:</p><ul><li><strong>输入</strong>: <span class="math inline">\({\bf x} \in \mathcal{X}\)</span>, 用户的特征</li><li><strong>输出</strong>: <span class="math inline">\({\bf y} \in \mathcal{Y}\)</span>, 是否发放信用卡</li><li><strong>目标函数</strong>: <span class="math inline">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span>, 是我们期望学到, 但是目前不知道的东西. 是最理想的公式</li><li><strong>数据</strong>: <span class="math inline">\(\mathcal{D} = \{({\bf x}_1, y_1), ({\bf x}_2, y_2), \ldots, ({\bf x}_n, y_n)\}\)</span>, 是之前积累的记录</li><li><strong>假设</strong>: <span class="math inline">\(g: \mathcal{X} \rightarrow \mathcal{Y}\)</span>, 是机器从数据中学到的函数. 我们通常都希望<span class="math inline">\(g\)</span>的表现足够好, 即<span class="math inline">\(g \approx f\)</span>. 注意这里<span class="math inline">\(g\)</span>不一定等于<span class="math inline">\(f\)</span>(实际上, 我们永远也无法知道真正的<span class="math inline">\(f\)</span>是什么样子, 只知道由<span class="math inline">\(f\)</span>产生的数据<span class="math inline">\(\mathcal{D}\)</span>)</li><li><strong>机器学习算法</strong>: <span class="math inline">\(\mathcal {A}\)</span>, 是由<span class="math inline">\(\mathcal {D}\)</span>产生<span class="math inline">\(g\)</span>的算法, 可以理解为<span class="math inline">\(\mathcal {A}\)</span>会从各种不同假设<span class="math inline">\(h_k\)</span>(这里<span class="math inline">\(h_k\)</span>有好有坏)构成的集合<span class="math inline">\(\mathcal{H}\)</span>中挑选出来一个最好的<span class="math inline">\(g\)</span>, 使得<span class="math inline">\(g \approx f\)</span>. 即<span class="math inline">\(\mathcal{A}\)</span>以<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(\mathcal{H}\)</span>为输入, 以<span class="math inline">\(g\)</span>为输出</li></ul><div class="figure"><img src="/2018/09/09/机器学习基石-1-学习问题/2.png" alt="机器学习过程"><p class="caption">机器学习过程</p></div><p><strong>我们所讲的机器学习模型, 指的就是</strong><span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{H}\)</span></p><p>在有个这些记号以后, 我们可以重新给机器学习下一个定义</p><blockquote><p>机器学习是使用数据计算假设<span class="math inline">\(g\)</span>以逼近目标函数<span class="math inline">\(f\)</span>的过程</p></blockquote><h3 id="机器学习与其它名词">机器学习与其它名词</h3><h4 id="机器学习与数据挖掘">机器学习与数据挖掘</h4><p>数据挖掘的一个简单定义是使用海量数据, 在其中找出一些有趣的现象或性质. 这里, 如果&quot;有用的性质&quot;就是&quot;能够逼近目标函数的假设&quot;, 那么数据挖掘和机器学习是没有区别的. 如果&quot;有用的性质&quot;与&quot;假设&quot;是相关联的, 那么数据挖掘在很大程度上可以帮助机器学习</p><p>传统上的数据挖掘还关注如何在大的数据集中进行有效计算, 不过现在已经很难将机器学习和数据挖掘这两个概念分开了.</p><h4 id="机器学习与人工智能">机器学习与人工智能</h4><p>人工智能要求计算机呈现出一些智能的行为. 由于机器学习逼近目标函数的过程就展示了一些智能, 因此我们可以说, 机器学习是实现人工智能的一种手段.</p><h4 id="机器学习与统计学">机器学习与统计学</h4><p>统计学是使用数据来对未知过程进行推论. 考虑到假设<span class="math inline">\(g\)</span>是推论结果, <span class="math inline">\(f\)</span>是不知道的事, 那么可以说统计是实现机器学习的一种方法. 但是传统的统计学从数学出发, 很多工具是为数学假设提供证明和推论. 而机器学习看重的是如何计算出结果. 总而言之, 统计学为机器学习提供了很多有力的工具</p><blockquote><p>+参考[http://txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/]</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;机器学习的概念&quot;&gt;机器学习的概念&lt;/h3&gt;
    
    </summary>
    
      <category term="林轩田课程笔记" scheme="http://yoursite.com/categories/%E6%9E%97%E8%BD%A9%E7%94%B0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
