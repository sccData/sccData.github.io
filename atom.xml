<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>scc技术博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-22T06:36:23.715Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>史崇辰</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习基石 2.学习判断是与非</title>
    <link href="http://yoursite.com/2018/09/10/%E5%8E%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-2-%E5%AD%A6%E4%B9%A0%E5%88%A4%E6%96%AD%E6%98%AF%E4%B8%8E%E9%9D%9E/"/>
    <id>http://yoursite.com/2018/09/10/原机器学习基石-2-学习判断是与非/</id>
    <published>2018-09-10T15:34:08.000Z</published>
    <updated>2018-09-22T06:36:23.715Z</updated>
    
    <content type="html"><![CDATA[<h3 id="感知机假设集合">感知机假设集合</h3><a id="more"></a><p>第一章里讲到机器学习的核心就是, 使用算法<span class="math inline">\(\mathcal{A}\)</span>接受数据<span class="math inline">\(\mathcal{D}\)</span>, 从假设集合(所有可能性)<span class="math inline">\(\mathcal{H}\)</span>中选出一个<span class="math inline">\(g\)</span>, 希望<span class="math inline">\(g \approx f\)</span>. 那么我们现在最关心的就是, <span class="math inline">\(\mathcal{H}\)</span>应该是什么样的.</p><p>以之前提到的银行审核发放信用卡的场景为例, 假设我们把每个使用者定义为向量<span class="math inline">\(\bf x\)</span>, 包含<span class="math inline">\(d\)</span>个维度, 例如<span class="math inline">\(x_1\)</span>代表年龄, <span class="math inline">\(x_2\)</span>代表年薪, 等等. 我们可以将这些维度综合起来给使用者一个整体的分数. 如果这个分数超过了某个标准, 那么就发放信用卡; 否则拒绝发放. 这样, 我们需要给每个<span class="math inline">\(x_i, i \in \{ 1, \ldots, d \}\)</span>来赋一个系数<span class="math inline">\(w_i\)</span>, 如果特征对最后的影响是正面的, 那么就给<span class="math inline">\(w_i\)</span>正值, 否则给负值. 如果我们在规定一个阈值<span class="math inline">\(\rm threshold\)</span>, 那么我们的决策方法就可以写成为, 如果<span class="math inline">\(\sum_{i=1}^d w_ix_i &gt; \rm threshold\)</span>, 就批准信用卡申请, 否则就拒绝.</p><p>我们可以进一步地规定输出空间<span class="math inline">\(\mathcal{Y} \in \{-1, +1\}\)</span>, 其中<span class="math inline">\(y=-1\)</span>时表示拒绝, <span class="math inline">\(y=1\)</span>时表示许可. 这样做的好处是我们可以直接使用<span class="math inline">\(\rm sign\)</span>函数来求出<span class="math inline">\(y\)</span>的值, 具体地说, 假设集合<span class="math inline">\(\mathcal{H}\)</span>中的每个元素<span class="math inline">\(h \in \mathcal{H}\)</span>都有如下形式 <span class="math display">\[h({\bf x}) ={\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold})\]</span> 其中<span class="math inline">\({\rm sign}\)</span>函数的定义为 <span class="math display">\[{\rm sign}(x) = \begin{cases} +1 &amp; {\rm if \ } x&gt;0 \\ -1 &amp; {\rm if \ } x&lt;0 \end{cases}\]</span> 即对用户的所有属性做一个加权打分, 看它是否超过阈值. 如果超过, 则批准; 否则就拒绝(如果正好等于阈值, 这种情况很少发生, 甚至可以随机决定<span class="math inline">\(y\)</span>是<span class="math inline">\(-1\)</span>还是<span class="math inline">\(1\)</span>).</p><p>这里我们说<span class="math inline">\(\mathcal{H}\)</span>是一个集合的原因是, 不同的<span class="math inline">\(\bf w\)</span>和<span class="math inline">\(\rm threshold\)</span>都对应了不同的<span class="math inline">\(h\)</span>, 所有这些可能性对应的所有<span class="math inline">\(h\)</span>构成了最后的假设集合<span class="math inline">\(\mathcal{H}\)</span>. <span class="math inline">\(h\)</span>这样的函数类型称为<strong>感知机(perceptron)</strong>, 其中<span class="math inline">\(\bf w\)</span>称为权重. 进一步地, 假设我们把<span class="math inline">\(-\rm threshold\)</span>看做是<span class="math inline">\((-\rm threshold) \cdot (+1)\)</span>, 然后把<span class="math inline">\(+1\)</span>看作是<span class="math inline">\(x_0\)</span>, 那么前面的公式形式可以进一步的简化, 即 <span class="math display">\[\begin{align*}h({\bf x}) &amp;= {\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold}) \\&amp;= {\rm sign}((\sum_{i=1}^d w_ix_i)+\underbrace{(-{\rm threshold})}_{w_0}\cdot \underbrace{(+1)}_{x_0}) \\&amp;= {\rm sign}(\sum_{i=0}^d w_ix_i) \\&amp;= {\rm sign}({\bf w}^\mathsf{T}{\bf x}) \end{align*}\]</span></p><p>这里<span class="math inline">\(\bf w\)</span>和<span class="math inline">\(\bf x\)</span>都看作是列向量, 即维度为<span class="math inline">\((d+1)1\)</span></p><p>我们可以通过一个图例来加强理解. 假如我们顾客的特征数(也就是前面说的属性维度)为<span class="math inline">\(2\)</span>, 那么我们可以把任意输入<span class="math inline">\(\bf x\)</span>画在一个平面<span class="math inline">\(\mathbb{R}^2\)</span>上(类似的, 如果特征数为<span class="math inline">\(d\)</span>, 那么每个输入<span class="math inline">\(\bf x\)</span>都可以在<span class="math inline">\(\mathbb{R}^d\)</span>空间表示, 只是会对我们的可视化造成困难), 每个输入对应平面上的一个点. 这样, <span class="math inline">\(\mathbb{R}^2\)</span>上的<span class="math inline">\(h\)</span>都有如下形式: <span class="math display">\[h({\bf x}) = \rm sign(w_0+w_1x_1+w_2x_2)\]</span> 可以看出, 每个<span class="math inline">\(h\)</span>其实都对应了<span class="math inline">\(\mathbb{R}^2\)</span>上的一条直线. 感知机规定位于直线某一侧的样本都被判定为正例, 另一侧的样本都被判定为负例. 不同的权重会产生不同的分类方式. 假设我们用蓝色的圈o表示正例, 红色的叉×表示负例, 下图给出了两个不同的感知机</p><div class="figure"><img src="/2018/09/10/原机器学习基石-2-学习判断是与非/1.png" alt="1"><p class="caption">1</p></div><div class="figure"><img src="/2018/09/10/原机器学习基石-2-学习判断是与非/2.png" alt="2"><p class="caption">2</p></div><p>可以看出来右边的感知机在训练集上效果更好, 因为它对所有例子做出了正确分类. 而左侧的感知机在训练集上表现稍逊(一个正例被误判为负, 两个负例被误判为正)</p><p>由于感知机都对应于一个超平面, 因此它也被称为是<strong>线性分类器</strong>(<span class="math inline">\(\mathbb R^2\)</span>的超平面是一条直线, <span class="math inline">\(\mathbb R^3\)</span>的超平面是一个平面, 以此类推).</p><h3 id="感知机学习算法">感知机学习算法</h3><p>在我们知道了<span class="math inline">\(h \in \mathcal H\)</span>的形态以后, 接下来的问题是设计算法<span class="math inline">\(\mathcal A\)</span>来选出最优的<span class="math inline">\(g\)</span>来逼近理想的<span class="math inline">\(f\)</span>. 尽管我们不知道<span class="math inline">\(f\)</span>具体应该是什么, 但是我们知道数据<span class="math inline">\(\mathcal D\)</span>是由<span class="math inline">\(f\)</span>生成的. 因此我们有理由相信, 好的<span class="math inline">\(g\)</span>满足对所有我们已经收集道的数据, 其输出与<span class="math inline">\(f\)</span>的输出尽可能接近, 即<span class="math inline">\(g({\bf x}_n) = f({\bf x}_n) = y_n\)</span>. 因此, 我们可以先找一个超平面, 至少能够对训练集中的数据正确分类. 然而难度在于, <span class="math inline">\(\mathcal H\)</span>的大小通常都是无限的.</p><p>一种解决方案是, 我们可以先初始化一个超平面<span class="math inline">\(g_0\)</span>(为了简单起见, 将其以其权重<span class="math inline">\({\bf w}_0\)</span>代表, 称为初始权重). 我们允许这个超平面犯错, 但我们要设计算法, 让超平面遇到<span class="math inline">\(\mathcal D\)</span>中的错分样本以后可以修正自己. 通常我们可以将<span class="math inline">\({\bf w}_0\)</span>初始化为零向量<span class="math inline">\(\bf 0\)</span>. 然后, 在每一步<span class="math inline">\(t\)</span>, 找到一个使<span class="math inline">\({\bf w}_t\)</span>错分的样本错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>. 即有 <span class="math display">\[\rm sign({\bf w}^T_t {\bf x}_{n(t)}) \not= y_{n(t)}\]</span> 接下里我们试着修正<span class="math inline">\({\bf w}_t\)</span>. 可以看到错分有两种情况:</p><ul><li><p><span class="math inline">\(y\)</span>本来应该是<span class="math inline">\(+1\)</span>, 但是模型判断出来是负值. 也就是说此时<span class="math inline">\(\bf w\)</span>与<span class="math inline">\(\bf x\)</span>之间的角度太大, 因此需要把<span class="math inline">\(\bf w\)</span>往靠近<span class="math inline">\(\bf x\)</span>的方向旋转使它们的角度变小. 可以通过让<span class="math inline">\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf x}_{n(t)}\)</span>达到这个目的</p></li><li><p><span class="math inline">\(y\)</span>本来应该是<span class="math inline">\(-1\)</span>, 但是模型判断出来是正值. 也就是说此时<span class="math inline">\(\bf w\)</span>与<span class="math inline">\(\bf x\)</span>之间的角度太小, 因此需要把<span class="math inline">\(\bf w\)</span>往远离<span class="math inline">\(\bf x\)</span>的方向旋转使它们的角度变大. 考虑到符号, 其实也可以通过让<span class="math inline">\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf w}_{n(t)}\)</span>达到这个目的</p></li></ul><p>因此, 在<span class="math inline">\(t+1\)</span>时刻, 我们总可以通过下式来修正<span class="math inline">\({\bf w}_t\)</span>, 即 <span class="math display">\[{\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}\]</span></p><div class="figure"><img src="/2018/09/10/原机器学习基石-2-学习判断是与非/3.png" alt="3"><p class="caption">3</p></div><div class="figure"><img src="/2018/09/10/原机器学习基石-2-学习判断是与非/4.png" alt="4"><p class="caption">4</p></div><p>感知机学习算法(Perceptron Learning Algorithm, PLA)就是重复上面的过程, 直到没有错误发生为止. 算法将最后得到的权重<span class="math inline">\(\bf w\)</span>(记做<span class="math inline">\({\bf w}_{PLA}\)</span>)返回<span class="math inline">\(g\)</span>. 完整写法如下:</p><p>对于<span class="math inline">\(t = 0,1, \ldots\)</span></p><p>1). 找到一个使<span class="math inline">\({\bf w}_t\)</span>错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>. 即有 <span class="math display">\[sign({\bf w}_t^T {\bf x}_{n(t)}) \not = y_{n(t)}\]</span> 2). 以如下方法修正<span class="math inline">\({\bf w}_t\)</span>: <span class="math display">\[{\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}\]</span> 直到便利了所有样本一遍以后都没有找到错误为止.</p><p>由此, 也引出两个问题:</p><ul><li>算法真的会停止吗?</li><li>能否确定算法返回的<span class="math inline">\(g \approx f\)</span>?</li></ul><h3 id="感知机的有效性与确定终止性">感知机的有效性与确定终止性</h3><p>回顾PLA算法的停止条件, 它是在没有找到错误的时候才停止, 这要求我们的数据可以用一条线将正例样本和负例样本分割开来(如果不存在这条线, PLA肯定是不可能停止的). 这种条件叫做<strong>线性可分条件</strong>. 接下来, 我们需要证明: 如果数据集的确是线性可分的, 感知机是否总是能找到一个超平面把数据恰好分开.</p><p>假设数据集<span class="math inline">\(\mathcal D\)</span>线性可分, 我们先证明存在一个超平面<span class="math inline">\({\bf w}_f\)</span>使得任意<span class="math inline">\(i \in \{1, \ldots, n\}\)</span>, <span class="math inline">\(y_i = {\rm sign}({\bf w}_f^T{\bf x}_i)\)</span>. 这意味着对每个<span class="math inline">\({\bf x}_i\)</span>, 它与超平面都有一定距离, 即 <span class="math display">\[\min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n &gt; 0\]</span> 其中<span class="math inline">\({\bf w}_f^T{\bf x}_n\)</span>是点<span class="math inline">\({\bf w}_n\)</span>到<span class="math inline">\({\bf w}_f\)</span>的带符号的距离. 在训练过程中遇到的所有错分点<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>(假设在时刻<span class="math inline">\(t\)</span>遇到), 肯定有 <span class="math display">\[y_{n(t)}{\bf w}_f^\mathsf{T}{\bf x}_{n(t)} \ge \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n &gt; 0\]</span> 我们可以先证明, <span class="math inline">\({\bf w}_t\)</span>被<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>纠正以后更加接近纠正以后更加接近<span class="math inline">\({\bf w}_f\)</span>. 我们可以通过两个向量的內积来判断它们是否接近: 两个向量越接近, 內积越大(可以理解为两个向量<span class="math inline">\(\bf u\)</span>和和<span class="math inline">\(\bf v\)</span>越接近, 其夹角, 那么<span class="math inline">\(\cos \theta\)</span>越大, 所以两者的內积<span class="math inline">\({\bf u} \cdot {\bf v} = |\!|{\bf u}|\!||\!|{\bf v}|\!|\cos \theta\)</span>越大), 则 <span class="math display">\[\begin{align*}{\bf w}_f^\mathsf{T}{\bf w}_{t+1} &amp;= {\bf w}_t^\mathsf{T}({\bf w}_t + y_{n(t)}{\bf x}_{n(t)}) \\&amp;\ge {\bf w}_f^\mathsf{T}{\bf w}_t + \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n \\&amp;&gt; {\bf w}_f^\mathsf{T}{\bf w}_t + 0 = {\bf w}_f^\mathsf{T}{\bf w}_t\hspace{3ex} \blacksquare\end{align*}\]</span></p><p>但是这里又有一个新的问题, 即內积变大不一定说明两个向量接近, 因为向量长度变大也会导致內积变大. 因此接下来我们要证明, 修正<span class="math inline">\({\bf w}_t\)</span>以后, 新的权重长度不会发生太大的变化. 这里要用到一个性质, 即<span class="math inline">\(PLA\)</span>仅在遇到错误的数据时才更新权重, 即如果权重<span class="math inline">\({\bf w}_t\)</span>被订正, 意味着<span class="math inline">\(sign({\bf w}_t^T{\bf w}_{n(t)}) \not = y_{n(t)}\)</span>, 也就是<span class="math inline">\(y_{n(t)}{\bf w}_t^T{\bf x}_{n(t)} \le 0\)</span>. 考虑到<span class="math inline">\(y_{n(t)}\)</span>是标量, 且取值只可能为<span class="math inline">\(1\)</span>或<span class="math inline">\(-1\)</span>(即<span class="math inline">\(y_{n(t)}^2 = 1\)</span>), <span class="math inline">\({\bf w}_t^T{\bf x}_{n(t)}\)</span>也是标量, 因此 <span class="math display">\[\begin{align*}|\!|{\bf w}_{t+1}|\!|^2 &amp;= |\!|{\bf w}_t + y_{n(t)}{\bf x}_{n(t)}|\!|^2\end{align*}\]</span> 简记<span class="math inline">\(y = y_{n(t)}\)</span>, <span class="math inline">\({\bf x} = {\bf x}_{n(t)}\)</span>, <span class="math inline">\({\bf w} = {\bf w}_t\)</span>, 则 <span class="math display">\[\begin{align*}|\!|{\bf w}_{t+1}|\!|^2 &amp;= ({\bf w}+y{\bf x})^\mathsf{T}({\bf w}+y{\bf x}) \\&amp;= {\bf w}^\mathsf{T}{\bf w} + 2y{\bf w}^\mathsf{T}{\bf x} + {\bf x}^\mathsf{T}{\bf x} \\&amp;\le |\!|{\bf w}|\!|^2 + |\!|{\bf x}|\!|^2 \hspace{3ex}(\because y{\bf w}^\mathsf{T}{\bf x} \le 0) \\&amp;\le |\!|{\bf w}|\!|^2 + \max_n|\!|{\bf x}_n|\!|^2\end{align*}\]</span> 即权重经过修正以后, 其长度最多增加<span class="math inline">\(\max_n |\!|{\bf x}_n|\!|^2\)</span></p><p>经由上面两部分, 假设权重的初始向量为<span class="math inline">\(\bf 0\)</span>, 我们可以求出经过<span class="math inline">\(T\)</span>步更新最后得到的权重<span class="math inline">\({\bf w}_{T}\)</span>与<span class="math inline">\(\bf w_{f}\)</span>之间的夹角余弦值的下界. 为了求这个值, 只需要求两个权重归一化以后內积的下界即可, 即 <span class="math display">\[\inf \left(\frac{ {\bf w}_f^\mathsf{T}} {|\!|{\bf w}_f|\!|} \cdot \frac{{\bf w}_T}{|\!|{\bf w}_T|\!|} \right)\]</span> 先看分子. 由于初始<span class="math inline">\({\bf w}_0 = {\bf 0}\)</span>, 因此由之前第一个证明的中间步骤, 我们可以写出第一次更新、第二次更新......后分子的下界, 即 <span class="math display">\[\begin{align*}{\bf w}_f^\mathsf{T}{\bf w}_1 &amp;\ge {\bf w}_f^\mathsf{T} \cdot {\bf 0} + \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \\{\bf w}_f^\mathsf{T}{\bf w}_2 &amp;\ge {\bf w}_f^\mathsf{T}\cdot{\bf w}_1 + \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \ge {\bf x}_f^\mathsf{T} \cdot {\bf 0} + 2\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \\&amp;\vdots \\{\bf w}_f^\mathsf{T}{\bf w}_T &amp;\ge T \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n\end{align*}\]</span></p><p>类似地, 对分母有 <span class="math display">\[|\!|{\bf w}_T|\!|^2 \le T\max_n|\!|{\bf x}_n|\!|^2\]</span> 因此, <span class="math display">\[\begin{align*}\frac{ {\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f|\!|} \cdot \frac{ {\bf w}_T}{|\!|{\bf w}_T|\!|} &amp;\ge \frac{T\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n}{|\!|{\bf w}_f|\!|\sqrt{T\max_n|\!|{\bf x}_n|\!|^2}} \\&amp;= \sqrt{T}\cdot \frac{\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n}{|\!|{\bf w}_f|\!|\sqrt{\max_n|\!|{\bf x}_n|\!|^2}}\end{align*}\]</span> 按照Fun TIme中的记法, 记<span class="math inline">\(R^2 = \max_n |\!|{\bf x}_n|\!|^2, \rho = \min_n y_n \frac{{\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f |\!|}{\bf x}_n\)</span>, 则 <span class="math display">\[\frac{ {\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f|\!|} \cdot \frac{ {\bf w}_T}{|\!|{\bf w}_T|\!|} \ge \sqrt{T}\cdot\frac{\rho}{R}\]</span> 由向量除以其长度得到的是单位向量, 长度为<span class="math inline">\(1\)</span>, 在这种情况下, 两者內积越大一定意味着两者夹角越小, 距离越近. 但是这里需要注意的是, 两者的距离不会无限接近, 到<span class="math inline">\(\cos \theta = 1\)</span>时会停止, 因为两个单位向量的內积最大值为1, 因此从上面的不等式可推出 <span class="math display">\[\sqrt{T} \cdot \frac{\rho}{R} \le 1 \Rightarrow T \le \frac{R^2}{\rho^2}\]</span> 即算法至多更新<span class="math inline">\(\frac{R^2}{\rho^2}\)</span>步后一定会停止.</p><h3 id="感知机在线性不可分数据上的应用">感知机在线性不可分数据上的应用</h3><p>由上面的证明, 假设数据集是线性可分的, 那么<span class="math inline">\(PLA\)</span>算法最后肯定会停止, 而且(对训练集)给出正确的分类. 该算法非常容易实现, 而且结束很快, 使用于任意<span class="math inline">\(\mathbb{R}^d\)</span>空间. 但是这个算法最大的问题, 它提前假设训练集是训练可分的, 而且我们不知道算法什么时候会终止(因为上面给出的上限中用到了<span class="math inline">\({\bf w}_f\)</span>, 而我们不知道它是多少--甚至不知道是否存在!(在线性不可分的时候该向量不存在))</p><p>那么我们来考虑一个最坏的情况, 即数据若的确是线性不可分的话, 应该如何应对. 由于数据产生的过程中可能会混入噪声, 这使得原本线性可分的数据也可能因为噪声的存在而不可分. 但是, 一般情况下, 噪声应该是一小部分, 即我们可以退而且其次, 不去寻找一个完美的超平面, 而是去寻找一个犯错误最少的超平面, 即 <span class="math display">\[{\bf w}_g \leftarrow \mathop{{\rm arg}\min}_{\bf w} \sum_{n=1}^N [\![ y_n \not = {\rm sign}({\bf w}^\mathsf{T}{\bf x}_n)]\!]\]</span> 然而, 求解这个问题被证明是NP难的, 只能采用近似算法求解. 例如, 我们可以保存一个最好的权重, 该权重到目前为止错分的数量最少. 该算法被称为&quot;口袋法&quot;, 其完整细节如下:</p><p>设定初始权重<span class="math inline">\(\hat{\bf w}\)</span></p><p>对时刻<span class="math inline">\(t=0, 1, \cdots\)</span></p><ol style="list-style-type: decimal"><li><p>随机寻找一个<span class="math inline">\({\bf w}_t\)</span>错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span></p></li><li><p>试图通过如下方法修正<span class="math inline">\({\bf w}_t\)</span> <span class="math display">\[   {\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}   \]</span></p></li><li><p>如果<span class="math inline">\({\bf w}_{t+1}\)</span>犯的错误比<span class="math inline">\(\hat{\bf w}\)</span>少, 那么将<span class="math inline">\(\hat{\bf w}\)</span>替换为<span class="math inline">\({\bf w}_{t+1}\)</span></p></li></ol><p>直到足够多次迭代完成. 我们将<span class="math inline">\(\hat{\bf w}\)</span>(称为<span class="math inline">\({\bf w}_{pocket}\)</span>)返回为<span class="math inline">\(g\)</span></p><p>注意在线性可分集合上也可以使用口袋法, 算法也可以返回一个无训练误差的解. 但是由于每次更新权重以后, 都要在所有数据上使用新旧权重各跑一遍, 来计算错分数量, 因此口袋法的执行时间通常比原始<span class="math inline">\(PLA\)</span>的计算时间长很多.</p><blockquote><p>+参考 <a href="http://txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/" target="_blank" rel="noopener">txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;感知机假设集合&quot;&gt;感知机假设集合&lt;/h3&gt;
    
    </summary>
    
      <category term="课程笔记" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石 1.学习问题</title>
    <link href="http://yoursite.com/2018/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-1-%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/09/09/机器学习基石-1-学习问题/</id>
    <published>2018-09-09T03:48:35.000Z</published>
    <updated>2018-09-09T08:17:36.707Z</updated>
    
    <content type="html"><![CDATA[<h3 id="机器学习的概念">机器学习的概念</h3><a id="more"></a><p>我们可以从人类的学习思维入手. 人类的学习过程, 是从观察出发, 经过大脑内化以后, 变成有用的技巧. 机器学习, 类似地, 是我们希望让计算机模拟人类的学习过程. 这时, 计算机观察到的东西被称作<strong>数据</strong>, 而思考过程实际上是<strong>计算过程</strong>, 技巧则是<strong>提高某一方面的表现</strong>. 因此,</p><div class="figure"><img src="/2018/09/09/机器学习基石-1-学习问题/1.png" alt="机器学习的过程"><p class="caption">机器学习的过程</p></div><p>为什么需要机器学习?</p><p>给定一张照片, 判断照片里的物体是不是一棵树. 使用传统的方法, 就需要对&quot;什么是树&quot;做出回答, 给出树的定义, 将其实现为程序. 按照规则进行判断, 并将其表述出来是很困难的. 然而, 一个小孩认识树的方法其实是通过观察, 经过经验的积累判断这个是树或者不是, 并不是教条的从长辈那里学习判断规则. 类似地, 我们可以让计算机自己从数据中学习树的判断方法. 因此, <strong>机器学习是构建复杂系统的一种方法</strong></p><p>机器学习的使用场景</p><ul><li><p>当我们不能提前想好各种情况, 手工编码规则时. 例如让机器人在火星上导航, 我们不可能提前想到它在火星上会遇到什么样的情况</p></li><li><p>当我们无法轻易地定义问题的解决方案时. 例如要做语音识别/视觉识别, 我们无法对音频信号做出准确定义</p></li><li><p>当人们需要做出快速决策时. 例如高频交易</p></li><li><p>当要让机器服务于海量用户时. 例如做服务个性化定制</p></li></ul><p>因此, 可以从以下三个关键点进行判断, 看是否适合使用机器学习</p><ol style="list-style-type: decimal"><li>问题是&quot;可以学习的&quot;, 即存在一些潜在的模式, 以至于性能可以被提高</li><li>这些规则难以清晰定义</li><li>手里掌握对应的数据</li></ol><h3 id="机器学习的应用">机器学习的应用</h3><p>机器学习在衣食住行四个方面都得到了广泛地应用</p><ul><li><p>衣: Abu-Mostafa 2012利用销售数据和对用户的调研结果构建推荐系统给用户推荐穿搭</p></li><li><p>食: Sadilek et al. 2013利用机器学习, 根据Twitter数据, 来判断餐厅的好坏</p></li><li><p>住: Tsanas and Xifara 2012利用已有房间的特点和耗能, 预测房屋的能用消耗</p></li></ul><p>此外还有两个领域: 教育和娱乐</p><ul><li>教育: 系统根据学生的答题情况, 有针对地提供题目让学生练习其薄弱的部分, 同时将太难的题推后给出. 即, 给定一名学生的答题历史和一个题目, 预测学生是否能做对这道题( KDDCup 2010 )</li><li>娱乐: 系统根据用户的历史打分, 预测用户对新电影的打分( KDDCup 2011 )</li></ul><h3 id="机器学习的过程">机器学习的过程</h3><h4 id="问题背景">问题背景</h4><p>以银行信用卡发卡这一问题为例. 假设银行收集了一些用户的基本信息, 例如下表</p><table><thead><tr class="header"><th>特征</th><th>值</th></tr></thead><tbody><tr class="odd"><td>年龄</td><td>23</td></tr><tr class="even"><td>性别</td><td>女</td></tr><tr class="odd"><td>所在地居住年数</td><td>1</td></tr><tr class="even"><td>工龄</td><td>0.5</td></tr><tr class="odd"><td>负债额</td><td>200,000</td></tr></tbody></table><p>银行要解决的问题是, 对于这样的客户, 是否应该给她发放信用卡</p><h4 id="问题形式化描述">问题形式化描述</h4><p>为了更加形式化地描述这个问题, 需要先定义一些符号:</p><ul><li><strong>输入</strong>: <span class="math inline">\({\bf x} \in \mathcal{X}\)</span>, 用户的特征</li><li><strong>输出</strong>: <span class="math inline">\({\bf y} \in \mathcal{Y}\)</span>, 是否发放信用卡</li><li><strong>目标函数</strong>: <span class="math inline">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span>, 是我们期望学到, 但是目前不知道的东西. 是最理想的公式</li><li><strong>数据</strong>: <span class="math inline">\(\mathcal{D} = \{({\bf x}_1, y_1), ({\bf x}_2, y_2), \ldots, ({\bf x}_n, y_n)}\)</span>, 是之前积累的记录</li><li><strong>假设</strong>: <span class="math inline">\(g: \mathcal{X} \rightarrow \mathcal{Y}\)</span>, 是机器从数据中学到的函数. 我们通常都希望<span class="math inline">\(g\)</span>的表现足够好, 即<span class="math inline">\(g \approx f\)</span>. 注意这里<span class="math inline">\(g\)</span>不一定等于<span class="math inline">\(f\)</span>(实际上, 我们永远也无法知道真正的<span class="math inline">\(f\)</span>是什么样子, 只知道由<span class="math inline">\(f\)</span>产生的数据<span class="math inline">\(\mathcal{D}\)</span>)</li><li><strong>机器学习算法</strong>: <span class="math inline">\(\mathcal {A}\)</span>, 是由<span class="math inline">\(\mathcal {D}\)</span>产生<span class="math inline">\(g\)</span>的算法, 可以理解为<span class="math inline">\(\mathcal {A}\)</span>会从各种不同假设<span class="math inline">\(h_k\)</span>(这里<span class="math inline">\(h_k\)</span>有好有坏)构成的集合<span class="math inline">\(\mathcal{H}\)</span>中挑选出来一个最好的<span class="math inline">\(g\)</span>, 使得<span class="math inline">\(g \approx f\)</span>. 即<span class="math inline">\(\mathcal{A}\)</span>以<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(\mathcal{H}\)</span>为输入, 以<span class="math inline">\(g\)</span>为输出</li></ul><div class="figure"><img src="/2018/09/09/机器学习基石-1-学习问题/2.png" alt="机器学习过程"><p class="caption">机器学习过程</p></div><p><strong>我们所讲的机器学习模型, 指的就是</strong><span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{H}\)</span></p><p>在有个这些记号以后, 我们可以重新给机器学习下一个定义</p><blockquote><p>机器学习是使用数据计算假设<span class="math inline">\(g\)</span>以逼近目标函数<span class="math inline">\(f\)</span>的过程</p></blockquote><h3 id="机器学习与其它名词">机器学习与其它名词</h3><h4 id="机器学习与数据挖掘">机器学习与数据挖掘</h4><p>数据挖掘的一个简单定义是使用海量数据, 在其中找出一些有趣的现象或性质. 这里, 如果&quot;有用的性质&quot;就是&quot;能够逼近目标函数的假设&quot;, 那么数据挖掘和机器学习是没有区别的. 如果&quot;有用的性质&quot;与&quot;假设&quot;是相关联的, 那么数据挖掘在很大程度上可以帮助机器学习</p><p>传统上的数据挖掘还关注如何在大的数据集中进行有效计算, 不过现在已经很难将机器学习和数据挖掘这两个概念分开了.</p><h4 id="机器学习与人工智能">机器学习与人工智能</h4><p>人工智能要求计算机呈现出一些智能的行为. 由于机器学习逼近目标函数的过程就展示了一些智能, 因此我们可以说, 机器学习是实现人工智能的一种手段.</p><h4 id="机器学习与统计学">机器学习与统计学</h4><p>统计学是使用数据来对未知过程进行推论. 考虑到假设<span class="math inline">\(g\)</span>是推论结果, <span class="math inline">\(f\)</span>是不知道的事, 那么可以说统计是实现机器学习的一种方法. 但是传统的统计学从数学出发, 很多工具是为数学假设提供证明和推论. 而机器学习看重的是如何计算出结果. 总而言之, 统计学为机器学习提供了很多有力的工具</p><blockquote><p>+参考 <a href="http://txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/" target="_blank" rel="noopener">txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;机器学习的概念&quot;&gt;机器学习的概念&lt;/h3&gt;
    
    </summary>
    
      <category term="课程笔记" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
