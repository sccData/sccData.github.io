<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>scc技术博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-30T14:53:49.880Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>史崇辰</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>复杂度分析（下）</title>
    <link href="http://yoursite.com/2018/09/29/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-2-%E7%8E%8B%E4%BA%89/"/>
    <id>http://yoursite.com/2018/09/29/数据结构与算法-2-王争/</id>
    <published>2018-09-29T15:22:54.000Z</published>
    <updated>2018-09-30T14:53:49.880Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除</p></blockquote><a id="more"></a><h3 id="最好最坏时间复杂度">最好、最坏时间复杂度</h3><p>先给个例子:</p><!--￼0--><p>按照上次的分析方法, 这段代码的复杂度是<span class="math inline">\(O(n)\)</span>, 其中, n代表数组的长度.</p><p>但是在数组中查找一个数据, 不需要从头到尾遍历一遍, 在中间可能就可以提前结束循环. 优化代码.</p><!--￼1--><p>如果数组的第一个元素正好是要查找的变量<span class="math inline">\(x\)</span>, 那时间复杂度是<span class="math inline">\(O(1)\)</span>. 但是如果数组中不存在变量<span class="math inline">\(x\)</span>, 那时间复杂度是<span class="math inline">\(O(n)\)</span>.</p><p>由此引入三个概念: 最好情况时间复杂度、最坏情况时间复杂度和平均情况复杂度.</p><p>最好情况时间复杂度和最坏情况时间复杂度都很好理解, 重点记录一下平均情况时间复杂度.</p><h4 id="平均情况时间复杂度">平均情况时间复杂度</h4><p>查找变量x在数组中的位置, 有n+1种情况: 在数组的0 n-1位置中和不在数组中. 把每种情况下, 需要遍历的元素累加起来, 然后再除以n+1, 就可以得到需要遍历的元素个数的平均值, 即: <span class="math display">\[\frac{1+2+3+\cdots+n-1+n}{n+1} = \frac{n(n+3)}{2(n+1)}\]</span> 将其简化后可得<span class="math inline">\(O(n)\)</span>. 但是, 这个结果是有问题的, 因为每种情况出现的概率不一样.</p><p>要查找的变量<span class="math inline">\(x\)</span>, 要么在数组里, 要么不在数组里. 为了方便理解, 假设在数组中与不在数组中的概率都为<span class="math inline">\(\frac{1}{2}\)</span>. 另外, 要查找的数据出现在<span class="math inline">\(0 \sim n-1\)</span>这<span class="math inline">\(n\)</span>个位置的概率也是一样的, 为<span class="math inline">\(\frac{1}{n}\)</span>. 所以, 根据概率乘法法则, 要查找的数据出现在<span class="math inline">\(0 \sim n-1\)</span>中任意位置的概率就是<span class="math inline">\(\frac{1}{2n}\)</span>.</p><p>计算过程为: <span class="math display">\[1 \times \frac{1}{2n}+2 \times \frac{1}{2n}+3\times\frac{1}{2n}+ \cdots +n \times \frac{1}{2n} + n \times \frac{1}{2} = \frac{3n+1}{4}\]</span> 这个值就是概率论中的<strong>加权平均值</strong>, 也叫作<strong>期望值</strong>, 所以平均时间复杂度的全称应该叫<strong>加权平均时间复杂度</strong>或者<strong>期望时间复杂度</strong>.</p><p>这段代码的加权平均时间复杂度仍然是<span class="math inline">\(O(n)\)</span>.</p><h4 id="均摊时间复杂度">均摊时间复杂度</h4><p>均摊时间复杂度, 听起来跟平均时间复杂度有点儿像. 但是, 其应用场景更加特殊、更加有限.</p><!--￼2--><p>最理想的情况是, 数组中有空闲空间, 我们只需要将数据插入到数组下表为<span class="math inline">\(count\)</span>的位置就可以了, 所以最好情况时间复杂度为<span class="math inline">\(O(1)\)</span>. 最坏的情况下, 数组中没有空闲空间了, 我们需要先做一次数组的遍历求和, 然后再将数据插入, 所以最坏情况时间复杂度为<span class="math inline">\(O(n)\)</span>.</p><p>而平均时间复杂度是<span class="math inline">\(O(1)\)</span>. 我们还是可以通过概率论的方法来分析.</p><p>假设数组的长度是n, 根据数据插入的位置的不同, 我们可以分为n种情况, 每种情况的时间复杂度是O(1). 除此之外, 还有一种&quot;额外&quot;的情况, 就是在数组没有空闲空间时插入一个数据, 这个时候的时间复杂度是O(n). 而且, 这n+1种情况发生的概率一样, 都是. 所以, 根据加权平均的计算方法, 我们求得的平均时间复杂度就是: <span class="math display">\[1 \times \frac{1}{n+1}+1\times\frac{1}{n+1}+\cdots+1\times\frac{1}{n+1}+n\times\frac{1}{n+1} = O(1)\]</span> 其实这个例子不需要引入概率论的知识, 对比一下<span class="math inline">\(insert()\)</span>和<span class="math inline">\(find()\)</span>的例子, 就会发现这两者有很大的差别.</p><p>首先, <span class="math inline">\(find()\)</span>在极端情况下, 复杂度才为<span class="math inline">\(O(1)\)</span>. 但<span class="math inline">\(insert()\)</span>在大部分情况下, 时间复杂度都为<span class="math inline">\(O(1)\)</span>. 只有个别情况下, 复杂度才为<span class="math inline">\(O(n)\)</span>. 这是<span class="math inline">\(insert()\)</span>第一个区别与<span class="math inline">\(find()\)</span>的地方.</p><p>第二个不同点, 对于<span class="math inline">\(insert()\)</span>函数来说, <span class="math inline">\(O(1)\)</span>时间复杂度的插入和<span class="math inline">\(O(n)\)</span>时间复杂度的插入, 出现的频率是非常有规律的, 而且有一定的前后时序关系, 一般都是一个<span class="math inline">\(O(n)\)</span>插入之后, 紧跟着<span class="math inline">\(n-1\)</span>个<span class="math inline">\(O(1)\)</span>的插入操作, 循环往复.</p><p>针对这样一种特殊场景的复杂度分析, 我们并不需要像之前那样, 找出所有的输入情况及相应的发生概率, 然后计算加权平均值. 为此, 引入一种更加简洁的分析方法: <strong>摊还分析法</strong>, 通过摊还分析法得到的时间复杂度我们起了一个名字, 叫<strong>均摊时间复杂度</strong>.</p><p>针对上述例子, 每一次<span class="math inline">\(O(n)\)</span>的插入操作, 都会跟着<span class="math inline">\(n-1\)</span>次<span class="math inline">\(O(1)\)</span>的插入操作, 所以把耗时多的那次操作均摊到接下来的<span class="math inline">\(n-1\)</span>次耗时少的操作上, 均摊下来, 这一组连续的操作的均摊时间复杂度就是<span class="math inline">\(O(1)\)</span>.</p><p>总结其应用场景:</p><p>对一个数据结构进行一组连续操作中, 大部分情况下时间复杂度都很低, 只有个别情况下时间复杂度比较高, 而且这些操作之间存在前后连贯的时序关系, 这个时候, 我们就可以将一组操作放在一块儿分析, 看是否能将较高时间复杂度那次操作的耗时, 平摊到其他那些时间复杂度比较低的操作上. 而且, 能够应用均摊时间复杂度分析的场合, 一般均摊时间复杂度就等于最好情况时间复杂度.</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="王争课程笔记" scheme="http://yoursite.com/categories/%E7%8E%8B%E4%BA%89%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构与算法" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>复杂度分析（上）</title>
    <link href="http://yoursite.com/2018/09/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-%E7%8E%8B%E4%BA%89/"/>
    <id>http://yoursite.com/2018/09/26/数据结构与算法-1-王争/</id>
    <published>2018-09-26T12:06:30.000Z</published>
    <updated>2018-09-29T08:48:59.089Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除</p></blockquote><a id="more"></a><h3 id="为什么需要复杂度分析">为什么需要复杂度分析？</h3><p>很多人对复杂度分析有疑问, 认为直接在机器上跑一遍, 就可以得出时间和空间复杂度. 对于这种说法, 我们认为是正确的, 并且很多书籍将其称为<strong>事后统计</strong>. 但是, 这种方法有很大的局限性.</p><ul><li>测试结果依赖于测试环境</li></ul><p>不同的硬件对测试结果影响较大</p><ul><li>测试结果受数据规模的影响很大</li></ul><p>数据规模的大小和有序度, 对测试结果影响较大</p><p>所以, 我们需要一个不用具体的测试数据来测试, 就可以粗略地估计算法的执行效率的方法.</p><h3 id="大o复杂度表示法">大<span class="math inline">\(O\)</span>复杂度表示法</h3><p>以一段代码为例来估计算法的执行时间</p><pre><code>int cal(int n) {    int sum = 0;    int i = 1;    for(; i &lt;= n; ++i){        sum = sum + i;    }    return sum;}</code></pre><p>由于是粗略估计, 假设每行代码执行的时间都一样, 为<span class="math inline">\(t\)</span>. 第2、3行代码分别需要1个<span class="math inline">\(t\)</span>的执行时间, 第4、5行都运行了<span class="math inline">\(n\)</span>遍, 所以需要<span class="math inline">\(2 n * t\)</span>的执行时间, 所以这段代码总的执行时间就是<span class="math inline">\((2 n + 2) * t\)</span>. 可以看出来, 所有的代码执行时间<span class="math inline">\(T(n)\)</span>与每行代码的执行次数成正比.</p><p>再看一段代码</p><pre><code>int cal(int n) {    int sum = 0;    int i = 1;    int j = 1;    for(; i &lt;= n; ++i){        j = 1;        for(; j &lt;= n; ++j){            sum = sum + i * j;        }    }}</code></pre><p>根据以上思路, 可以得出$T(n) = (2n^2 + 2n + 3) * t $.</p><p><em>从中我们可以总结得到一个非常重要的规律, 所有代码的执行时间<span class="math inline">\(T(n)\)</span>与每行代码的执行次数<span class="math inline">\(n\)</span>成正比</em> <span class="math display">\[T(n) = O(f(n))\]</span> 其中<span class="math inline">\(T(n)\)</span>表示代码执行的时间; n表示数据规模的大小; <span class="math inline">\(f(n)\)</span>表示每行代码执行的次数总和. 公式中的<span class="math inline">\(O\)</span>, 表示代码的执行时间<span class="math inline">\(T(n)\)</span>与<span class="math inline">\(f(n)\)</span>表达式成正比.</p><p>所以<span class="math inline">\(T(n) = O(2n + 2)\)</span>, <span class="math inline">\(T(n) = O(2n^2 + 2n + 3)\)</span>, 这就是大<span class="math inline">\(O\)</span>时间复杂度表示法. 大<span class="math inline">\(O\)</span>时间复杂度实际表示的是代码执行时间随数据规模增长的变化趋势, 所以, 也叫做渐进时间复杂度, 简称时间复杂度.</p><p>当<span class="math inline">\(n\)</span>很大的时候, 我们只需记录一个最大量级就可以了, 例如<span class="math inline">\(T(n) = O(n)\)</span>; <span class="math inline">\(T(n) = O(n^2)\)</span>.</p><h3 id="时间复杂度分析">时间复杂度分析</h3><ul><li><p>只关注循环次数最多的一段代码</p><pre><code>int cal(int n) {    int sum = 0;    int i = 1;    for(; i &lt;= n; ++i){        sum = sum + i;    }    return sum;}</code></pre></li></ul><p>总的时间复杂度为<span class="math inline">\(O(n)\)</span></p><ul><li><p>加法法则: 总复杂度等于量级最大的那段代码的复杂度</p><pre><code>int cal(int n){    int sum_1 = 0;    int p = 1;    for(; p &lt; 100; ++p){        sum_1 = sum_1 + p;    }    int sum_2 = 0;    int q = 1;    for(; q&lt;n; ++q){        sum_2 = sum_2 + q;    }    int sum_3 = 0;    int i = 1;    int j = 1;    for(; i&lt;=n; ++i){        for(; j&lt;=n; ++j){            sum_3 = sum_3 + i * j;        }    }return sum_1 + sum_2 + sum_3;}</code></pre></li></ul><p>总的时间复杂度为<span class="math inline">\(O(n^2)\)</span></p><ul><li><p>乘法法则: 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积</p><pre><code>int cal(int n){    int ret = 0;    int i = 1;    for(; i&lt;n; ++i){        ret = ret + f(i);    }}int f(int n){    int sum = 0;    int i = 1;    for(; i&lt;n; ++i){        sum = sum + i;    }    return sum;}</code></pre></li></ul><p>总的时间复杂度为<span class="math inline">\(O(n^2)\)</span></p><h3 id="几种常见时间复杂度实例分析">几种常见时间复杂度实例分析</h3><p>复杂度量级(按数量级递增)</p><ul><li>常量阶<span class="math inline">\(O(1)\)</span></li><li>对数阶<span class="math inline">\(O(logn)\)</span></li><li>线性阶<span class="math inline">\(O(n)\)</span></li><li>线性对数阶<span class="math inline">\(O(nlogn)\)</span></li><li>平方阶<span class="math inline">\(O(n^2)\)</span>、立方阶<span class="math inline">\(O(n^3) \cdots k\)</span>次方阶<span class="math inline">\(O(n^k)\)</span></li><li>指数阶<span class="math inline">\(O(2^n)\)</span></li><li>阶乘阶<span class="math inline">\(O(n!)\)</span></li></ul><p>将上述时间复杂度错略的分为两类：<strong>多项式量级</strong>和<strong>非多项式量级</strong>. 其中, 非多项式量级只有两个: <span class="math inline">\(O(2^n)\)</span>和<span class="math inline">\(O(n!)\)</span>.</p><p>我们把时间复杂度为非多项式量级的算法问题叫做<strong>NP问题</strong>(Non-Deterministic Polynomial, 非确定多项式).</p><p>当数据规模<span class="math inline">\(n\)</span>越来越大时, 非多项式量级算法的执行时间会急剧增加.</p><p>因此, NP问题不是我们讨论的重点. 接下来, 我们主要来看几种常见的<strong>多项式时间复杂度</strong>.</p><ol style="list-style-type: decimal"><li><span class="math inline">\(O(1)\)</span></li></ol><p><span class="math inline">\(O(1)\)</span>只是常量级时间复杂度的一种表示方法, 并不是指只执行了一行代码.</p><pre><code>int i = 8;int j = 6;int sum = i + j;</code></pre><p>只要代码的执行时间不随<span class="math inline">\(n\)</span>的增长而增长, 这样代码的时间复杂度都记作<span class="math inline">\(O(1)\)</span>. <strong>一般情况下, 只要算法中不存在循环语句、递归语句, 即使有成千上万行代码, 其时间复杂度也是<span class="math inline">\(O(1)\)</span></strong>.</p><ol start="2" style="list-style-type: decimal"><li><p><span class="math inline">\(O(logn)\)</span>、<span class="math inline">\(O(nlogn)\)</span></p><pre><code>i = 1;while(i&lt;=n){    i = i * 2;}</code></pre></li></ol><p>从代码中可以看出, 变量<span class="math inline">\(i\)</span>的值为: <span class="math display">\[   2^0\ \ 2^1\ \ 2^2\  \cdots \ 2^k\  \cdots \ 2^x = n\]</span> 通过求解<span class="math inline">\(2^x = n\)</span>, 就可以知道代码的执行次数. 所以其为<span class="math inline">\(O(\log_2n)\)</span>.</p><p>因为<span class="math inline">\(\log_3n\)</span>就等于<span class="math inline">\(\log_32 * \log_2n\)</span>, 所以<span class="math inline">\(O(\log_3n) = O(C * \log_2n)\)</span>, 其中<span class="math inline">\(C = \log_32\)</span>是一个常量. 因此, 在对数时间复杂度的表示方法里, 忽略对数的&quot;底&quot;, 统一表示为<span class="math inline">\(O(\log n)\)</span>.</p><p>如果一段代码的时间复杂度是<span class="math inline">\(O(\log n)\)</span>, 循环<span class="math inline">\(n\)</span>遍, 时间复杂度就是<span class="math inline">\(O(n\log n)\)</span>.</p><ol start="3" style="list-style-type: decimal"><li><p><span class="math inline">\(O(m+n)\)</span>、<span class="math inline">\(O(m*n)\)</span></p><pre><code>int call(int m, int n){    int sum_1 = 0;    int i = 1;    for(; i&lt;m; ++i){        sum_1 = sum_1 + 1;    }    int sum_2 = 0;    int j = 1;    for(; j&lt;n; ++j){        sum_2 = sum_2 + j;    }    return sum_1 + sum_2;}</code></pre></li></ol><p>从代码中可以看出, <span class="math inline">\(m\)</span>和<span class="math inline">\(n\)</span>是表示两个数据规模, 我们无法评判谁的数量级大, 所以, 时间复杂度就为<span class="math inline">\(O(m+n)\)</span>.</p><p>乘法类似.</p><h3 id="空间复杂度">空间复杂度</h3><p>空间复杂度全程就是<strong>渐进空间复杂度</strong>, <strong>表示算法的存储空间与数据规模之间的增长关系</strong>.</p><pre><code>void print(int n){    int i = 0;    int[] a = new int[n];    for(i; i&lt;n; ++i){        a[i] = i*i;    }    for(i=n-1; i&gt;=0; --i){        print out a[i];    }}</code></pre><p>第<span class="math inline">\(2\)</span>行代码中, 我们申请了一个空间存储变量<span class="math inline">\(i\)</span>, 但是它是常量阶, 跟数据规模<span class="math inline">\(n\)</span>没有关系, 所以忽略. 第<span class="math inline">\(3\)</span>行申请了一个大小为<span class="math inline">\(n\)</span>的<span class="math inline">\(int\)</span>类型数组, 除此之外, 剩下的代码都没有占用更多的空间, 所以整段代码的空间<span class="math inline">\(O(n)\)</span>.</p><p>常见的空间复杂度就是<span class="math inline">\(O(1)\)</span>、<span class="math inline">\(O(n)\)</span>、<span class="math inline">\(O(n^2)\)</span>.</p><h3 id="学习关键">学习关键</h3><p><strong>多练</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="王争课程笔记" scheme="http://yoursite.com/categories/%E7%8E%8B%E4%BA%89%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构与算法" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石 3.机器学习的类型</title>
    <link href="http://yoursite.com/2018/09/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%B1%BB%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/09/25/机器学习基石-3-机器学习的类型/</id>
    <published>2018-09-25T05:42:31.000Z</published>
    <updated>2018-09-29T08:48:40.861Z</updated>
    
    <content type="html"><![CDATA[<h3 id="根据输出空间mathcaly分类">根据输出空间<span class="math inline">\(\mathcal{Y}\)</span>分类</h3><a id="more"></a><h4 id="二分类问题">二分类问题</h4><p>重新回顾一下&quot;是非题&quot;的形式. 为了解决这个问题, 需要我们提供一批训练数据<span class="math inline">\(\mathcal{D}\)</span>, 其中我们需要指出对哪些用户发放信用卡, 哪些不发. 像这样答案只有两种可能性(&quot;要&quot;或&quot;不要&quot;)的问题称为<strong>二元分类问题</strong>, 其输出空间<span class="math inline">\(\mathcal{Y}\)</span>通常用集合<span class="math inline">\(\{-1, +1\}\)</span>表示, 类似于&quot;判断题&quot;. 这种问题类型的例子有很多, 包括</p><ul><li>要不要发信用卡</li><li>电子邮件是不是垃圾邮件</li><li>病人有没有生病</li><li>广告是否会赚钱</li></ul><p>等等。</p><p>二元分类问题是机器学习中最基本也是最核心的问题, 很多理论推到和算法模型设计都是从这一类问题出发.</p><h4 id="多分类问题">多分类问题</h4><p>二元分类问题很容易进行扩展, 即如果答案有多个离散的可能性, 那么问题演变为<strong>多元分类问题</strong>. 假设目标类别有<span class="math inline">\(K\)</span>种, 那么<span class="math inline">\(\mathcal{Y}=\{1, 2, \cdots, K\}\)</span>. 一个典型的例子是对硬币进行分类, 看投入的是<span class="math inline">\(1\)</span>角、<span class="math inline">\(5\)</span>角还是<span class="math inline">\(1\)</span>元. 这种问题类似于“选择题”. 这种问题类型的例子包括</p><ul><li>识别手写数字是<span class="math inline">\(0\)</span>到<span class="math inline">\(9\)</span>这十个数字中的哪一种</li><li>识别图片中的水果是哪一种水果</li><li>邮件的进一步分类, 例如是垃圾邮件、社交网络邮件、重要邮件还是促销活动邮件等等</li></ul><h4 id="回归问题">回归问题</h4><p>如果将医疗领域中的问题对应到上述问题中, 那么这两种问题可以对应如下：</p><ul><li>二元分类问题：给定病人特征, 判断病人是否患病</li><li>多元分类问题：给定病人特征, 判断病人患的是哪种癌症</li></ul><p>但是还有一类问题， 例如判断病人手术后多少天可以出院. 这种问题的输出是整个实数集, 或者实数集中的一个连续区间. 这种问题通常被称为<strong>回归分析</strong>. 此时<span class="math inline">\(\mathcal{Y} \in \mathbb{R}\)</span>或<span class="math inline">\(\mathcal{Y} = [{\rm lower},{\rm upper}] \subset \mathbb{R}\)</span>. 这种问题类型的例子包括</p><ul><li>根据公司的状况, 预测其次日股票价格</li><li>根据大气状况, 预测明日气温</li></ul><p>回归问题是一种历史悠久的统计问题, 也是机器学习领域里非常核心的问题</p><h4 id="结构化分析">结构化分析</h4><p>在自然语言处理(<span class="math inline">\(\rm NLP\)</span>)这个领域里, 有一项任务是对输入句子中的每个词标注其词性(<span class="math inline">\({\rm Part\ of\ Speech}, {\rm POS}\)</span>). 例如输入&quot;<span class="math inline">\(\rm {I\ love\ ML}\)</span>&quot;, 程序应该可以将&quot;I&quot;标记为代词, &quot;<span class="math inline">\(\rm{love}\)</span>&quot;标记为动词, &quot;<span class="math inline">\(\rm{ML}\)</span>&quot;标记为名词. 这种任务可以看作是一种多元分类问题, 但是如果输入是以句子为单位, 由于句子中有结构性, 因此输出也是一个结构. 这样的问题可以看做是一个巨大的多类别分类问题, 各个类别是隐藏的, 看不到, 而且不同类别之间有联系, 使得穷举所有可能性变得不可能. 但是我们知道输出存在一定的结构性, 并希望程序能够正确给出判定. 这种问题称为<strong>结构化分析</strong>, 此时<span class="math inline">\(\mathcal{Y}\)</span>是一种结构. 这种问题类型的例子包括</p><ul><li>给定蛋白质数据, 判断蛋白质的结构</li><li>给定语言文本, 给出语法树</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;根据输出空间mathcaly分类&quot;&gt;根据输出空间&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{Y}\)&lt;/span&gt;分类&lt;/h3&gt;
    
    </summary>
    
      <category term="林轩田课程笔记" scheme="http://yoursite.com/categories/%E6%9E%97%E8%BD%A9%E7%94%B0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石 2.学习判断是与非</title>
    <link href="http://yoursite.com/2018/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-2-%E5%AD%A6%E4%B9%A0%E5%88%A4%E6%96%AD%E6%98%AF%E4%B8%8E%E9%9D%9E/"/>
    <id>http://yoursite.com/2018/09/10/机器学习基石-2-学习判断是与非/</id>
    <published>2018-09-10T15:34:08.000Z</published>
    <updated>2018-09-29T08:48:28.745Z</updated>
    
    <content type="html"><![CDATA[<h3 id="感知机假设集合">感知机假设集合</h3><a id="more"></a><p>第一章里讲到机器学习的核心就是, 使用算法<span class="math inline">\(\mathcal{A}\)</span>接受数据<span class="math inline">\(\mathcal{D}\)</span>, 从假设集合(所有可能性)<span class="math inline">\(\mathcal{H}\)</span>中选出一个<span class="math inline">\(g\)</span>, 希望<span class="math inline">\(g \approx f\)</span>. 那么我们现在最关心的就是, <span class="math inline">\(\mathcal{H}\)</span>应该是什么样的.</p><p>以之前提到的银行审核发放信用卡的场景为例, 假设我们把每个使用者定义为向量<span class="math inline">\(\bf x\)</span>, 包含<span class="math inline">\(d\)</span>个维度, 例如<span class="math inline">\(x_1\)</span>代表年龄, <span class="math inline">\(x_2\)</span>代表年薪, 等等. 我们可以将这些维度综合起来给使用者一个整体的分数. 如果这个分数超过了某个标准, 那么就发放信用卡; 否则拒绝发放. 这样, 我们需要给每个<span class="math inline">\(x_i, i \in \{ 1, \ldots, d \}\)</span>来赋一个系数<span class="math inline">\(w_i\)</span>, 如果特征对最后的影响是正面的, 那么就给<span class="math inline">\(w_i\)</span>正值, 否则给负值. 如果我们在规定一个阈值<span class="math inline">\(\rm threshold\)</span>, 那么我们的决策方法就可以写成为, 如果<span class="math inline">\(\sum_{i=1}^d w_ix_i &gt; \rm threshold\)</span>, 就批准信用卡申请, 否则就拒绝.</p><p>我们可以进一步地规定输出空间<span class="math inline">\(\mathcal{Y} \in \{-1, +1\}\)</span>, 其中<span class="math inline">\(y=-1\)</span>时表示拒绝, <span class="math inline">\(y=1\)</span>时表示许可. 这样做的好处是我们可以直接使用<span class="math inline">\(\rm sign\)</span>函数来求出<span class="math inline">\(y\)</span>的值, 具体地说, 假设集合<span class="math inline">\(\mathcal{H}\)</span>中的每个元素<span class="math inline">\(h \in \mathcal{H}\)</span>都有如下形式 <span class="math display">\[h({\bf x}) ={\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold})\]</span> 其中<span class="math inline">\({\rm sign}\)</span>函数的定义为 <span class="math display">\[{\rm sign}(x) = \begin{cases} +1 &amp; {\rm if \ } x&gt;0 \\ -1 &amp; {\rm if \ } x&lt;0 \end{cases}\]</span> 即对用户的所有属性做一个加权打分, 看它是否超过阈值. 如果超过, 则批准; 否则就拒绝(如果正好等于阈值, 这种情况很少发生, 甚至可以随机决定<span class="math inline">\(y\)</span>是<span class="math inline">\(-1\)</span>还是<span class="math inline">\(1\)</span>).</p><p>这里我们说<span class="math inline">\(\mathcal{H}\)</span>是一个集合的原因是, 不同的<span class="math inline">\(\bf w\)</span>和<span class="math inline">\(\rm threshold\)</span>都对应了不同的<span class="math inline">\(h\)</span>, 所有这些可能性对应的所有<span class="math inline">\(h\)</span>构成了最后的假设集合<span class="math inline">\(\mathcal{H}\)</span>. <span class="math inline">\(h\)</span>这样的函数类型称为<strong>感知机(perceptron)</strong>, 其中<span class="math inline">\(\bf w\)</span>称为权重. 进一步地, 假设我们把<span class="math inline">\(-\rm threshold\)</span>看做是<span class="math inline">\((-\rm threshold) \cdot (+1)\)</span>, 然后把<span class="math inline">\(+1\)</span>看作是<span class="math inline">\(x_0\)</span>, 那么前面的公式形式可以进一步的简化, 即 <span class="math display">\[\begin{align*}h({\bf x}) &amp;= {\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold}) \\&amp;= {\rm sign}((\sum_{i=1}^d w_ix_i)+\underbrace{(-{\rm threshold})}_{w_0}\cdot \underbrace{(+1)}_{x_0}) \\&amp;= {\rm sign}(\sum_{i=0}^d w_ix_i) \\&amp;= {\rm sign}({\bf w}^\mathsf{T}{\bf x}) \end{align*}\]</span></p><p>这里<span class="math inline">\(\bf w\)</span>和<span class="math inline">\(\bf x\)</span>都看作是列向量, 即维度为<span class="math inline">\((d+1)1\)</span></p><p>我们可以通过一个图例来加强理解. 假如我们顾客的特征数(也就是前面说的属性维度)为<span class="math inline">\(2\)</span>, 那么我们可以把任意输入<span class="math inline">\(\bf x\)</span>画在一个平面<span class="math inline">\(\mathbb{R}^2\)</span>上(类似的, 如果特征数为<span class="math inline">\(d\)</span>, 那么每个输入<span class="math inline">\(\bf x\)</span>都可以在<span class="math inline">\(\mathbb{R}^d\)</span>空间表示, 只是会对我们的可视化造成困难), 每个输入对应平面上的一个点. 这样, <span class="math inline">\(\mathbb{R}^2\)</span>上的<span class="math inline">\(h\)</span>都有如下形式: <span class="math display">\[h({\bf x}) = \rm sign(w_0+w_1x_1+w_2x_2)\]</span> 可以看出, 每个<span class="math inline">\(h\)</span>其实都对应了<span class="math inline">\(\mathbb{R}^2\)</span>上的一条直线. 感知机规定位于直线某一侧的样本都被判定为正例, 另一侧的样本都被判定为负例. 不同的权重会产生不同的分类方式. 假设我们用蓝色的圈o表示正例, 红色的叉×表示负例, 下图给出了两个不同的感知机</p><div class="figure"><img src="/2018/09/10/机器学习基石-2-学习判断是与非/1.png" alt="1"><p class="caption">1</p></div><div class="figure"><img src="/2018/09/10/机器学习基石-2-学习判断是与非/2.png" alt="2"><p class="caption">2</p></div><p>可以看出来右边的感知机在训练集上效果更好, 因为它对所有例子做出了正确分类. 而左侧的感知机在训练集上表现稍逊(一个正例被误判为负, 两个负例被误判为正)</p><p>由于感知机都对应于一个超平面, 因此它也被称为是<strong>线性分类器</strong>(<span class="math inline">\(\mathbb R^2\)</span>的超平面是一条直线, <span class="math inline">\(\mathbb R^3\)</span>的超平面是一个平面, 以此类推).</p><h3 id="感知机学习算法">感知机学习算法</h3><p>在我们知道了<span class="math inline">\(h \in \mathcal H\)</span>的形态以后, 接下来的问题是设计算法<span class="math inline">\(\mathcal A\)</span>来选出最优的<span class="math inline">\(g\)</span>来逼近理想的<span class="math inline">\(f\)</span>. 尽管我们不知道<span class="math inline">\(f\)</span>具体应该是什么, 但是我们知道数据<span class="math inline">\(\mathcal D\)</span>是由<span class="math inline">\(f\)</span>生成的. 因此我们有理由相信, 好的<span class="math inline">\(g\)</span>满足对所有我们已经收集道的数据, 其输出与<span class="math inline">\(f\)</span>的输出尽可能接近, 即<span class="math inline">\(g({\bf x}_n) = f({\bf x}_n) = y_n\)</span>. 因此, 我们可以先找一个超平面, 至少能够对训练集中的数据正确分类. 然而难度在于, <span class="math inline">\(\mathcal H\)</span>的大小通常都是无限的.</p><p>一种解决方案是, 我们可以先初始化一个超平面<span class="math inline">\(g_0\)</span>(为了简单起见, 将其以其权重<span class="math inline">\({\bf w}_0\)</span>代表, 称为初始权重). 我们允许这个超平面犯错, 但我们要设计算法, 让超平面遇到<span class="math inline">\(\mathcal D\)</span>中的错分样本以后可以修正自己. 通常我们可以将<span class="math inline">\({\bf w}_0\)</span>初始化为零向量<span class="math inline">\(\bf 0\)</span>. 然后, 在每一步<span class="math inline">\(t\)</span>, 找到一个使<span class="math inline">\({\bf w}_t\)</span>错分的样本错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>. 即有 <span class="math display">\[\rm sign({\bf w}^T_t {\bf x}_{n(t)}) \not= y_{n(t)}\]</span> 接下里我们试着修正<span class="math inline">\({\bf w}_t\)</span>. 可以看到错分有两种情况:</p><ul><li><p><span class="math inline">\(y\)</span>本来应该是<span class="math inline">\(+1\)</span>, 但是模型判断出来是负值. 也就是说此时<span class="math inline">\(\bf w\)</span>与<span class="math inline">\(\bf x\)</span>之间的角度太大, 因此需要把<span class="math inline">\(\bf w\)</span>往靠近<span class="math inline">\(\bf x\)</span>的方向旋转使它们的角度变小. 可以通过让<span class="math inline">\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf x}_{n(t)}\)</span>达到这个目的</p></li><li><p><span class="math inline">\(y\)</span>本来应该是<span class="math inline">\(-1\)</span>, 但是模型判断出来是正值. 也就是说此时<span class="math inline">\(\bf w\)</span>与<span class="math inline">\(\bf x\)</span>之间的角度太小, 因此需要把<span class="math inline">\(\bf w\)</span>往远离<span class="math inline">\(\bf x\)</span>的方向旋转使它们的角度变大. 考虑到符号, 其实也可以通过让<span class="math inline">\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf w}_{n(t)}\)</span>达到这个目的</p></li></ul><p>因此, 在<span class="math inline">\(t+1\)</span>时刻, 我们总可以通过下式来修正<span class="math inline">\({\bf w}_t\)</span>, 即 <span class="math display">\[{\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}\]</span></p><div class="figure"><img src="/2018/09/10/机器学习基石-2-学习判断是与非/3.png" alt="3"><p class="caption">3</p></div><div class="figure"><img src="/2018/09/10/机器学习基石-2-学习判断是与非/4.png" alt="4"><p class="caption">4</p></div><p>感知机学习算法(Perceptron Learning Algorithm, PLA)就是重复上面的过程, 直到没有错误发生为止. 算法将最后得到的权重<span class="math inline">\(\bf w\)</span>(记做<span class="math inline">\({\bf w}_{PLA}\)</span>)返回<span class="math inline">\(g\)</span>. 完整写法如下:</p><p>对于<span class="math inline">\(t = 0,1, \ldots\)</span></p><p>1). 找到一个使<span class="math inline">\({\bf w}_t\)</span>错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>. 即有 <span class="math display">\[sign({\bf w}_t^T {\bf x}_{n(t)}) \not = y_{n(t)}\]</span> 2). 以如下方法修正<span class="math inline">\({\bf w}_t\)</span>: <span class="math display">\[{\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}\]</span> 直到便利了所有样本一遍以后都没有找到错误为止.</p><p>由此, 也引出两个问题:</p><ul><li>算法真的会停止吗?</li><li>能否确定算法返回的<span class="math inline">\(g \approx f\)</span>?</li></ul><h3 id="感知机的有效性与确定终止性">感知机的有效性与确定终止性</h3><p>回顾PLA算法的停止条件, 它是在没有找到错误的时候才停止, 这要求我们的数据可以用一条线将正例样本和负例样本分割开来(如果不存在这条线, PLA肯定是不可能停止的). 这种条件叫做<strong>线性可分条件</strong>. 接下来, 我们需要证明: 如果数据集的确是线性可分的, 感知机是否总是能找到一个超平面把数据恰好分开.</p><p>假设数据集<span class="math inline">\(\mathcal D\)</span>线性可分, 我们先证明存在一个超平面<span class="math inline">\({\bf w}_f\)</span>使得任意<span class="math inline">\(i \in \{1, \ldots, n\}\)</span>, <span class="math inline">\(y_i = {\rm sign}({\bf w}_f^T{\bf x}_i)\)</span>. 这意味着对每个<span class="math inline">\({\bf x}_i\)</span>, 它与超平面都有一定距离, 即 <span class="math display">\[\min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n &gt; 0\]</span> 其中<span class="math inline">\({\bf w}_f^T{\bf x}_n\)</span>是点<span class="math inline">\({\bf w}_n\)</span>到<span class="math inline">\({\bf w}_f\)</span>的带符号的距离. 在训练过程中遇到的所有错分点<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>(假设在时刻<span class="math inline">\(t\)</span>遇到), 肯定有 <span class="math display">\[y_{n(t)}{\bf w}_f^\mathsf{T}{\bf x}_{n(t)} \ge \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n &gt; 0\]</span> 我们可以先证明, <span class="math inline">\({\bf w}_t\)</span>被<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span>纠正以后更加接近纠正以后更加接近<span class="math inline">\({\bf w}_f\)</span>. 我们可以通过两个向量的內积来判断它们是否接近: 两个向量越接近, 內积越大(可以理解为两个向量<span class="math inline">\(\bf u\)</span>和和<span class="math inline">\(\bf v\)</span>越接近, 其夹角, 那么<span class="math inline">\(\cos \theta\)</span>越大, 所以两者的內积<span class="math inline">\({\bf u} \cdot {\bf v} = |\!|{\bf u}|\!||\!|{\bf v}|\!|\cos \theta\)</span>越大), 则 <span class="math display">\[\begin{align*}{\bf w}_f^\mathsf{T}{\bf w}_{t+1} &amp;= {\bf w}_t^\mathsf{T}({\bf w}_t + y_{n(t)}{\bf x}_{n(t)}) \\&amp;\ge {\bf w}_f^\mathsf{T}{\bf w}_t + \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n \\&amp;&gt; {\bf w}_f^\mathsf{T}{\bf w}_t + 0 = {\bf w}_f^\mathsf{T}{\bf w}_t\hspace{3ex} \blacksquare\end{align*}\]</span></p><p>但是这里又有一个新的问题, 即內积变大不一定说明两个向量接近, 因为向量长度变大也会导致內积变大. 因此接下来我们要证明, 修正<span class="math inline">\({\bf w}_t\)</span>以后, 新的权重长度不会发生太大的变化. 这里要用到一个性质, 即<span class="math inline">\(PLA\)</span>仅在遇到错误的数据时才更新权重, 即如果权重<span class="math inline">\({\bf w}_t\)</span>被订正, 意味着<span class="math inline">\(sign({\bf w}_t^T{\bf w}_{n(t)}) \not = y_{n(t)}\)</span>, 也就是<span class="math inline">\(y_{n(t)}{\bf w}_t^T{\bf x}_{n(t)} \le 0\)</span>. 考虑到<span class="math inline">\(y_{n(t)}\)</span>是标量, 且取值只可能为<span class="math inline">\(1\)</span>或<span class="math inline">\(-1\)</span>(即<span class="math inline">\(y_{n(t)}^2 = 1\)</span>), <span class="math inline">\({\bf w}_t^T{\bf x}_{n(t)}\)</span>也是标量, 因此 <span class="math display">\[\begin{align*}|\!|{\bf w}_{t+1}|\!|^2 &amp;= |\!|{\bf w}_t + y_{n(t)}{\bf x}_{n(t)}|\!|^2\end{align*}\]</span> 简记<span class="math inline">\(y = y_{n(t)}\)</span>, <span class="math inline">\({\bf x} = {\bf x}_{n(t)}\)</span>, <span class="math inline">\({\bf w} = {\bf w}_t\)</span>, 则 <span class="math display">\[\begin{align*}|\!|{\bf w}_{t+1}|\!|^2 &amp;= ({\bf w}+y{\bf x})^\mathsf{T}({\bf w}+y{\bf x}) \\&amp;= {\bf w}^\mathsf{T}{\bf w} + 2y{\bf w}^\mathsf{T}{\bf x} + {\bf x}^\mathsf{T}{\bf x} \\&amp;\le |\!|{\bf w}|\!|^2 + |\!|{\bf x}|\!|^2 \hspace{3ex}(\because y{\bf w}^\mathsf{T}{\bf x} \le 0) \\&amp;\le |\!|{\bf w}|\!|^2 + \max_n|\!|{\bf x}_n|\!|^2\end{align*}\]</span> 即权重经过修正以后, 其长度最多增加<span class="math inline">\(\max_n |\!|{\bf x}_n|\!|^2\)</span></p><p>经由上面两部分, 假设权重的初始向量为<span class="math inline">\(\bf 0\)</span>, 我们可以求出经过<span class="math inline">\(T\)</span>步更新最后得到的权重<span class="math inline">\({\bf w}_{T}\)</span>与<span class="math inline">\(\bf w_{f}\)</span>之间的夹角余弦值的下界. 为了求这个值, 只需要求两个权重归一化以后內积的下界即可, 即 <span class="math display">\[\inf \left(\frac{ {\bf w}_f^\mathsf{T}} {|\!|{\bf w}_f|\!|} \cdot \frac{{\bf w}_T}{|\!|{\bf w}_T|\!|} \right)\]</span> 先看分子. 由于初始<span class="math inline">\({\bf w}_0 = {\bf 0}\)</span>, 因此由之前第一个证明的中间步骤, 我们可以写出第一次更新、第二次更新......后分子的下界, 即 <span class="math display">\[\begin{align*}{\bf w}_f^\mathsf{T}{\bf w}_1 &amp;\ge {\bf w}_f^\mathsf{T} \cdot {\bf 0} + \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \\{\bf w}_f^\mathsf{T}{\bf w}_2 &amp;\ge {\bf w}_f^\mathsf{T}\cdot{\bf w}_1 + \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \ge {\bf x}_f^\mathsf{T} \cdot {\bf 0} + 2\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \\&amp;\vdots \\{\bf w}_f^\mathsf{T}{\bf w}_T &amp;\ge T \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n\end{align*}\]</span></p><p>类似地, 对分母有 <span class="math display">\[|\!|{\bf w}_T|\!|^2 \le T\max_n|\!|{\bf x}_n|\!|^2\]</span> 因此, <span class="math display">\[\begin{align*}\frac{ {\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f|\!|} \cdot \frac{ {\bf w}_T}{|\!|{\bf w}_T|\!|} &amp;\ge \frac{T\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n}{|\!|{\bf w}_f|\!|\sqrt{T\max_n|\!|{\bf x}_n|\!|^2}} \\&amp;= \sqrt{T}\cdot \frac{\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n}{|\!|{\bf w}_f|\!|\sqrt{\max_n|\!|{\bf x}_n|\!|^2}}\end{align*}\]</span> 按照Fun TIme中的记法, 记<span class="math inline">\(R^2 = \max_n |\!|{\bf x}_n|\!|^2, \rho = \min_n y_n \frac{{\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f |\!|}{\bf x}_n\)</span>, 则 <span class="math display">\[\frac{ {\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f|\!|} \cdot \frac{ {\bf w}_T}{|\!|{\bf w}_T|\!|} \ge \sqrt{T}\cdot\frac{\rho}{R}\]</span> 由向量除以其长度得到的是单位向量, 长度为<span class="math inline">\(1\)</span>, 在这种情况下, 两者內积越大一定意味着两者夹角越小, 距离越近. 但是这里需要注意的是, 两者的距离不会无限接近, 到<span class="math inline">\(\cos \theta = 1\)</span>时会停止, 因为两个单位向量的內积最大值为1, 因此从上面的不等式可推出 <span class="math display">\[\sqrt{T} \cdot \frac{\rho}{R} \le 1 \Rightarrow T \le \frac{R^2}{\rho^2}\]</span> 即算法至多更新<span class="math inline">\(\frac{R^2}{\rho^2}\)</span>步后一定会停止.</p><h3 id="感知机在线性不可分数据上的应用">感知机在线性不可分数据上的应用</h3><p>由上面的证明, 假设数据集是线性可分的, 那么<span class="math inline">\(PLA\)</span>算法最后肯定会停止, 而且(对训练集)给出正确的分类. 该算法非常容易实现, 而且结束很快, 使用于任意<span class="math inline">\(\mathbb{R}^d\)</span>空间. 但是这个算法最大的问题, 它提前假设训练集是训练可分的, 而且我们不知道算法什么时候会终止(因为上面给出的上限中用到了<span class="math inline">\({\bf w}_f\)</span>, 而我们不知道它是多少--甚至不知道是否存在!(在线性不可分的时候该向量不存在))</p><p>那么我们来考虑一个最坏的情况, 即数据若的确是线性不可分的话, 应该如何应对. 由于数据产生的过程中可能会混入噪声, 这使得原本线性可分的数据也可能因为噪声的存在而不可分. 但是, 一般情况下, 噪声应该是一小部分, 即我们可以退而且其次, 不去寻找一个完美的超平面, 而是去寻找一个犯错误最少的超平面, 即 <span class="math display">\[{\bf w}_g \leftarrow \mathop{{\rm arg}\min}_{\bf w} \sum_{n=1}^N [\![ y_n \not = {\rm sign}({\bf w}^\mathsf{T}{\bf x}_n)]\!]\]</span> 然而, 求解这个问题被证明是NP难的, 只能采用近似算法求解. 例如, 我们可以保存一个最好的权重, 该权重到目前为止错分的数量最少. 该算法被称为&quot;口袋法&quot;, 其完整细节如下:</p><p>设定初始权重<span class="math inline">\(\hat{\bf w}\)</span></p><p>对时刻<span class="math inline">\(t=0, 1, \cdots\)</span></p><ol style="list-style-type: decimal"><li><p>随机寻找一个<span class="math inline">\({\bf w}_t\)</span>错分的样本<span class="math inline">\(({\bf x}_{n(t)}, y_{n(t)})\)</span></p></li><li><p>试图通过如下方法修正<span class="math inline">\({\bf w}_t\)</span> <span class="math display">\[   {\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)}   \]</span></p></li><li><p>如果<span class="math inline">\({\bf w}_{t+1}\)</span>犯的错误比<span class="math inline">\(\hat{\bf w}\)</span>少, 那么将<span class="math inline">\(\hat{\bf w}\)</span>替换为<span class="math inline">\({\bf w}_{t+1}\)</span></p></li></ol><p>直到足够多次迭代完成. 我们将<span class="math inline">\(\hat{\bf w}\)</span>(称为<span class="math inline">\({\bf w}_{pocket}\)</span>)返回为<span class="math inline">\(g\)</span></p><p>注意在线性可分集合上也可以使用口袋法, 算法也可以返回一个无训练误差的解. 但是由于每次更新权重以后, 都要在所有数据上使用新旧权重各跑一遍, 来计算错分数量, 因此口袋法的执行时间通常比原始<span class="math inline">\(PLA\)</span>的计算时间长很多.</p><blockquote><p>+参考 <a href="http://txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/" target="_blank" rel="noopener">txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;感知机假设集合&quot;&gt;感知机假设集合&lt;/h3&gt;
    
    </summary>
    
      <category term="林轩田课程笔记" scheme="http://yoursite.com/categories/%E6%9E%97%E8%BD%A9%E7%94%B0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石 1.学习问题</title>
    <link href="http://yoursite.com/2018/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-1-%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/09/09/机器学习基石-1-学习问题/</id>
    <published>2018-09-09T03:48:35.000Z</published>
    <updated>2018-09-29T08:48:05.977Z</updated>
    
    <content type="html"><![CDATA[<h3 id="机器学习的概念">机器学习的概念</h3><a id="more"></a><p>我们可以从人类的学习思维入手. 人类的学习过程, 是从观察出发, 经过大脑内化以后, 变成有用的技巧. 机器学习, 类似地, 是我们希望让计算机模拟人类的学习过程. 这时, 计算机观察到的东西被称作<strong>数据</strong>, 而思考过程实际上是<strong>计算过程</strong>, 技巧则是<strong>提高某一方面的表现</strong>. 因此,</p><div class="figure"><img src="/2018/09/09/机器学习基石-1-学习问题/1.png" alt="机器学习的过程"><p class="caption">机器学习的过程</p></div><p>为什么需要机器学习?</p><p>给定一张照片, 判断照片里的物体是不是一棵树. 使用传统的方法, 就需要对&quot;什么是树&quot;做出回答, 给出树的定义, 将其实现为程序. 按照规则进行判断, 并将其表述出来是很困难的. 然而, 一个小孩认识树的方法其实是通过观察, 经过经验的积累判断这个是树或者不是, 并不是教条的从长辈那里学习判断规则. 类似地, 我们可以让计算机自己从数据中学习树的判断方法. 因此, <strong>机器学习是构建复杂系统的一种方法</strong></p><p>机器学习的使用场景</p><ul><li><p>当我们不能提前想好各种情况, 手工编码规则时. 例如让机器人在火星上导航, 我们不可能提前想到它在火星上会遇到什么样的情况</p></li><li><p>当我们无法轻易地定义问题的解决方案时. 例如要做语音识别/视觉识别, 我们无法对音频信号做出准确定义</p></li><li><p>当人们需要做出快速决策时. 例如高频交易</p></li><li><p>当要让机器服务于海量用户时. 例如做服务个性化定制</p></li></ul><p>因此, 可以从以下三个关键点进行判断, 看是否适合使用机器学习</p><ol style="list-style-type: decimal"><li>问题是&quot;可以学习的&quot;, 即存在一些潜在的模式, 以至于性能可以被提高</li><li>这些规则难以清晰定义</li><li>手里掌握对应的数据</li></ol><h3 id="机器学习的应用">机器学习的应用</h3><p>机器学习在衣食住行四个方面都得到了广泛地应用</p><ul><li><p>衣: Abu-Mostafa 2012利用销售数据和对用户的调研结果构建推荐系统给用户推荐穿搭</p></li><li><p>食: Sadilek et al. 2013利用机器学习, 根据Twitter数据, 来判断餐厅的好坏</p></li><li><p>住: Tsanas and Xifara 2012利用已有房间的特点和耗能, 预测房屋的能用消耗</p></li></ul><p>此外还有两个领域: 教育和娱乐</p><ul><li>教育: 系统根据学生的答题情况, 有针对地提供题目让学生练习其薄弱的部分, 同时将太难的题推后给出. 即, 给定一名学生的答题历史和一个题目, 预测学生是否能做对这道题( KDDCup 2010 )</li><li>娱乐: 系统根据用户的历史打分, 预测用户对新电影的打分( KDDCup 2011 )</li></ul><h3 id="机器学习的过程">机器学习的过程</h3><h4 id="问题背景">问题背景</h4><p>以银行信用卡发卡这一问题为例. 假设银行收集了一些用户的基本信息, 例如下表</p><table><thead><tr class="header"><th>特征</th><th>值</th></tr></thead><tbody><tr class="odd"><td>年龄</td><td>23</td></tr><tr class="even"><td>性别</td><td>女</td></tr><tr class="odd"><td>所在地居住年数</td><td>1</td></tr><tr class="even"><td>工龄</td><td>0.5</td></tr><tr class="odd"><td>负债额</td><td>200,000</td></tr></tbody></table><p>银行要解决的问题是, 对于这样的客户, 是否应该给她发放信用卡</p><h4 id="问题形式化描述">问题形式化描述</h4><p>为了更加形式化地描述这个问题, 需要先定义一些符号:</p><ul><li><strong>输入</strong>: <span class="math inline">\({\bf x} \in \mathcal{X}\)</span>, 用户的特征</li><li><strong>输出</strong>: <span class="math inline">\({\bf y} \in \mathcal{Y}\)</span>, 是否发放信用卡</li><li><strong>目标函数</strong>: <span class="math inline">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span>, 是我们期望学到, 但是目前不知道的东西. 是最理想的公式</li><li><strong>数据</strong>: <span class="math inline">\(\mathcal{D} = \{({\bf x}_1, y_1), ({\bf x}_2, y_2), \ldots, ({\bf x}_n, y_n)\}\)</span>, 是之前积累的记录</li><li><strong>假设</strong>: <span class="math inline">\(g: \mathcal{X} \rightarrow \mathcal{Y}\)</span>, 是机器从数据中学到的函数. 我们通常都希望<span class="math inline">\(g\)</span>的表现足够好, 即<span class="math inline">\(g \approx f\)</span>. 注意这里<span class="math inline">\(g\)</span>不一定等于<span class="math inline">\(f\)</span>(实际上, 我们永远也无法知道真正的<span class="math inline">\(f\)</span>是什么样子, 只知道由<span class="math inline">\(f\)</span>产生的数据<span class="math inline">\(\mathcal{D}\)</span>)</li><li><strong>机器学习算法</strong>: <span class="math inline">\(\mathcal {A}\)</span>, 是由<span class="math inline">\(\mathcal {D}\)</span>产生<span class="math inline">\(g\)</span>的算法, 可以理解为<span class="math inline">\(\mathcal {A}\)</span>会从各种不同假设<span class="math inline">\(h_k\)</span>(这里<span class="math inline">\(h_k\)</span>有好有坏)构成的集合<span class="math inline">\(\mathcal{H}\)</span>中挑选出来一个最好的<span class="math inline">\(g\)</span>, 使得<span class="math inline">\(g \approx f\)</span>. 即<span class="math inline">\(\mathcal{A}\)</span>以<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(\mathcal{H}\)</span>为输入, 以<span class="math inline">\(g\)</span>为输出</li></ul><div class="figure"><img src="/2018/09/09/机器学习基石-1-学习问题/2.png" alt="机器学习过程"><p class="caption">机器学习过程</p></div><p><strong>我们所讲的机器学习模型, 指的就是</strong><span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{H}\)</span></p><p>在有个这些记号以后, 我们可以重新给机器学习下一个定义</p><blockquote><p>机器学习是使用数据计算假设<span class="math inline">\(g\)</span>以逼近目标函数<span class="math inline">\(f\)</span>的过程</p></blockquote><h3 id="机器学习与其它名词">机器学习与其它名词</h3><h4 id="机器学习与数据挖掘">机器学习与数据挖掘</h4><p>数据挖掘的一个简单定义是使用海量数据, 在其中找出一些有趣的现象或性质. 这里, 如果&quot;有用的性质&quot;就是&quot;能够逼近目标函数的假设&quot;, 那么数据挖掘和机器学习是没有区别的. 如果&quot;有用的性质&quot;与&quot;假设&quot;是相关联的, 那么数据挖掘在很大程度上可以帮助机器学习</p><p>传统上的数据挖掘还关注如何在大的数据集中进行有效计算, 不过现在已经很难将机器学习和数据挖掘这两个概念分开了.</p><h4 id="机器学习与人工智能">机器学习与人工智能</h4><p>人工智能要求计算机呈现出一些智能的行为. 由于机器学习逼近目标函数的过程就展示了一些智能, 因此我们可以说, 机器学习是实现人工智能的一种手段.</p><h4 id="机器学习与统计学">机器学习与统计学</h4><p>统计学是使用数据来对未知过程进行推论. 考虑到假设<span class="math inline">\(g\)</span>是推论结果, <span class="math inline">\(f\)</span>是不知道的事, 那么可以说统计是实现机器学习的一种方法. 但是传统的统计学从数学出发, 很多工具是为数学假设提供证明和推论. 而机器学习看重的是如何计算出结果. 总而言之, 统计学为机器学习提供了很多有力的工具</p><blockquote><p>+参考 <a href="http://txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/" target="_blank" rel="noopener">txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;机器学习的概念&quot;&gt;机器学习的概念&lt;/h3&gt;
    
    </summary>
    
      <category term="林轩田课程笔记" scheme="http://yoursite.com/categories/%E6%9E%97%E8%BD%A9%E7%94%B0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
