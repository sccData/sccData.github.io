<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Numpy基础(一)]]></title>
    <url>%2F2018%2F09%2F16%2FNumpy%E5%9F%BA%E7%A1%80-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Numpy的ndarray: 一种多维数组对象 1import numpy as np 1data = np.random.randn(2, 3) 1data array([[-0.27145581, 0.20453947, 1.20194872], [-0.02400358, 0.24259435, -0.65364157]]) 1data * 10 array([[-2.71455807, 2.04539469, 12.01948724], [-0.24003576, 2.4259435 , -6.53641571]]) 1data + data array([[-0.54291161, 0.40907894, 2.40389745], [-0.04800715, 0.4851887 , -1.30728314]]) 1data.shape (2, 3) 1data.dtype dtype(&#39;float64&#39;) 创建ndarray创建数组最简单的办法就是使用array函数. 它接受一切序列型的对象(包括其他数组), 然后产生一个新的含有传入数据的Numpy数组 1data1 = [6, 7.5, 8, 0, 1] 1arr1 = np.array(data1) 1arr1 array([6. , 7.5, 8. , 0. , 1. ]) 1data2 = [[1, 2, 3, 4], [5, 6, 7, 8]] 1arr2 = np.array(data2) 1arr2 array([[1, 2, 3, 4], [5, 6, 7, 8]]) 1arr2.ndim 2 1arr2.shape (2, 4) np.array会自动推断生成数组的数据类型 1arr1.dtype dtype(&#39;float64&#39;) 1arr2.dtype dtype(&#39;int64&#39;) 1np.zeros(10) array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 1np.zeros((3, 6)) array([[0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.]]) 1np.empty((2, 3, 2)) array([[[1.74618620e-076, 3.97062373e+246], [1.16318408e-028, 2.21471564e+160], [2.59982058e-056, 3.59853464e+179]], [[5.93300900e-038, 5.04621361e+180], [8.37170571e-144, 1.01849500e+248], [1.16096643e-028, 5.30581644e+180]]]) 1np.arange(15) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) 1np.ones(10) array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) array默认复制所有输入数据; asarray, 如果输入已经是ndarray, 则不再复制 12a = [1, 2, 3]np.asarray(a) array([1, 2, 3]) 1np.eye(2) array([[1., 0.], [0., 1.]]) 1np.zeros_like(data2) array([[0, 0, 0, 0], [0, 0, 0, 0]]) ndarray的数据类型12import numpy as nparr1 = np.array([1, 2, 3], dtype=np.float64) 1arr2 = np.array([1, 2, 3], dtype=np.int32) 1arr1.dtype dtype(&#39;float64&#39;) 1arr2.dtype dtype(&#39;int32&#39;) 转换数组的数据类型 1arr = np.array([1, 2, 3, 4, 5]) 1arr.dtype dtype(&#39;int64&#39;) 1float_arr = arr.astype(np.float64) 1float_arr.dtype dtype(&#39;float64&#39;) 在本例中, 整数被转换成了浮点数.如果浮点数转换成整数, 则小数部分将会被截取删除 1arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1]) 1arr array([ 3.7, -1.2, -2.6, 0.5, 12.9, 10.1]) 1arr.astype(np.int32) array([ 3, -1, -2, 0, 12, 10], dtype=int32) 如果某字符串数组表示全是数字, 也可以用astype将其转换成数值形式 1numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_) 1numeric_strings array([b&#39;1.25&#39;, b&#39;-9.6&#39;, b&#39;42&#39;], dtype=&#39;|S4&#39;) 1numeric_strings.astype(float) array([ 1.25, -9.6 , 42. ]) 123int_array = np.arange(10)calibers = np.array([.22, .270, .357, .380, .44, .50], dtype = np.float64)int_array.astype(calibers.dtype) array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) 1empty_uint32 = np.empty(8, dtype='u4') 1empty_uint32 array([112, 111, 114, 97, 114, 105, 108, 121], dtype=uint32) 使用astype时总是生成一个新的数组 Numpy数组的运算1arr = np.array([[1., 2., 3.], [4., 5., 6.]]) 1arr array([[1., 2., 3.], [4., 5., 6.]]) 1arr * arr array([[ 1., 4., 9.], [16., 25., 36.]]) 1arr - arr array([[0., 0., 0.], [0., 0., 0.]]) 11/arr array([[1. , 0.5 , 0.33333333], [0.25 , 0.2 , 0.16666667]]) 1arr ** 0.5 array([[1. , 1.41421356, 1.73205081], [2. , 2.23606798, 2.44948974]]) 1arr2 = np.array([[0., 4., 1.], [7., 2., 12.]]) 1arr2 array([[ 0., 4., 1.], [ 7., 2., 12.]]) 1arr2 &gt; arr array([[False, True, False], [ True, False, True]]) 不同大小的数组之间的运算叫做广播 基本的索引和切片1arr = np.arange(10) 1arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 1arr[5] 5 1arr[5:8] array([5, 6, 7]) 1arr[5:8] = 12 1arr array([ 0, 1, 2, 3, 4, 12, 12, 12, 8, 9]) 如上所示, 当你将标量值赋值给一个切片时, 该值会自动传播到整个选区.跟列表最重要的区别在于, 数组切片是原始数组的视图. 这意味着数据不会被复制, 视图上的任何修改都会直接反映到原数组上. 1arr_slice = arr[5:8] 1arr_slice array([12, 12, 12]) 当改变arr_slice, 变化也会体现在原数组上 1arr_slice[1] = 12345 1arr array([ 0, 1, 2, 3, 4, 12, 12345, 12, 8, 9]) 1arr_slice[:] = 64 1arr array([ 0, 1, 2, 3, 4, 64, 64, 64, 8, 9]) 如果你想要得到的是ndarry切片的一份副本而非视图, 就需要明确地进行复制操作, 例如arr[5:8].copy() 1arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 1arr2d[2] array([7, 8, 9]) 1arr2d[0][2] 3 1arr2d[0, 2] 3 轴0作为行, 轴1作为列 1arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) 1arr3d array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[0] array([[1, 2, 3], [4, 5, 6]]) 标量和数组都可以传递给arr3d[0]: 12345old_values = arr3d[0].copy()arr3d[0] = 42In [48]:arr3d array([[[42, 42, 42], [42, 42, 42]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[0] = old_values 1arr3d array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[1, 0] array([7, 8, 9]) 1x = arr3d[1] 1x array([[ 7, 8, 9], [10, 11, 12]]) 1x[0] array([7, 8, 9]) 在上面所有这些选取数组子集的例子中, 返回的数组都是视图 布尔索引1names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe']) 1data = np.random.randn(7, 4) 1names array([&#39;Bob&#39;, &#39;Joe&#39;, &#39;Will&#39;, &#39;Bob&#39;, &#39;Will&#39;, &#39;Joe&#39;, &#39;Joe&#39;], dtype=&#39;&lt;U4&#39;) 1data array([[ 0.33750495, 0.87858221, -1.17761082, 2.34847091], [-0.13196555, 0.52047032, 0.4773208 , 0.23596828], [ 1.3720038 , -0.70659583, -0.23109342, -0.12345765], [-0.34662676, -0.33928776, -1.42790341, 0.84134272], [-0.15224377, -1.1089024 , -0.97038997, 0.56403746], [-0.25892721, -0.60506496, 1.86708232, 0.132893 ], [ 0.45939055, -0.28324557, 2.50077175, -0.22691978]]) 1names == 'Bob' array([ True, False, False, True, False, False, False]) 1data[names=='Bob'] array([[ 0.33750495, 0.87858221, -1.17761082, 2.34847091], [-0.34662676, -0.33928776, -1.42790341, 0.84134272]]) 1data[names=='Bob', 2:] array([[-1.17761082, 2.34847091], [-1.42790341, 0.84134272]]) 1data[names=='Bob', 3] array([2.34847091, 0.84134272]) 1names != 'Bob' array([False, True, True, False, True, True, True]) 1data[~(names == 'Bob')] array([[-0.13196555, 0.52047032, 0.4773208 , 0.23596828], [ 1.3720038 , -0.70659583, -0.23109342, -0.12345765], [-0.15224377, -1.1089024 , -0.97038997, 0.56403746], [-0.25892721, -0.60506496, 1.86708232, 0.132893 ], [ 0.45939055, -0.28324557, 2.50077175, -0.22691978]]) 1cond = names == 'Bob' 1data[~cond] array([[-0.13196555, 0.52047032, 0.4773208 , 0.23596828], [ 1.3720038 , -0.70659583, -0.23109342, -0.12345765], [-0.15224377, -1.1089024 , -0.97038997, 0.56403746], [-0.25892721, -0.60506496, 1.86708232, 0.132893 ], [ 0.45939055, -0.28324557, 2.50077175, -0.22691978]]) 1mask = (names == 'Bob') | (names == 'will') 1data[mask] array([[ 0.33750495, 0.87858221, -1.17761082, 2.34847091], [-0.34662676, -0.33928776, -1.42790341, 0.84134272]]) 使用布尔值索引数据时, 总是生成数据的拷贝, 即使返回的数组并没有任何变化 Python的关键字and和or对布尔值数组并没有用, 请使用&amp;和|代替 1data[data&lt;0] = 0 1data array([[0.33750495, 0.87858221, 0. , 2.34847091], [0. , 0.52047032, 0.4773208 , 0.23596828], [1.3720038 , 0. , 0. , 0. ], [0. , 0. , 0. , 0.84134272], [0. , 0. , 0. , 0.56403746], [0. , 0. , 1.86708232, 0.132893 ], [0.45939055, 0. , 2.50077175, 0. ]]) 1data[names!='Joe'] = 7 1data array([[7. , 7. , 7. , 7. ], [0. , 0.52047032, 0.4773208 , 0.23596828], [7. , 7. , 7. , 7. ], [7. , 7. , 7. , 7. ], [7. , 7. , 7. , 7. ], [0. , 0. , 1.86708232, 0.132893 ], [0.45939055, 0. , 2.50077175, 0. ]]) 神奇索引1arr = np.empty((8, 4)) 12for i in range(8): arr[i] = i 1arr array([[0., 0., 0., 0.], [1., 1., 1., 1.], [2., 2., 2., 2.], [3., 3., 3., 3.], [4., 4., 4., 4.], [5., 5., 5., 5.], [6., 6., 6., 6.], [7., 7., 7., 7.]]) 为了选出一个符合特定顺序的子集, 可以通过传递一个包含指明所需顺序的列表或数组来完成 1arr[[4, 3, 0, 6]] array([[4., 4., 4., 4.], [3., 3., 3., 3.], [0., 0., 0., 0.], [6., 6., 6., 6.]]) 1arr[[-3, -5, -7]] array([[5., 5., 5., 5.], [3., 3., 3., 3.], [1., 1., 1., 1.]]) 1arr = np.arange(32).reshape((8, 4)) 1arr array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]) 1arr[[1, 5, 7, 2], [0, 3, 1, 2]] array([ 4, 23, 29, 10]) 1arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]] array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) 神奇索引和切片不同, 它总是将数据复制到一个新的数组中 数组转置和换轴1arr = np.arange(15).reshape((3, 5)) 1arr array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) 1arr.T array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 1arr = np.random.randn(6, 3) 1arr array([[-0.34408898, -1.37979923, -1.31569516], [ 0.4300182 , 0.53458254, 2.31329317], [ 0.91202655, -0.8713753 , 0.69748309], [ 1.80515511, -0.30402588, 0.59708737], [-0.72718466, 1.16915288, -1.35944484], [ 0.70524754, -0.82653288, -0.23491233]]) 1np.dot(arr.T, arr) array([[ 7.34791663, -1.02712211, 2.08288264], [-1.02712211, 13.10926223, 3.60236728], [ 2.08288264, 3.60236728, 5.8434541 ]]) 换轴 对于高维数组, transpose需要用到一个由轴编号组成的元组, 才能进行转置 对多维数组来说, 确定最底层的一个基本元素位置需要用到的索引个数是维度. 这里可以理解为对shape返回元组的索引 1arr = np.arange(16).reshape((2, 2, 4)) 1arr array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) 1arr.transpose((1, 0, 2)) array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) 1arr.swapaxes(1, 2) array([[[ 0, 4], [ 1, 5], [ 2, 6], [ 3, 7]], [[ 8, 12], [ 9, 13], [10, 14], [11, 15]]]) swapaxes返回的是数据的视图. 而没有对数据进行复制]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习基石 2.学习判断是与非]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%8E%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-2-%E5%AD%A6%E4%B9%A0%E5%88%A4%E6%96%AD%E6%98%AF%E4%B8%8E%E9%9D%9E%2F</url>
    <content type="text"><![CDATA[感知机假设集合 第一章里讲到机器学习的核心就是, 使用算法\(\mathcal{A}\)接受数据\(\mathcal{D}\), 从假设集合(所有可能性)\(\mathcal{H}\)中选出一个\(g\), 希望\(g \approx f\). 那么我们现在最关心的就是, \(\mathcal{H}\)应该是什么样的. 以之前提到的银行审核发放信用卡的场景为例, 假设我们把每个使用者定义为向量\(\bf x\), 包含\(d\)个维度, 例如\(x_1\)代表年龄, \(x_2\)代表年薪, 等等. 我们可以将这些维度综合起来给使用者一个整体的分数. 如果这个分数超过了某个标准, 那么就发放信用卡; 否则拒绝发放. 这样, 我们需要给每个\(x_i, i \in \{ 1, \ldots, d \}\)来赋一个系数\(w_i\), 如果特征对最后的影响是正面的, 那么就给\(w_i\)正值, 否则给负值. 如果我们在规定一个阈值\(\rm threshold\), 那么我们的决策方法就可以写成为, 如果\(\sum_{i=1}^d w_ix_i &gt; \rm threshold\), 就批准信用卡申请, 否则就拒绝. 我们可以进一步地规定输出空间\(\mathcal{Y} \in \{-1, +1\}\), 其中\(y=-1\)时表示拒绝, \(y=1\)时表示许可. 这样做的好处是我们可以直接使用\(\rm sign\)函数来求出\(y\)的值, 具体地说, 假设集合\(\mathcal{H}\)中的每个元素\(h \in \mathcal{H}\)都有如下形式 \[ h({\bf x}) ={\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold}) \] 其中\({\rm sign}\)函数的定义为 \[ {\rm sign}(x) = \begin{cases} +1 &amp; {\rm if \ } x&gt;0 \\ -1 &amp; {\rm if \ } x&lt;0 \end{cases} \] 即对用户的所有属性做一个加权打分, 看它是否超过阈值. 如果超过, 则批准; 否则就拒绝(如果正好等于阈值, 这种情况很少发生, 甚至可以随机决定\(y\)是\(-1\)还是\(1\)). 这里我们说\(\mathcal{H}\)是一个集合的原因是, 不同的\(\bf w\)和\(\rm threshold\)都对应了不同的\(h\), 所有这些可能性对应的所有\(h\)构成了最后的假设集合\(\mathcal{H}\). \(h\)这样的函数类型称为感知机(perceptron), 其中\(\bf w\)称为权重. 进一步地, 假设我们把\(-\rm threshold\)看做是\((-\rm threshold) \cdot (+1)\), 然后把\(+1\)看作是\(x_0\), 那么前面的公式形式可以进一步的简化, 即 \[ \begin{align*} h({\bf x}) &amp;= {\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold}) \\ &amp;= {\rm sign}((\sum_{i=1}^d w_ix_i)+\underbrace{(-{\rm threshold})}_{w_0}\cdot \underbrace{(+1)}_{x_0}) \\ &amp;= {\rm sign}(\sum_{i=0}^d w_ix_i) \\ &amp;= {\rm sign}({\bf w}^\mathsf{T}{\bf x}) \end{align*} \] 这里\(\bf w\)和\(\bf x\)都看作是列向量, 即维度为\((d+1)1\) 我们可以通过一个图例来加强理解. 假如我们顾客的特征数(也就是前面说的属性维度)为\(2\), 那么我们可以把任意输入\(\bf x\)画在一个平面\(\mathbb{R}^2\)上(类似的, 如果特征数为\(d\), 那么每个输入\(\bf x\)都可以在\(\mathbb{R}^d\)空间表示, 只是会对我们的可视化造成困难), 每个输入对应平面上的一个点. 这样, \(\mathbb{R}^2\)上的\(h\)都有如下形式: \[ h({\bf x}) = \rm sign(w_0+w_1x_1+w_2x_2) \] 可以看出, 每个\(h\)其实都对应了\(\mathbb{R}^2\)上的一条直线. 感知机规定位于直线某一侧的样本都被判定为正例, 另一侧的样本都被判定为负例. 不同的权重会产生不同的分类方式. 假设我们用蓝色的圈o表示正例, 红色的叉×表示负例, 下图给出了两个不同的感知机 1 2 可以看出来右边的感知机在训练集上效果更好, 因为它对所有例子做出了正确分类. 而左侧的感知机在训练集上表现稍逊(一个正例被误判为负, 两个负例被误判为正) 由于感知机都对应于一个超平面, 因此它也被称为是线性分类器(\(\mathbb R^2\)的超平面是一条直线, \(\mathbb R^3\)的超平面是一个平面, 以此类推). 感知机学习算法 在我们知道了\(h \in \mathcal H\)的形态以后, 接下来的问题是设计算法\(\mathcal A\)来选出最优的\(g\)来逼近理想的\(f\). 尽管我们不知道\(f\)具体应该是什么, 但是我们知道数据\(\mathcal D\)是由\(f\)生成的. 因此我们有理由相信, 好的\(g\)满足对所有我们已经收集道的数据, 其输出与\(f\)的输出尽可能接近, 即\(g({\bf x}_n) = f({\bf x}_n) = y_n\). 因此, 我们可以先找一个超平面, 至少能够对训练集中的数据正确分类. 然而难度在于, \(\mathcal H\)的大小通常都是无限的. 一种解决方案是, 我们可以先初始化一个超平面\(g_0\)为了简单起见将其以其权重(为了简单起见, 将其以其权重\({\bf w}_0\)代表, 称为初始权重). 我们允许这个超平面犯错, 但我们要设计算法, 让超平面遇到\(\mathcal D\)中的错分样本以后可以修正自己. 通常我们可以将\({\bf w}_0\)初始化为零向量\(\bf 0\). 然后, 在每一步\(t\), 找到一个使\({\bf w}_t\)错分的样本错分的样本\(({\bf x}_{n(t)}, y_{n(t)})\). 即有 \[ \rm sign({\bf w}^T_t {\bf x}_{n(t)}) \not= y_{n(t)} \] 接下里我们试着修正\({\bf w}_t\). 可以看到错分有两种情况: \(y\)本来应该是\(+1\), 但是模型判断出来是负值. 也就是说此时\(\bf w\)与\(\bf x\)之间的角度太大, 因此需要把\(\bf w\)往靠近\(\bf x\)的方向旋转使它们的角度变小. 可以通过让\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf x}_{n(t)}\)达到这个目的 \(y\)本来应该是\(-1\), 但是模型判断出来是正值. 也就是说此时\(\bf w\)与\(\bf x\)之间的角度太小, 因此需要把\(\bf w\)往远离\(\bf x\)的方向旋转使它们的角度变大. 考虑到符号, 其实也可以通过让\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf w}_{n(t)}\)达到这个目的 因此, 在\(t+1\)时刻, 我们总可以通过下式来修正\({\bf w}_t\), 即 \[ {\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)} \] 3 4 感知机学习算法(Perceptron Learning Algorithm, PLA)就是重复上面的过程, 直到没有错误发生为止. 算法将最后得到的权重\(\bf w\)(记做\({\bf w}_{PLA}\))返回\(g\). 完整写法如下: 对于\(t = 0,1, \ldots\) 1). 找到一个使\({\bf w}_t\)错分的样本\(({\bf x}_{n(t)}, y_{n(t)})\). 即有 \[ sign({\bf w}_t^T {\bf x}_{n(t)}) \not = y_{n(t)} \] 2). 以如下方法修正\({\bf w}_t\): \[ {\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)} \] 直到便利了所有样本一遍以后都没有找到错误为止. 由此, 也引出两个问题: 算法真的会停止吗? 能否确定算法返回的\(g \approx f\)?]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F09%2F09%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习基石 1.学习问题]]></title>
    <url>%2F2018%2F09%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-1-%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[机器学习的概念 我们可以从人类的学习思维入手. 人类的学习过程, 是从观察出发, 经过大脑内化以后, 变成有用的技巧. 机器学习, 类似地, 是我们希望让计算机模拟人类的学习过程. 这时, 计算机观察到的东西被称作数据, 而思考过程实际上是计算过程, 技巧则是提高某一方面的表现. 因此, 为什么需要机器学习? 给定一张照片, 判断照片里的物体是不是一棵树. 使用传统的方法, 就需要对”什么是树”做出回答, 给出树的定义, 将其实现为程序. 按照规则进行判断, 并将其表述出来是很困难的. 然而, 一个小孩认识树的方法其实是通过观察, 经过经验的积累判断这个是树或者不是, 并不是教条的从长辈那里学习判断规则. 类似地, 我们可以让计算机自己从数据中学习树的判断方法. 因此, 机器学习是构建复杂系统的一种方法 机器学习的使用场景 当我们不能提前想好各种情况, 手工编码规则时. 例如让机器人在火星上导航, 我们不可能提前想到它在火星上会遇到什么样的情况 当我们无法轻易地定义问题的解决方案时. 例如要做语音识别/视觉识别, 我们无法对音频信号做出准确定义 当人们需要做出快速决策时. 例如高频交易 当要让机器服务于海量用户时. 例如做服务个性化定制 因此, 可以从以下三个关键点进行判断, 看是否适合使用机器学习 问题是”可以学习的”, 即存在一些潜在的模式, 以至于性能可以被提高 这些规则难以清晰定义 手里掌握对应的数据 机器学习的应用机器学习在衣食住行四个方面都得到了广泛地应用 衣: Abu-Mostafa 2012利用销售数据和对用户的调研结果构建推荐系统给用户推荐穿搭 食: Sadilek et al. 2013利用机器学习, 根据Twitter数据, 来判断餐厅的好坏 住: Tsanas and Xifara 2012利用已有房间的特点和耗能, 预测房屋的能用消耗 此外还有两个领域: 教育和娱乐 教育: 系统根据学生的答题情况, 有针对地提供题目让学生练习其薄弱的部分, 同时将太难的题推后给出. 即, 给定一名学生的答题历史和一个题目, 预测学生是否能做对这道题( KDDCup 2010 ) 娱乐: 系统根据用户的历史打分, 预测用户对新电影的打分( KDDCup 2011 ) 机器学习的过程问题背景以银行信用卡发卡这一问题为例. 假设银行收集了一些用户的基本信息, 例如下表 特征 值 年龄 23 性别 女 所在地居住年数 1 工龄 0.5 负债额 200,000 银行要解决的问题是, 对于这样的客户, 是否应该给她发放信用卡 问题形式化描述为了更加形式化地描述这个问题, 需要先定义一些符号: 输入: ${\bf x} \in \mathcal{X}$, 用户的特征 输出: ${\bf y} \in \mathcal{Y}$, 是否发放信用卡 目标函数: $f: \mathcal{X} \rightarrow \mathcal{Y}$, 是我们期望学到, 但是目前不知道的东西. 是最理想的公式 数据: $\mathcal{D} = {({\bf x}_1, y_1), ({\bf x}_2, y_2), \ldots, ({\bf x}_n, y_n)}$, 是之前积累的记录 假设: $g: \mathcal{X} \rightarrow \mathcal{Y}$, 是机器从数据中学到的函数. 我们通常都希望$g$的表现足够好, 即$g \approx f$. 注意这里$g$不一定等于$f$(实际上, 我们永远也无法知道真正的$f$是什么样子, 只知道由$f$产生的数据$\mathcal{D}$) 机器学习算法: $\mathcal {A}$, 是由$\mathcal {D}$产生$g$的算法, 可以理解为$\mathcal {A}$会从各种不同假设$h_k$(这里$h_k$有好有坏)构成的集合$\mathcal{H}$中挑选出来一个最好的$g$, 使得$g \approx f$. 即$\mathcal{A}$以$\mathcal{D}$和$\mathcal{H}$为输入, 以$g$为输出 我们所讲的机器学习模型, 指的就是$\mathcal{A}$和$\mathcal{H}$ 在有个这些记号以后, 我们可以重新给机器学习下一个定义 机器学习是使用数据计算假设$g$以逼近目标函数$f$的过程 机器学习与其它名词机器学习与数据挖掘数据挖掘的一个简单定义是使用海量数据, 在其中找出一些有趣的现象或性质. 这里, 如果”有用的性质”就是”能够逼近目标函数的假设”, 那么数据挖掘和机器学习是没有区别的. 如果”有用的性质”与”假设”是相关联的, 那么数据挖掘在很大程度上可以帮助机器学习 传统上的数据挖掘还关注如何在大的数据集中进行有效计算, 不过现在已经很难将机器学习和数据挖掘这两个概念分开了. 机器学习与人工智能人工智能要求计算机呈现出一些智能的行为. 由于机器学习逼近目标函数的过程就展示了一些智能, 因此我们可以说, 机器学习是实现人工智能的一种手段. 机器学习与统计学统计学是使用数据来对未知过程进行推论. 考虑到假设$g$是推论结果, $f$是不知道的事, 那么可以说统计是实现机器学习的一种方法. 但是传统的统计学从数学出发, 很多工具是为数学假设提供证明和推论. 而机器学习看重的是如何计算出结果. 总而言之, 统计学为机器学习提供了很多有力的工具 +参考 txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
