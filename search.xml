<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[复杂度分析（下）]]></title>
    <url>%2F2018%2F09%2F29%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-2-%E7%8E%8B%E4%BA%89%2F</url>
    <content type="text"><![CDATA[+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除 最好、最坏时间复杂度 先给个例子: // n 表示数组 array 的长度 int find(int[] array, int n, int x){ int i = 0; int pos = -1; for(; i&lt;n; ++i){ if (array[i] == x){ pos = i; } } return pos } 按照上次的分析方法, 这段代码的复杂度是\(O(n)\), 其中, n代表数组的长度. 但是在数组中查找一个数据, 不需要从头到尾遍历一遍, 在中间可能就可以提前结束循环. 优化代码. // n 表示数组 array 的长度 int find(int[] array, int n, int x){ int i = 0; int pos = -1; for(; i&lt;n; ++i){ if(array[i] == x){ pos = i; break; } } return pos; } 如果数组的第一个元素正好是要查找的变量\(x\), 那时间复杂度是\(O(1)\). 但是如果数组中不存在变量\(x\), 那时间复杂度是\(O(n)\). 由此引入三个概念: 最好情况时间复杂度、最坏情况时间复杂度和平均情况复杂度. 最好情况时间复杂度和最坏情况时间复杂度都很好理解, 重点记录一下平均情况时间复杂度. 平均情况时间复杂度 查找变量x在数组中的位置, 有n+1种情况: 在数组的\(0 \sim n-1\)位置中和不在数组中. 把每种情况下, 需要遍历的元素累加起来, 然后再除以n+1, 就可以得到需要遍历的元素个数的平均值, 即: \[ \frac{1+2+3+\cdots+n-1+n}{n+1} = \frac{n(n+3)}{2(n+1)} \] 将其简化后可得\(O(n)\). 但是, 这个结果是有问题的, 因为每种情况出现的概率不一样. 要查找的变量\(x\), 要么在数组里, 要么不在数组里. 为了方便理解, 假设在数组中与不在数组中的概率都为\(\frac{1}{2}\). 另外, 要查找的数据出现在\(0 \sim n-1\)这\(n\)个位置的概率也是一样的, 为\(\frac{1}{n}\). 所以, 根据概率乘法法则, 要查找的数据出现在\(0 \sim n-1\)中任意位置的概率就是\(\frac{1}{2n}\). 计算过程为: \[ 1 \times \frac{1}{2n}+2 \times \frac{1}{2n}+3\times\frac{1}{2n}+ \cdots +n \times \frac{1}{2n} + n \times \frac{1}{2} = \frac{3n+1}{4} \] 这个值就是概率论中的加权平均值, 也叫作期望值, 所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度. 这段代码的加权平均时间复杂度仍然是\(O(n)\). 均摊时间复杂度 均摊时间复杂度, 听起来跟平均时间复杂度有点儿像. 但是, 其应用场景更加特殊、更加有限. // array 表示一个长度为 n 的数组 // 代码中的 array.length 就等于 n int[] array = new int[n]; int count = 0; void insert(int val){ if(count == array.length){ int sum = 0; for(int i=0; i&lt;array.length; ++i){ sum = sum + array[i]; } array[0] = sum; count = 1; } array[count] = val; ++count; } 最理想的情况是, 数组中有空闲空间, 我们只需要将数据插入到数组下表为\(count\)的位置就可以了, 所以最好情况时间复杂度为\(O(1)\). 最坏的情况下, 数组中没有空闲空间了, 我们需要先做一次数组的遍历求和, 然后再将数据插入, 所以最坏情况时间复杂度为\(O(n)\). 而平均时间复杂度是\(O(1)\). 我们还是可以通过概率论的方法来分析. 假设数组的长度是n, 根据数据插入的位置的不同, 我们可以分为n种情况, 每种情况的时间复杂度是O(1). 除此之外, 还有一种&quot;额外&quot;的情况, 就是在数组没有空闲空间时插入一个数据, 这个时候的时间复杂度是O(n). 而且, 这n+1种情况发生的概率一样, 都是. 所以, 根据加权平均的计算方法, 我们求得的平均时间复杂度就是: \[ 1 \times \frac{1}{n+1}+1\times\frac{1}{n+1}+\cdots+1\times\frac{1}{n+1}+n\times\frac{1}{n+1} = O(1) \] 其实这个例子不需要引入概率论的知识, 对比一下\(insert()\)和\(find()\)的例子, 就会发现这两者有很大的差别. 首先, \(find()\)在极端情况下, 复杂度才为\(O(1)\). 但\(insert()\)在大部分情况下, 时间复杂度都为\(O(1)\). 只有个别情况下, 复杂度才为\(O(n)\). 这是\(insert()\)第一个区别与\(find()\)的地方. 第二个不同点, 对于\(insert()\)函数来说, \(O(1)\)时间复杂度的插入和\(O(n)\)时间复杂度的插入, 出现的频率是非常有规律的, 而且有一定的前后时序关系, 一般都是一个\(O(n)\)插入之后, 紧跟着\(n-1\)个\(O(1)\)的插入操作, 循环往复. 针对这样一种特殊场景的复杂度分析, 我们并不需要像之前那样, 找出所有的输入情况及相应的发生概率, 然后计算加权平均值. 为此, 引入一种更加简洁的分析方法: 摊还分析法, 通过摊还分析法得到的时间复杂度我们起了一个名字, 叫均摊时间复杂度. 针对上述例子, 每一次\(O(n)\)的插入操作, 都会跟着\(n-1\)次\(O(1)\)的插入操作, 所以把耗时多的那次操作均摊到接下来的\(n-1\)次耗时少的操作上, 均摊下来, 这一组连续的操作的均摊时间复杂度就是\(O(1)\). 总结其应用场景: 对一个数据结构进行一组连续操作中, 大部分情况下时间复杂度都很低, 只有个别情况下时间复杂度比较高, 而且这些操作之间存在前后连贯的时序关系, 这个时候, 我们就可以将一组操作放在一块儿分析, 看是否能将较高时间复杂度那次操作的耗时, 平摊到其他那些时间复杂度比较低的操作上. 而且, 能够应用均摊时间复杂度分析的场合, 一般均摊时间复杂度就等于最好情况时间复杂度.]]></content>
      <categories>
        <category>王争课程笔记</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂度分析（上）]]></title>
    <url>%2F2018%2F09%2F26%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-%E7%8E%8B%E4%BA%89%2F</url>
    <content type="text"><![CDATA[+文本内容是对王争《数据结构与算法之美》课程的笔记， 如果有任何侵权行为， 请联系博主删除 为什么需要复杂度分析？ 很多人对复杂度分析有疑问, 认为直接在机器上跑一遍, 就可以得出时间和空间复杂度. 对于这种说法, 我们认为是正确的, 并且很多书籍将其称为事后统计. 但是, 这种方法有很大的局限性. 测试结果依赖于测试环境 不同的硬件对测试结果影响较大 测试结果受数据规模的影响很大 数据规模的大小和有序度, 对测试结果影响较大 所以, 我们需要一个不用具体的测试数据来测试, 就可以粗略地估计算法的执行效率的方法. 大\(O\)复杂度表示法 以一段代码为例来估计算法的执行时间 int cal(int n) { int sum = 0; int i = 1; for(; i &lt;= n; ++i){ sum = sum + i; } return sum; } 由于是粗略估计, 假设每行代码执行的时间都一样, 为\(t\). 第2、3行代码分别需要1个\(t\)的执行时间, 第4、5行都运行了\(n\)遍, 所以需要\(2 n * t\)的执行时间, 所以这段代码总的执行时间就是\((2 n + 2) * t\). 可以看出来, 所有的代码执行时间\(T(n)\)与每行代码的执行次数成正比. 再看一段代码 int cal(int n) { int sum = 0; int i = 1; int j = 1; for(; i &lt;= n; ++i){ j = 1; for(; j &lt;= n; ++j){ sum = sum + i * j; } } } 根据以上思路, 可以得出$T(n) = (2n^2 + 2n + 3) * t $. 从中我们可以总结得到一个非常重要的规律, 所有代码的执行时间\(T(n)\)与每行代码的执行次数\(n\)成正比 \[ T(n) = O(f(n)) \] 其中\(T(n)\)表示代码执行的时间; n表示数据规模的大小; \(f(n)\)表示每行代码执行的次数总和. 公式中的\(O\), 表示代码的执行时间\(T(n)\)与\(f(n)\)表达式成正比. 所以\(T(n) = O(2n + 2)\), \(T(n) = O(2n^2 + 2n + 3)\), 这就是大\(O\)时间复杂度表示法. 大\(O\)时间复杂度实际表示的是代码执行时间随数据规模增长的变化趋势, 所以, 也叫做渐进时间复杂度, 简称时间复杂度. 当\(n\)很大的时候, 我们只需记录一个最大量级就可以了, 例如\(T(n) = O(n)\); \(T(n) = O(n^2)\). 时间复杂度分析 只关注循环次数最多的一段代码 int cal(int n) { int sum = 0; int i = 1; for(; i &lt;= n; ++i){ sum = sum + i; } return sum; } 总的时间复杂度为\(O(n)\) 加法法则: 总复杂度等于量级最大的那段代码的复杂度 int cal(int n){ int sum_1 = 0; int p = 1; for(; p &lt; 100; ++p){ sum_1 = sum_1 + p; } int sum_2 = 0; int q = 1; for(; q&lt;n; ++q){ sum_2 = sum_2 + q; } int sum_3 = 0; int i = 1; int j = 1; for(; i&lt;=n; ++i){ for(; j&lt;=n; ++j){ sum_3 = sum_3 + i * j; } } return sum_1 + sum_2 + sum_3; } 总的时间复杂度为\(O(n^2)\) 乘法法则: 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 int cal(int n){ int ret = 0; int i = 1; for(; i&lt;n; ++i){ ret = ret + f(i); } } int f(int n){ int sum = 0; int i = 1; for(; i&lt;n; ++i){ sum = sum + i; } return sum; } 总的时间复杂度为\(O(n^2)\) 几种常见时间复杂度实例分析 复杂度量级(按数量级递增) 常量阶\(O(1)\) 对数阶\(O(logn)\) 线性阶\(O(n)\) 线性对数阶\(O(nlogn)\) 平方阶\(O(n^2)\)、立方阶\(O(n^3) \cdots k\)次方阶\(O(n^k)\) 指数阶\(O(2^n)\) 阶乘阶\(O(n!)\) 将上述时间复杂度错略的分为两类：多项式量级和非多项式量级. 其中, 非多项式量级只有两个: \(O(2^n)\)和\(O(n!)\). 我们把时间复杂度为非多项式量级的算法问题叫做NP问题(Non-Deterministic Polynomial, 非确定多项式). 当数据规模\(n\)越来越大时, 非多项式量级算法的执行时间会急剧增加. 因此, NP问题不是我们讨论的重点. 接下来, 我们主要来看几种常见的多项式时间复杂度. \(O(1)\) \(O(1)\)只是常量级时间复杂度的一种表示方法, 并不是指只执行了一行代码. int i = 8; int j = 6; int sum = i + j; 只要代码的执行时间不随\(n\)的增长而增长, 这样代码的时间复杂度都记作\(O(1)\). 一般情况下, 只要算法中不存在循环语句、递归语句, 即使有成千上万行代码, 其时间复杂度也是\(O(1)\). \(O(logn)\)、\(O(nlogn)\) i = 1; while(i&lt;=n){ i = i * 2; } 从代码中可以看出, 变量\(i\)的值为: \[ 2^0\ \ 2^1\ \ 2^2\ \cdots \ 2^k\ \cdots \ 2^x = n \] 通过求解\(2^x = n\), 就可以知道代码的执行次数. 所以其为\(O(\log_2n)\). 因为\(\log_3n\)就等于\(\log_32 * \log_2n\), 所以\(O(\log_3n) = O(C * \log_2n)\), 其中\(C = \log_32\)是一个常量. 因此, 在对数时间复杂度的表示方法里, 忽略对数的&quot;底&quot;, 统一表示为\(O(\log n)\). 如果一段代码的时间复杂度是\(O(\log n)\), 循环\(n\)遍, 时间复杂度就是\(O(n\log n)\). \(O(m+n)\)、\(O(m*n)\) int call(int m, int n){ int sum_1 = 0; int i = 1; for(; i&lt;m; ++i){ sum_1 = sum_1 + 1; } int sum_2 = 0; int j = 1; for(; j&lt;n; ++j){ sum_2 = sum_2 + j; } return sum_1 + sum_2; } 从代码中可以看出, \(m\)和\(n\)是表示两个数据规模, 我们无法评判谁的数量级大, 所以, 时间复杂度就为\(O(m+n)\). 乘法类似. 空间复杂度 空间复杂度全程就是渐进空间复杂度, 表示算法的存储空间与数据规模之间的增长关系. void print(int n){ int i = 0; int[] a = new int[n]; for(i; i&lt;n; ++i){ a[i] = i*i; } for(i=n-1; i&gt;=0; --i){ print out a[i]; } } 第\(2\)行代码中, 我们申请了一个空间存储变量\(i\), 但是它是常量阶, 跟数据规模\(n\)没有关系, 所以忽略. 第\(3\)行申请了一个大小为\(n\)的\(int\)类型数组, 除此之外, 剩下的代码都没有占用更多的空间, 所以整段代码的空间\(O(n)\). 常见的空间复杂度就是\(O(1)\)、\(O(n)\)、\(O(n^2)\). 学习关键 多练]]></content>
      <categories>
        <category>王争课程笔记</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基石 3.机器学习的类型]]></title>
    <url>%2F2018%2F09%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[根据输出空间\(\mathcal{Y}\)分类 二分类问题 重新回顾一下&quot;是非题&quot;的形式. 为了解决这个问题, 需要我们提供一批训练数据\(\mathcal{D}\), 其中我们需要指出对哪些用户发放信用卡, 哪些不发. 像这样答案只有两种可能性(&quot;要&quot;或&quot;不要&quot;)的问题称为二元分类问题, 其输出空间\(\mathcal{Y}\)通常用集合\(\{-1, +1\}\)表示, 类似于&quot;判断题&quot;. 这种问题类型的例子有很多, 包括 要不要发信用卡 电子邮件是不是垃圾邮件 病人有没有生病 广告是否会赚钱 等等。 二元分类问题是机器学习中最基本也是最核心的问题, 很多理论推导和算法模型设计都是从这一类问题出发. 多分类问题 二元分类问题很容易进行扩展, 即如果答案有多个离散的可能性, 那么问题演变为多元分类问题. 假设目标类别有\(K\)种, 那么\(\mathcal{Y}=\{1, 2, \cdots, K\}\). 一个典型的例子是对硬币进行分类, 看投入的是\(1\)角、\(5\)角还是\(1\)元. 这种问题类似于“选择题”. 这种问题类型的例子包括 识别手写数字是\(0\)到\(9\)这十个数字中的哪一种 识别图片中的水果是哪一种水果 邮件的进一步分类, 例如是垃圾邮件、社交网络邮件、重要邮件还是促销活动邮件等等 回归问题 如果将医疗领域中的问题对应到上述问题中, 那么这两种问题可以对应如下： 二元分类问题：给定病人特征, 判断病人是否患病 多元分类问题：给定病人特征, 判断病人患的是哪种癌症 但是还有一类问题， 例如判断病人手术后多少天可以出院. 这种问题的输出是整个实数集, 或者实数集中的一个连续区间. 这种问题通常被称为回归分析. 此时\(\mathcal{Y} \in \mathbb{R}\)或\(\mathcal{Y} = [{\rm lower},{\rm upper}] \subset \mathbb{R}\). 这种问题类型的例子包括 根据公司的状况, 预测其次日股票价格 根据大气状况, 预测明日气温 回归问题是一种历史悠久的统计问题, 也是机器学习领域里非常核心的问题 结构化分析 在自然语言处理(\(\rm NLP\))这个领域里, 有一项任务是对输入句子中的每个词标注其词性(\({\rm Part\ of\ Speech}, {\rm POS}\)). 例如输入&quot;\(\rm {I\ love\ ML}\)&quot;, 程序应该可以将&quot;I&quot;标记为代词, &quot;\(\rm{love}\)&quot;标记为动词, &quot;\(\rm{ML}\)&quot;标记为名词. 这种任务可以看作是一种多元分类问题, 但是如果输入是以句子为单位, 由于句子中有结构性, 因此输出也是一个结构. 这样的问题可以看做是一个巨大的多类别分类问题, 各个类别是隐藏的, 看不到, 而且不同类别之间有联系, 使得穷举所有可能性变得不可能. 但是我们知道输出存在一定的结构性, 并希望程序能够正确给出判定. 这种问题称为结构化分析, 此时\(\mathcal{Y}\)是一种结构. 这种问题类型的例子包括 给定蛋白质数据, 判断蛋白质的结构 给定语言文本, 给出语法树 根据数据标签\(y_n\)分类 有监督学习 考虑第一节中的硬币分类问题. 我们可以将所有硬币的特征收集起来, 设成\({\bf x}_n\), 同时可以将硬币的面额给出, 称为\(y_n\). 这两部分可以一起给到机器学习的算法\(\mathcal A\)里, 得到\(g\). 这种每个特征组\({\bf x}_n\)都有对应的\(y_n\)的学习问题称作有监督学习. 这里&quot;监督&quot;的意义在于, 对每个特征都可以给出对应的标签, 是一种&quot;完整&quot;的教学. 无监督学习 如果对所有数据, 都没有标签给出, 机器也可以通过类似&quot;自学&quot;或者&quot;自己研究&quot;的方式将其归类. 这种学习问题称作&quot;无监督学习&quot;. 注意这种自动分类(称为聚类)的方法并不一定能得到正确的类数, 而且如何判读聚类结果的好坏也是一个难题. 这种问题类型的例子包括 将文章按照主题归类 将用户聚合称为用户群 当然, 无监督学习不止聚类这一种方向, 还包括了 密度估计, 即给定\(X = \{ {\bf x}_1, \ldots, {\bf x}_n \}\), 判断哪里稠密哪里稀疏. 例如给定一些带有地点的交通事故资料, 判断哪里是事故多发区(有点像回归分析) 奇异点检测, 即给定\(X = \{ {\bf x}_1, \ldots, {\bf x}_n \}\), 判断哪些是异常点. 例如给定网络日志, 判断哪些是爬取日志(有点像极端情况下的二元分类) 无监督学习的目标比较分散, 难以衡量算法的好坏 半监督学习 介于有监督学习和无监督学习之间, 只有少量数据有标签, 大部分标签都没有, 使用算法判断数据的类别. 这类问题的特点是标记数据很贵(标签获取不容易). 强化学习 类似于训练宠物的方法: 当人们训练宠物时, 无法直接告诉它给定讯号\({\bf x}_n\)以后期望的\(y_n\), 但是可以在它做了错误的回应以后施加惩罚, 通过这种方式告诉它这种做法是错的(当然也可以在它做了的正确的或者你不排斥的回应以后施加奖励). 即此时系统的输入包括了数据\(X\), 系统的行为\(\tilde{y}\)和奖惩函数\(\rm goodness\). 这种问题类型的例子包括 广告系统: \(X\)为用户特征, \(\tilde{y}\)为给出的广告, \(\rm goodness\)是该广告的收入. 这样系统可以学到对给定用户应该给出什么广告 德州扑克: \(X\)为牌型和底池, \(\tilde{y}\)为叫牌策略, \(\rm goodness\)是牌局结束后的收益. 这样系统可以学到对给定局面的叫牌策略 强化学习实际上是从一些隐含的数据中学习, 这些数据通常是顺序的 根据学习方式\(f \Rightarrow ({\bf x}_n, y_n)\) 批处理学习 读进所有的数据, 训练出来模型\(g\), 使用\(g\)来处理未知数据. 批处理学习是最常见的一种学习方法 在线学习 每看到一个样本就做出预测, 然后根据正确的标签对模型做出更新, 让模型的效果越来越好. 在线学习是一种循序渐进的学习方式. \(PLA\)就很容易被改写为在线学习方法, 因为它看到一个错分样本就会更新权重. 另外, 强化学习通常都是在线学习, 因为每次我们只能得到部分数据. 主动学习 批处理学习有点像填鸭式教育, 在线学习有点像老师课堂讲授, 但是这两种方式其实都是机器被动学习. 近几年一种新提出的研究方式类似于让机器来主动提问, 即对输入\({\bf x}_n\)来提问对应的\(y_n\). 由于机器提问可以有技巧, 因此我们希望这种学习可以加速学习过程, 同时减少对标签的需求. 当标注样本比较昂贵的情况下, 主动学习比较有用 根据输入空间\(\mathcal{X}\)分类 具体特征 这种情况下, 输入\(\mathcal{X} \subseteq \mathbb{R}^d\)中的每个分量(称为特征)都有具体且复杂的物理意义. 这些特征都包含了人类的智慧, 称为&quot;领域知识&quot;, 是机器学习能处理的最简单的输入 原始特征 假设要处理的是手写数字识别问题, 我们可以对输入做一些分析, 提取出具体特征, 包括数字是否对称, 笔画是否有弯折等等. 但是, 也可以直接将原始的每个像素的灰度值组合成一个256维向量(假设图片是\(16 \times 16\)的). 这个输入比具体特征要抽象一些, 求解也更困难, 不过这些数据里仍然包含了一些简单的物理意义. 原始特征通常需要机器或者人来转换为具体特征, 这个转换的过程称为特征工程 抽象特征 以之前提到的预测用户对电影评分的比赛为例, 对于这个问题, 输入\(\mathcal{X} \subseteq \mathbb{N} \times \mathbb{N}\)实际上是用户编号和电影编号组成的二元组, 而这些编号对机器来讲没有任何物理意义, 因此更加需要特征转换]]></content>
      <categories>
        <category>林轩田课程笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基石 2.学习判断是与非]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-2-%E5%AD%A6%E4%B9%A0%E5%88%A4%E6%96%AD%E6%98%AF%E4%B8%8E%E9%9D%9E%2F</url>
    <content type="text"><![CDATA[感知机假设集合 第一章里讲到机器学习的核心就是, 使用算法\(\mathcal{A}\)接受数据\(\mathcal{D}\), 从假设集合(所有可能性)\(\mathcal{H}\)中选出一个\(g\), 希望\(g \approx f\). 那么我们现在最关心的就是, \(\mathcal{H}\)应该是什么样的. 以之前提到的银行审核发放信用卡的场景为例, 假设我们把每个使用者定义为向量\(\bf x\), 包含\(d\)个维度, 例如\(x_1\)代表年龄, \(x_2\)代表年薪, 等等. 我们可以将这些维度综合起来给使用者一个整体的分数. 如果这个分数超过了某个标准, 那么就发放信用卡; 否则拒绝发放. 这样, 我们需要给每个\(x_i, i \in \{ 1, \ldots, d \}\)来赋一个系数\(w_i\), 如果特征对最后的影响是正面的, 那么就给\(w_i\)正值, 否则给负值. 如果我们在规定一个阈值\(\rm threshold\), 那么我们的决策方法就可以写成为, 如果\(\sum_{i=1}^d w_ix_i &gt; \rm threshold\), 就批准信用卡申请, 否则就拒绝. 我们可以进一步地规定输出空间\(\mathcal{Y} \in \{-1, +1\}\), 其中\(y=-1\)时表示拒绝, \(y=1\)时表示许可. 这样做的好处是我们可以直接使用\(\rm sign\)函数来求出\(y\)的值, 具体地说, 假设集合\(\mathcal{H}\)中的每个元素\(h \in \mathcal{H}\)都有如下形式 \[ h({\bf x}) ={\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold}) \] 其中\({\rm sign}\)函数的定义为 \[ {\rm sign}(x) = \begin{cases} +1 &amp; {\rm if \ } x&gt;0 \\ -1 &amp; {\rm if \ } x&lt;0 \end{cases} \] 即对用户的所有属性做一个加权打分, 看它是否超过阈值. 如果超过, 则批准; 否则就拒绝(如果正好等于阈值, 这种情况很少发生, 甚至可以随机决定\(y\)是\(-1\)还是\(1\)). 这里我们说\(\mathcal{H}\)是一个集合的原因是, 不同的\(\bf w\)和\(\rm threshold\)都对应了不同的\(h\), 所有这些可能性对应的所有\(h\)构成了最后的假设集合\(\mathcal{H}\). \(h\)这样的函数类型称为感知机(perceptron), 其中\(\bf w\)称为权重. 进一步地, 假设我们把\(-\rm threshold\)看做是\((-\rm threshold) \cdot (+1)\), 然后把\(+1\)看作是\(x_0\), 那么前面的公式形式可以进一步的简化, 即 \[ \begin{align*} h({\bf x}) &amp;= {\rm sign}((\sum_{i=1}^d w_ix_i) - {\rm threshold}) \\ &amp;= {\rm sign}((\sum_{i=1}^d w_ix_i)+\underbrace{(-{\rm threshold})}_{w_0}\cdot \underbrace{(+1)}_{x_0}) \\ &amp;= {\rm sign}(\sum_{i=0}^d w_ix_i) \\ &amp;= {\rm sign}({\bf w}^\mathsf{T}{\bf x}) \end{align*} \] 这里\(\bf w\)和\(\bf x\)都看作是列向量, 即维度为\((d+1)1\) 我们可以通过一个图例来加强理解. 假如我们顾客的特征数(也就是前面说的属性维度)为\(2\), 那么我们可以把任意输入\(\bf x\)画在一个平面\(\mathbb{R}^2\)上(类似的, 如果特征数为\(d\), 那么每个输入\(\bf x\)都可以在\(\mathbb{R}^d\)空间表示, 只是会对我们的可视化造成困难), 每个输入对应平面上的一个点. 这样, \(\mathbb{R}^2\)上的\(h\)都有如下形式: \[ h({\bf x}) = \rm sign(w_0+w_1x_1+w_2x_2) \] 可以看出, 每个\(h\)其实都对应了\(\mathbb{R}^2\)上的一条直线. 感知机规定位于直线某一侧的样本都被判定为正例, 另一侧的样本都被判定为负例. 不同的权重会产生不同的分类方式. 假设我们用蓝色的圈o表示正例, 红色的叉×表示负例, 下图给出了两个不同的感知机 1 2 可以看出来右边的感知机在训练集上效果更好, 因为它对所有例子做出了正确分类. 而左侧的感知机在训练集上表现稍逊(一个正例被误判为负, 两个负例被误判为正) 由于感知机都对应于一个超平面, 因此它也被称为是线性分类器(\(\mathbb R^2\)的超平面是一条直线, \(\mathbb R^3\)的超平面是一个平面, 以此类推). 感知机学习算法 在我们知道了\(h \in \mathcal H\)的形态以后, 接下来的问题是设计算法\(\mathcal A\)来选出最优的\(g\)来逼近理想的\(f\). 尽管我们不知道\(f\)具体应该是什么, 但是我们知道数据\(\mathcal D\)是由\(f\)生成的. 因此我们有理由相信, 好的\(g\)满足对所有我们已经收集道的数据, 其输出与\(f\)的输出尽可能接近, 即\(g({\bf x}_n) = f({\bf x}_n) = y_n\). 因此, 我们可以先找一个超平面, 至少能够对训练集中的数据正确分类. 然而难度在于, \(\mathcal H\)的大小通常都是无限的. 一种解决方案是, 我们可以先初始化一个超平面\(g_0\)(为了简单起见, 将其以其权重\({\bf w}_0\)代表, 称为初始权重). 我们允许这个超平面犯错, 但我们要设计算法, 让超平面遇到\(\mathcal D\)中的错分样本以后可以修正自己. 通常我们可以将\({\bf w}_0\)初始化为零向量\(\bf 0\). 然后, 在每一步\(t\), 找到一个使\({\bf w}_t\)错分的样本错分的样本\(({\bf x}_{n(t)}, y_{n(t)})\). 即有 \[ \rm sign({\bf w}^T_t {\bf x}_{n(t)}) \not= y_{n(t)} \] 接下里我们试着修正\({\bf w}_t\). 可以看到错分有两种情况: \(y\)本来应该是\(+1\), 但是模型判断出来是负值. 也就是说此时\(\bf w\)与\(\bf x\)之间的角度太大, 因此需要把\(\bf w\)往靠近\(\bf x\)的方向旋转使它们的角度变小. 可以通过让\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf x}_{n(t)}\)达到这个目的 \(y\)本来应该是\(-1\), 但是模型判断出来是正值. 也就是说此时\(\bf w\)与\(\bf x\)之间的角度太小, 因此需要把\(\bf w\)往远离\(\bf x\)的方向旋转使它们的角度变大. 考虑到符号, 其实也可以通过让\({\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)} {\bf w}_{n(t)}\)达到这个目的 因此, 在\(t+1\)时刻, 我们总可以通过下式来修正\({\bf w}_t\), 即 \[ {\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)} \] 3 4 感知机学习算法(Perceptron Learning Algorithm, PLA)就是重复上面的过程, 直到没有错误发生为止. 算法将最后得到的权重\(\bf w\)(记做\({\bf w}_{PLA}\))返回\(g\). 完整写法如下: 对于\(t = 0,1, \ldots\) 1). 找到一个使\({\bf w}_t\)错分的样本\(({\bf x}_{n(t)}, y_{n(t)})\). 即有 \[ sign({\bf w}_t^T {\bf x}_{n(t)}) \not = y_{n(t)} \] 2). 以如下方法修正\({\bf w}_t\): \[ {\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)} \] 直到便利了所有样本一遍以后都没有找到错误为止. 由此, 也引出两个问题: 算法真的会停止吗? 能否确定算法返回的\(g \approx f\)? 感知机的有效性与确定终止性 回顾PLA算法的停止条件, 它是在没有找到错误的时候才停止, 这要求我们的数据可以用一条线将正例样本和负例样本分割开来(如果不存在这条线, PLA肯定是不可能停止的). 这种条件叫做线性可分条件. 接下来, 我们需要证明: 如果数据集的确是线性可分的, 感知机是否总是能找到一个超平面把数据恰好分开. 假设数据集\(\mathcal D\)线性可分, 我们先证明存在一个超平面\({\bf w}_f\)使得任意\(i \in \{1, \ldots, n\}\), \(y_i = {\rm sign}({\bf w}_f^T{\bf x}_i)\). 这意味着对每个\({\bf x}_i\), 它与超平面都有一定距离, 即 \[ \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n &gt; 0 \] 其中\({\bf w}_f^T{\bf x}_n\)是点\({\bf w}_n\)到\({\bf w}_f\)的带符号的距离. 在训练过程中遇到的所有错分点\(({\bf x}_{n(t)}, y_{n(t)})\)(假设在时刻\(t\)遇到), 肯定有 \[ y_{n(t)}{\bf w}_f^\mathsf{T}{\bf x}_{n(t)} \ge \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n &gt; 0 \] 我们可以先证明, \({\bf w}_t\)被\(({\bf x}_{n(t)}, y_{n(t)})\)纠正以后更加接近纠正以后更加接近\({\bf w}_f\). 我们可以通过两个向量的內积来判断它们是否接近: 两个向量越接近, 內积越大(可以理解为两个向量\(\bf u\)和和\(\bf v\)越接近, 其夹角, 那么\(\cos \theta\)越大, 所以两者的內积\({\bf u} \cdot {\bf v} = |\!|{\bf u}|\!||\!|{\bf v}|\!|\cos \theta\)越大), 则 \[ \begin{align*} {\bf w}_f^\mathsf{T}{\bf w}_{t+1} &amp;= {\bf w}_t^\mathsf{T}({\bf w}_t + y_{n(t)}{\bf x}_{n(t)}) \\ &amp;\ge {\bf w}_f^\mathsf{T}{\bf w}_t + \min_n y_n {\bf w}_f^\mathsf{T}{\bf x}_n \\ &amp;&gt; {\bf w}_f^\mathsf{T}{\bf w}_t + 0 = {\bf w}_f^\mathsf{T}{\bf w}_t\hspace{3ex} \blacksquare \end{align*} \] 但是这里又有一个新的问题, 即內积变大不一定说明两个向量接近, 因为向量长度变大也会导致內积变大. 因此接下来我们要证明, 修正\({\bf w}_t\)以后, 新的权重长度不会发生太大的变化. 这里要用到一个性质, 即\(PLA\)仅在遇到错误的数据时才更新权重, 即如果权重\({\bf w}_t\)被订正, 意味着\(sign({\bf w}_t^T{\bf w}_{n(t)}) \not = y_{n(t)}\), 也就是\(y_{n(t)}{\bf w}_t^T{\bf x}_{n(t)} \le 0\). 考虑到\(y_{n(t)}\)是标量, 且取值只可能为\(1\)或\(-1\)(即\(y_{n(t)}^2 = 1\)), \({\bf w}_t^T{\bf x}_{n(t)}\)也是标量, 因此 \[ \begin{align*} |\!|{\bf w}_{t+1}|\!|^2 &amp;= |\!|{\bf w}_t + y_{n(t)}{\bf x}_{n(t)}|\!|^2 \end{align*} \] 简记\(y = y_{n(t)}\), \({\bf x} = {\bf x}_{n(t)}\), \({\bf w} = {\bf w}_t\), 则 \[ \begin{align*} |\!|{\bf w}_{t+1}|\!|^2 &amp;= ({\bf w}+y{\bf x})^\mathsf{T}({\bf w}+y{\bf x}) \\ &amp;= {\bf w}^\mathsf{T}{\bf w} + 2y{\bf w}^\mathsf{T}{\bf x} + {\bf x}^\mathsf{T}{\bf x} \\ &amp;\le |\!|{\bf w}|\!|^2 + |\!|{\bf x}|\!|^2 \hspace{3ex}(\because y{\bf w}^\mathsf{T}{\bf x} \le 0) \\ &amp;\le |\!|{\bf w}|\!|^2 + \max_n|\!|{\bf x}_n|\!|^2 \end{align*} \] 即权重经过修正以后, 其长度最多增加\(\max_n |\!|{\bf x}_n|\!|^2\) 经由上面两部分, 假设权重的初始向量为\(\bf 0\), 我们可以求出经过\(T\)步更新最后得到的权重\({\bf w}_{T}\)与\(\bf w_{f}\)之间的夹角余弦值的下界. 为了求这个值, 只需要求两个权重归一化以后內积的下界即可, 即 \[ \inf \left(\frac{ {\bf w}_f^\mathsf{T}} {|\!|{\bf w}_f|\!|} \cdot \frac{{\bf w}_T}{|\!|{\bf w}_T|\!|} \right) \] 先看分子. 由于初始\({\bf w}_0 = {\bf 0}\), 因此由之前第一个证明的中间步骤, 我们可以写出第一次更新、第二次更新......后分子的下界, 即 \[ \begin{align*} {\bf w}_f^\mathsf{T}{\bf w}_1 &amp;\ge {\bf w}_f^\mathsf{T} \cdot {\bf 0} + \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \\ {\bf w}_f^\mathsf{T}{\bf w}_2 &amp;\ge {\bf w}_f^\mathsf{T}\cdot{\bf w}_1 + \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \ge {\bf x}_f^\mathsf{T} \cdot {\bf 0} + 2\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \\ &amp;\vdots \\ {\bf w}_f^\mathsf{T}{\bf w}_T &amp;\ge T \min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n \end{align*} \] 类似地, 对分母有 \[ |\!|{\bf w}_T|\!|^2 \le T\max_n|\!|{\bf x}_n|\!|^2 \] 因此, \[ \begin{align*} \frac{ {\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f|\!|} \cdot \frac{ {\bf w}_T}{|\!|{\bf w}_T|\!|} &amp;\ge \frac{T\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n}{|\!|{\bf w}_f|\!|\sqrt{T\max_n|\!|{\bf x}_n|\!|^2}} \\ &amp;= \sqrt{T}\cdot \frac{\min_n y_n{\bf w}_f^\mathsf{T}{\bf x}_n}{|\!|{\bf w}_f|\!|\sqrt{\max_n|\!|{\bf x}_n|\!|^2}} \end{align*} \] 按照Fun TIme中的记法, 记\(R^2 = \max_n |\!|{\bf x}_n|\!|^2, \rho = \min_n y_n \frac{{\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f |\!|}{\bf x}_n\), 则 \[ \frac{ {\bf w}_f^\mathsf{T}}{|\!|{\bf w}_f|\!|} \cdot \frac{ {\bf w}_T}{|\!|{\bf w}_T|\!|} \ge \sqrt{T}\cdot\frac{\rho}{R} \] 由向量除以其长度得到的是单位向量, 长度为\(1\), 在这种情况下, 两者內积越大一定意味着两者夹角越小, 距离越近. 但是这里需要注意的是, 两者的距离不会无限接近, 到\(\cos \theta = 1\)时会停止, 因为两个单位向量的內积最大值为1, 因此从上面的不等式可推出 \[ \sqrt{T} \cdot \frac{\rho}{R} \le 1 \Rightarrow T \le \frac{R^2}{\rho^2} \] 即算法至多更新\(\frac{R^2}{\rho^2}\)步后一定会停止. 感知机在线性不可分数据上的应用 由上面的证明, 假设数据集是线性可分的, 那么\(PLA\)算法最后肯定会停止, 而且(对训练集)给出正确的分类. 该算法非常容易实现, 而且结束很快, 使用于任意\(\mathbb{R}^d\)空间. 但是这个算法最大的问题, 它提前假设训练集是训练可分的, 而且我们不知道算法什么时候会终止(因为上面给出的上限中用到了\({\bf w}_f\), 而我们不知道它是多少--甚至不知道是否存在!(在线性不可分的时候该向量不存在)) 那么我们来考虑一个最坏的情况, 即数据若的确是线性不可分的话, 应该如何应对. 由于数据产生的过程中可能会混入噪声, 这使得原本线性可分的数据也可能因为噪声的存在而不可分. 但是, 一般情况下, 噪声应该是一小部分, 即我们可以退而且其次, 不去寻找一个完美的超平面, 而是去寻找一个犯错误最少的超平面, 即 \[ {\bf w}_g \leftarrow \mathop{{\rm arg}\min}_{\bf w} \sum_{n=1}^N [\![ y_n \not = {\rm sign}({\bf w}^\mathsf{T}{\bf x}_n)]\!] \] 然而, 求解这个问题被证明是NP难的, 只能采用近似算法求解. 例如, 我们可以保存一个最好的权重, 该权重到目前为止错分的数量最少. 该算法被称为&quot;口袋法&quot;, 其完整细节如下: 设定初始权重\(\hat{\bf w}\) 对时刻\(t=0, 1, \cdots\) 随机寻找一个\({\bf w}_t\)错分的样本\(({\bf x}_{n(t)}, y_{n(t)})\) 试图通过如下方法修正\({\bf w}_t\) \[ {\bf w}_{t+1} \leftarrow {\bf w}_t + y_{n(t)}{\bf x}_{n(t)} \] 如果\({\bf w}_{t+1}\)犯的错误比\(\hat{\bf w}\)少, 那么将\(\hat{\bf w}\)替换为\({\bf w}_{t+1}\) 直到足够多次迭代完成. 我们将\(\hat{\bf w}\)(称为\({\bf w}_{pocket}\))返回为\(g\) 注意在线性可分集合上也可以使用口袋法, 算法也可以返回一个无训练误差的解. 但是由于每次更新权重以后, 都要在所有数据上使用新旧权重各跑一遍, 来计算错分数量, 因此口袋法的执行时间通常比原始\(PLA\)的计算时间长很多. +参考 txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/]]></content>
      <categories>
        <category>林轩田课程笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基石 1.学习问题]]></title>
    <url>%2F2018%2F09%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-1-%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[机器学习的概念 我们可以从人类的学习思维入手. 人类的学习过程, 是从观察出发, 经过大脑内化以后, 变成有用的技巧. 机器学习, 类似地, 是我们希望让计算机模拟人类的学习过程. 这时, 计算机观察到的东西被称作数据, 而思考过程实际上是计算过程, 技巧则是提高某一方面的表现. 因此, 机器学习的过程 为什么需要机器学习? 给定一张照片, 判断照片里的物体是不是一棵树. 使用传统的方法, 就需要对&quot;什么是树&quot;做出回答, 给出树的定义, 将其实现为程序. 按照规则进行判断, 并将其表述出来是很困难的. 然而, 一个小孩认识树的方法其实是通过观察, 经过经验的积累判断这个是树或者不是, 并不是教条的从长辈那里学习判断规则. 类似地, 我们可以让计算机自己从数据中学习树的判断方法. 因此, 机器学习是构建复杂系统的一种方法 机器学习的使用场景 当我们不能提前想好各种情况, 手工编码规则时. 例如让机器人在火星上导航, 我们不可能提前想到它在火星上会遇到什么样的情况 当我们无法轻易地定义问题的解决方案时. 例如要做语音识别/视觉识别, 我们无法对音频信号做出准确定义 当人们需要做出快速决策时. 例如高频交易 当要让机器服务于海量用户时. 例如做服务个性化定制 因此, 可以从以下三个关键点进行判断, 看是否适合使用机器学习 问题是&quot;可以学习的&quot;, 即存在一些潜在的模式, 以至于性能可以被提高 这些规则难以清晰定义 手里掌握对应的数据 机器学习的应用 机器学习在衣食住行四个方面都得到了广泛地应用 衣: Abu-Mostafa 2012利用销售数据和对用户的调研结果构建推荐系统给用户推荐穿搭 食: Sadilek et al. 2013利用机器学习, 根据Twitter数据, 来判断餐厅的好坏 住: Tsanas and Xifara 2012利用已有房间的特点和耗能, 预测房屋的能用消耗 此外还有两个领域: 教育和娱乐 教育: 系统根据学生的答题情况, 有针对地提供题目让学生练习其薄弱的部分, 同时将太难的题推后给出. 即, 给定一名学生的答题历史和一个题目, 预测学生是否能做对这道题( KDDCup 2010 ) 娱乐: 系统根据用户的历史打分, 预测用户对新电影的打分( KDDCup 2011 ) 机器学习的过程 问题背景 以银行信用卡发卡这一问题为例. 假设银行收集了一些用户的基本信息, 例如下表 特征 值 年龄 23 性别 女 所在地居住年数 1 工龄 0.5 负债额 200,000 银行要解决的问题是, 对于这样的客户, 是否应该给她发放信用卡 问题形式化描述 为了更加形式化地描述这个问题, 需要先定义一些符号: 输入: \({\bf x} \in \mathcal{X}\), 用户的特征 输出: \({\bf y} \in \mathcal{Y}\), 是否发放信用卡 目标函数: \(f: \mathcal{X} \rightarrow \mathcal{Y}\), 是我们期望学到, 但是目前不知道的东西. 是最理想的公式 数据: \(\mathcal{D} = \{({\bf x}_1, y_1), ({\bf x}_2, y_2), \ldots, ({\bf x}_n, y_n)\}\), 是之前积累的记录 假设: \(g: \mathcal{X} \rightarrow \mathcal{Y}\), 是机器从数据中学到的函数. 我们通常都希望\(g\)的表现足够好, 即\(g \approx f\). 注意这里\(g\)不一定等于\(f\)(实际上, 我们永远也无法知道真正的\(f\)是什么样子, 只知道由\(f\)产生的数据\(\mathcal{D}\)) 机器学习算法: \(\mathcal {A}\), 是由\(\mathcal {D}\)产生\(g\)的算法, 可以理解为\(\mathcal {A}\)会从各种不同假设\(h_k\)(这里\(h_k\)有好有坏)构成的集合\(\mathcal{H}\)中挑选出来一个最好的\(g\), 使得\(g \approx f\). 即\(\mathcal{A}\)以\(\mathcal{D}\)和\(\mathcal{H}\)为输入, 以\(g\)为输出 机器学习过程 我们所讲的机器学习模型, 指的就是\(\mathcal{A}\)和\(\mathcal{H}\) 在有个这些记号以后, 我们可以重新给机器学习下一个定义 机器学习是使用数据计算假设\(g\)以逼近目标函数\(f\)的过程 机器学习与其它名词 机器学习与数据挖掘 数据挖掘的一个简单定义是使用海量数据, 在其中找出一些有趣的现象或性质. 这里, 如果&quot;有用的性质&quot;就是&quot;能够逼近目标函数的假设&quot;, 那么数据挖掘和机器学习是没有区别的. 如果&quot;有用的性质&quot;与&quot;假设&quot;是相关联的, 那么数据挖掘在很大程度上可以帮助机器学习 传统上的数据挖掘还关注如何在大的数据集中进行有效计算, 不过现在已经很难将机器学习和数据挖掘这两个概念分开了. 机器学习与人工智能 人工智能要求计算机呈现出一些智能的行为. 由于机器学习逼近目标函数的过程就展示了一些智能, 因此我们可以说, 机器学习是实现人工智能的一种手段. 机器学习与统计学 统计学是使用数据来对未知过程进行推论. 考虑到假设\(g\)是推论结果, \(f\)是不知道的事, 那么可以说统计是实现机器学习的一种方法. 但是传统的统计学从数学出发, 很多工具是为数学假设提供证明和推论. 而机器学习看重的是如何计算出结果. 总而言之, 统计学为机器学习提供了很多有力的工具 +参考 txshi-mt.com/2017/08/01/NTUML-1-the-Learning-Problem/]]></content>
      <categories>
        <category>林轩田课程笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
